---
date: '2022-10-01'
---

## October 2022

### Announcements


#### Farnam Maintenance
The biannual scheduled maintenance for the Farnam cluster will be occurring Oct 4-6. During this time, the cluster will be unavailable. See the Farnam maintenance email announcements for more details.

#### Gibbs Maintenance
Additionally, the Gibbs storage system will be unavailable on Grace and Ruddle on Oct 4 to deploy an urgent firmware fix. All jobs on those clusters will be held, and no new jobs will be able to start during the upgrade to avoid job failures.

#### New Command for Interactive Jobs

The new version of Slurm (the scheduler) has improved the process of launching an interactive compute job. Instead of the clunky `srun --pty bash` syntax from previous versions, this is now replaced with `salloc`. In addition, the interactive partition is now the default partition for jobs launched using `salloc`. Thus a simple (1 core, 1 hour) interactive job can be requested like this:

```
salloc
```

which will submit the job and then move your shell to the allocated compute node. 

For MPI users, this allows multi-node parallel jobs to be properly launched inside an interactive compute job, which did not work as expected previously. For example, here is a two-node job, launched with `salloc` and then a parallel job-step launched with `srun`:

```
[user@grace1 ~]$ salloc --nodes 2 --ntasks 2 --cpus-per-task 1
salloc: Nodes p09r07n[24,28] are ready for job

[user@p09r07n24 ~]$ srun hostname
p09r07n24.grace.hpc.yale.internal
P09r07n28.grace.hpc.yale.internal
```

For more information on `salloc`, please refer to [Slurmâ€™s documentation](https://slurm.schedmd.com/salloc.html).


### Software Highlights

- **cellranger/7.0.1** is now available on Farnam.
- **LAMMPS/23Jun2022-foss-2020b-kokkos** is now available on Grace.
