
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Yale Center for Research Computing Documentation">
      
      
      
        <meta name="author" content="YCRC">
      
      
        <link rel="canonical" href="https://docs.ycrc.yale.edu/clusters/mccleary/">
      
      <link rel="icon" href="../../assets/images/favicon.ico">
      <meta name="generator" content="mkdocs-1.3.1, mkdocs-material-7.3.6">
    
    
      
        <title>McCleary - Yale Center for Research Computing</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.a57b2b03.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.3f5d1f46.min.css">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
      <link rel="stylesheet" href="../../_static/stylesheets/extra.css">
    
    
      

  


  

  


  <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-QM84KPHSPL"),document.addEventListener("DOMContentLoaded",function(){"undefined"!=typeof location$&&location$.subscribe(function(t){gtag("config","G-QM84KPHSPL",{page_path:t.pathname})})})</script>
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-QM84KPHSPL"></script>


    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="none" data-md-color-accent="none">
  
    
    <script>function __prefix(e){return new URL("../..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#mccleary" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <!--
  Copyright (c) 2016-2020 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->



  


<header class="md-header md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Yale Center for Research Computing" class="md-header__button md-logo" aria-label="Yale Center for Research Computing" data-md-component="logo">
      
  <img src="../../img/manual.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>

    <!-- Header title -->
    <div class="md-header__title" data-md-component="header-title">
        <div class="md-header__ellipsis">
            <span class="md-header__topic md-ellipsis branding">
              <a href="../.." >
                  <kern style="letter-spacing:-0.09em">Y</kern>ale <em>Center for Research Computing</em>
              </a>
            </span>
          <span class="md-header__topic md-ellipsis">
            
              McCleary
            
          </span>
        </div>

    </div>

    

    

    <!-- Search Interface -->
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    


    <!-- Repository containing source -->
    
      <div class="md-header-nav__source">
        
<a href="https://github.com/ycrc/ycrc.github.io/tree/src" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    github/ycrc/docs
  </div>
</a>
      </div>
    

  </nav>
  
    
      <!--
  Copyright (c) 2016-2019 Martin Donath <martin.donath@squidfunk.com>
  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:
  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.
  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->

<!-- Determine class according to level -->


  



<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link md-tabs__link--active">
        About
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../clusters-at-yale/" class="md-tabs__link">
        User Guide
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../data/" class="md-tabs__link">
        Data & Storage
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../clusters-at-yale/guides/" class="md-tabs__link">
        Software & Tools
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../resources/" class="md-tabs__link">
        Training & Other Resources
      </a>
    </li>
  

      
      <li class="md-tabs__item status"><a class="md-tabs__link" href="http://research.computing.yale.edu/system-status"><span id='status-icon'></span> Systems Status</a></li>
    </ul>
  </div>
</nav>

<div id="system-status-message"></div>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Yale Center for Research Computing" class="md-nav__button md-logo" aria-label="Yale Center for Research Computing" data-md-component="logo">
      
  <img src="../../img/manual.svg" alt="logo">

    </a>
    Yale Center for Research Computing
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/ycrc/ycrc.github.io/tree/src" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    github/ycrc/docs
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_1" type="checkbox" id="__nav_1" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_1">
          About
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="About" data-md-level="1">
        <label class="md-nav__title" for="__nav_1">
          <span class="md-nav__icon md-icon"></span>
          About
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../news/" class="md-nav__link">
        News
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_1_3" type="checkbox" id="__nav_1_3" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_1_3">
          HPC Clusters
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="HPC Clusters" data-md-level="2">
        <label class="md-nav__title" for="__nav_1_3">
          <span class="md-nav__icon md-icon"></span>
          HPC Clusters
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../grace/" class="md-nav__link">
        Grace
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          McCleary
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        McCleary
      </a>
      
        


<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#access-the-cluster" class="md-nav__link">
    Access the Cluster
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#system-status-and-monitoring" class="md-nav__link">
    System Status and Monitoring
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#installed-applications" class="md-nav__link">
    Installed Applications
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#partitions-and-hardware" class="md-nav__link">
    Partitions and Hardware
  </a>
  
    <nav class="md-nav" aria-label="Partitions and Hardware">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#public-partitions" class="md-nav__link">
    Public Partitions
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#private-partitions" class="md-nav__link">
    Private Partitions
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ycga-partitions" class="md-nav__link">
    YCGA Partitions
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#public-datasets" class="md-nav__link">
    Public Datasets
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ycga-data" class="md-nav__link">
    YCGA Data
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#storage" class="md-nav__link">
    Storage
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../milgram/" class="md-nav__link">
        Milgram
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../misha/" class="md-nav__link">
        Misha
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../bouchet/" class="md-nav__link">
        Bouchet
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../maintenance/" class="md-nav__link">
        Maintenance
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../user-group/" class="md-nav__link">
        YCRC User Group
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="https://research.computing.yale.edu/about" class="md-nav__link">
        About the YCRC
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../glossary/" class="md-nav__link">
        Glossary
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          User Guide
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="User Guide" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          User Guide
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/" class="md-nav__link">
        Getting Started
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/access/accounts/" class="md-nav__link">
        Accounts & Best Practices
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="https://research.computing.yale.edu/account-request" class="md-nav__link">
        Request an Account
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/help-requests/" class="md-nav__link">
        Help Requests
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/troubleshoot/" class="md-nav__link">
        Troubleshoot Login
      </a>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_6" type="checkbox" id="__nav_2_6" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_6">
          Access the Clusters
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Access the Clusters" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_6">
          <span class="md-nav__icon md-icon"></span>
          Access the Clusters
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/access/" class="md-nav__link">
        Log on to the Clusters
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_6_2" type="checkbox" id="__nav_2_6_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_6_2">
          Web Portal
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Web Portal" data-md-level="3">
        <label class="md-nav__title" for="__nav_2_6_2">
          <span class="md-nav__icon md-icon"></span>
          Web Portal
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/access/ood/" class="md-nav__link">
        Access the Web Portal
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/access/ood-remote-desktop/" class="md-nav__link">
        Remote Desktop
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/access/ood-jupyter/" class="md-nav__link">
        Jupyter
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/access/ood-rstudio/" class="md-nav__link">
        RStudio
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/access/ood-vscode/" class="md-nav__link">
        VSCode
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_6_3" type="checkbox" id="__nav_2_6_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_6_3">
          SSH Connection
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="SSH Connection" data-md-level="3">
        <label class="md-nav__title" for="__nav_2_6_3">
          <span class="md-nav__icon md-icon"></span>
          SSH Connection
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/access/ssh/" class="md-nav__link">
        Connect with SSH
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/access/x11/" class="md-nav__link">
        Graphical Interfaces (X11)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/access/advanced-config/" class="md-nav__link">
        Advanced SSH Configuration
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/access/vpn/" class="md-nav__link">
        Access from Off Campus (VPN)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/access/mfa/" class="md-nav__link">
        Multi-factor Authentication
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/access/courses/" class="md-nav__link">
        Courses
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_7" type="checkbox" id="__nav_2_7" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_7">
          Applications & Software
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Applications & Software" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_7">
          <span class="md-nav__icon md-icon"></span>
          Applications & Software
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../applications/" class="md-nav__link">
        Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../applications/compile/" class="md-nav__link">
        Build Software
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../applications/modules/" class="md-nav__link">
        Software Modules
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../applications/toolchains/" class="md-nav__link">
        Module Toolchains
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../applications/lifecycle/" class="md-nav__link">
        Module Lifecycle
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_8" type="checkbox" id="__nav_2_8" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_8">
          Job Scheduling
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Job Scheduling" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_8">
          <span class="md-nav__icon md-icon"></span>
          Job Scheduling
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/job-scheduling/" class="md-nav__link">
        Run Jobs with Slurm
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/job-scheduling/resource-requests/" class="md-nav__link">
        Request Compute Resources
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/job-scheduling/getusage/" class="md-nav__link">
        Monitor Overall Slurm Usage
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/job-scheduling/common-job-failures/" class="md-nav__link">
        Common Job Failures
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/job-scheduling/cmd-line-args/" class="md-nav__link">
        Pass Values into Jobs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/job-scheduling/jobstats/" class="md-nav__link">
        Job Performance Monitoring
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/job-scheduling/fairshare/" class="md-nav__link">
        Priority & Wait Time
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/job-scheduling/dsq/" class="md-nav__link">
        Job Arrays with dSQ
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/job-scheduling/dependency/" class="md-nav__link">
        Jobs with Dependencies
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/job-scheduling/scrontab/" class="md-nav__link">
        Recurring Jobs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/job-scheduling/priority-tier/" class="md-nav__link">
        Priority Tier
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/job-scheduling/scavenge/" class="md-nav__link">
        Scavenge Partition
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/job-scheduling/mpi/" class="md-nav__link">
        MPI Partition
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/job-scheduling/slurm-examples/" class="md-nav__link">
        Submission Script Examples
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/glossary/" class="md-nav__link">
        Glossary
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          Data & Storage
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Data & Storage" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Data & Storage
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../data/" class="md-nav__link">
        Data Storage
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../data/hpc-storage/" class="md-nav__link">
        HPC Storage
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../data/backups/" class="md-nav__link">
        Backups and Snapshots
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../data/google-drive/" class="md-nav__link">
        Google Drive
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../data/ycga-data/" class="md-nav__link">
        YCGA Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6" type="checkbox" id="__nav_3_6" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3_6">
          Transfer Data
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Transfer Data" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_6">
          <span class="md-nav__icon md-icon"></span>
          Transfer Data
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../data/transfer/" class="md-nav__link">
        Transfer to Cluster
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/guides/rclone/" class="md-nav__link">
        Rclone
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../data/globus/" class="md-nav__link">
        Large Transfers with Globus
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../data/staging/" class="md-nav__link">
        Stage Data for Compute Jobs
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_7" type="checkbox" id="__nav_3_7" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3_7">
          Manage & Share
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Manage & Share" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_7">
          <span class="md-nav__icon md-icon"></span>
          Manage & Share
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../data/permissions/" class="md-nav__link">
        Share with Cluster Users
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../data/archive/" class="md-nav__link">
        Archive Your Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../data/external/" class="md-nav__link">
        Share Data Outside Yale
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../data/group-change/" class="md-nav__link">
        Group Change
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../data/glossary/" class="md-nav__link">
        Glossary
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          Software & Tools
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Software & Tools" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Software & Tools
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/guides/" class="md-nav__link">
        Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4_2" type="checkbox" id="__nav_4_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4_2">
          Software
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Software" data-md-level="2">
        <label class="md-nav__title" for="__nav_4_2">
          <span class="md-nav__icon md-icon"></span>
          Software
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4_2_1" type="checkbox" id="__nav_4_2_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4_2_1">
          Python
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Python" data-md-level="3">
        <label class="md-nav__title" for="__nav_4_2_1">
          <span class="md-nav__icon md-icon"></span>
          Python
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/guides/python/" class="md-nav__link">
        Python
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/guides/conda/" class="md-nav__link">
        Conda
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/guides/pytorch/" class="md-nav__link">
        Pytorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/guides/tensorflow/" class="md-nav__link">
        Tensorflow
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/guides/mpi4py/" class="md-nav__link">
        MPI with Python
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/guides/atlas/" class="md-nav__link">
        ATLAS Computing Environment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/guides/cesm/" class="md-nav__link">
        CESM/CAM
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/guides/comsol/" class="md-nav__link">
        COMSOL
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/guides/gaussian/" class="md-nav__link">
        Gaussian
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/guides/isca/" class="md-nav__link">
        Isca
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/guides/matlab/" class="md-nav__link">
        MATLAB
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/guides/mathematica/" class="md-nav__link">
        Mathematica
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/guides/namd/" class="md-nav__link">
        NAMD
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/guides/nextflow/" class="md-nav__link">
        Nextflow
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/guides/r/" class="md-nav__link">
        R
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/guides/vasp/" class="md-nav__link">
        VASP
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4_3" type="checkbox" id="__nav_4_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4_3">
          Tools
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Tools" data-md-level="2">
        <label class="md-nav__title" for="__nav_4_3">
          <span class="md-nav__icon md-icon"></span>
          Tools
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/guides/containers/" class="md-nav__link">
        Containers
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/guides/mysql/" class="md-nav__link">
        Mysql
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/guides/parallel/" class="md-nav__link">
        Parallel
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/guides/tmux/" class="md-nav__link">
        tmux
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/guides/virtualgl/" class="md-nav__link">
        VirtualGL
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/guides/xvfb/" class="md-nav__link">
        XVFB
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4_4" type="checkbox" id="__nav_4_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4_4">
          Guides
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Guides" data-md-level="2">
        <label class="md-nav__title" for="__nav_4_4">
          <span class="md-nav__icon md-icon"></span>
          Guides
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/guides/cryoem/" class="md-nav__link">
        Cryo-EM on McCleary
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/guides/github/" class="md-nav__link">
        GitHub
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/guides/github_pages/" class="md-nav__link">
        GitHub Pages
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/guides/gpus-cuda/" class="md-nav__link">
        GPUs and CUDA
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../clusters-at-yale/guides/spark/" class="md-nav__link">
        Spark
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5">
          Training & Other Resources
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Training & Other Resources" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Training & Other Resources
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../resources/" class="md-nav__link">
        Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_2" type="checkbox" id="__nav_5_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5_2">
          Tutorials
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Tutorials" data-md-level="2">
        <label class="md-nav__title" for="__nav_5_2">
          <span class="md-nav__icon md-icon"></span>
          Tutorials
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../resources/intro_to_hpc_tutorial/" class="md-nav__link">
        Introduction to HPC Tutorials
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../resources/online-tutorials/" class="md-nav__link">
        Online Tutorials
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../resources/yale_library/" class="md-nav__link">
        Yale Library
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../resources/sw_carpentry/" class="md-nav__link">
        Software Carpentry
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_3" type="checkbox" id="__nav_5_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5_3">
          Training
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Training" data-md-level="2">
        <label class="md-nav__title" for="__nav_5_3">
          <span class="md-nav__icon md-icon"></span>
          Training
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="https://research.computing.yale.edu/training" class="md-nav__link">
        YCRC Workshops
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="https://ycrc.yale.edu/youtube" class="md-nav__link">
        YCRC YouTube Channel
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../resources/national-hpcs/" class="md-nav__link">
        National HPCs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../resources/glossary/" class="md-nav__link">
        Glossary
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#access-the-cluster" class="md-nav__link">
    Access the Cluster
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#system-status-and-monitoring" class="md-nav__link">
    System Status and Monitoring
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#installed-applications" class="md-nav__link">
    Installed Applications
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#partitions-and-hardware" class="md-nav__link">
    Partitions and Hardware
  </a>
  
    <nav class="md-nav" aria-label="Partitions and Hardware">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#public-partitions" class="md-nav__link">
    Public Partitions
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#private-partitions" class="md-nav__link">
    Private Partitions
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ycga-partitions" class="md-nav__link">
    YCGA Partitions
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#public-datasets" class="md-nav__link">
    Public Datasets
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ycga-data" class="md-nav__link">
    YCGA Data
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#storage" class="md-nav__link">
    Storage
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/ycrc/ycrc.github.io/tree/src/docs/clusters/mccleary.md" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                <h1 id="mccleary">McCleary</h1>
<p><img alt="Beatrix McCleary Hamburg" class="cluster-portrait" src="/img/beatrix-mccleary.jpg" /></p>
<p>McCleary is a shared-use resource for the <a href="https://medicine.yale.edu">Yale School of Medicine</a> (YSM), life science researchers elsewhere on campus and projects related to the <a href="http://ycga.yale.edu/">Yale Center for Genome Analysis</a>. It consists of a variety of compute nodes networked over ethernet and mounts several shared filesystems.</p>
<p>McCleary is named for <a href="https://www.nytimes.com/2018/04/19/obituaries/beatrix-hamburg-barrier-breaking-scholar-is-dead-at-94.html">Beatrix McCleary Hamburg</a>, who received her medical degree in 1948 and was the first female African American graduate of Yale School of Medicine. The McCleary HPC cluster is Yale's first direct-to-chip liquid cooled cluster, moving the YCRC and the Yale research computing community into a more environmentally friendly future.</p>
<hr />
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>Farnam or Ruddle user? Farnam and Ruddle were both retired in summer 2023. See <a href="/clusters/mccleary-farnam-ruddle">our explainer</a> for what you need to know about using McCleary and how it differs from Farnam and Ruddle.</p>
</div>
<h2 id="access-the-cluster">Access the Cluster</h2>
<p>Once you have <a href="https://research.computing.yale.edu/support/hpc/account-request">an account</a>, the cluster can be accessed <a href="/clusters-at-yale/access">via ssh</a> or through the <a href="/clusters-at-yale/access/ood/">Open OnDemand web portal</a>.</p>
<h2 id="system-status-and-monitoring">System Status and Monitoring</h2>
<p>For system status messages and the schedule for upcoming maintenance, please see the <a href="https://research.computing.yale.edu/support/hpc/system-status">system status page</a>. For a current node-level view of job activity, see the <a href="http://cluster.ycrc.yale.edu/mccleary/">cluster monitor page (VPN only)</a>.</p>
<h2 id="installed-applications">Installed Applications</h2>
<p>A large number of software and applications are installed on our clusters.
These are made available to researchers via <a href="/applications/modules/">software modules</a>.</p>
<details class="summary">
<summary>Available Software Modules (click to expand)</summary>
<table>
<thead>
<tr>
<th>Package</th>
<th>Versions</th>
</tr>
</thead>
<tbody>
<tr>
<td>ACTC</td>
<td>1.1-GCCcore-10.2.0,1.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>ADMIXTURE</td>
<td>1.3.0</td>
</tr>
<tr>
<td>AFNI</td>
<td>23.2.08-foss-2020b,2022.1.14,2023.1.01-foss-2020b-Python-3.8.6,2023.1.07-foss-2020b,24.1.22-foss-2022b</td>
</tr>
<tr>
<td>ANTLR</td>
<td>2.7.7-GCCcore-12.2.0-Java-11</td>
</tr>
<tr>
<td>ANTs</td>
<td>2.3.5-foss-2020b</td>
</tr>
<tr>
<td>APBS</td>
<td>1.4.2.1-1-linux64,3.4.1.Linux</td>
</tr>
<tr>
<td>APR</td>
<td>1.7.0-GCCcore-10.2.0,1.7.5-GCCcore-12.2.0</td>
</tr>
<tr>
<td>APR-util</td>
<td>1.6.1-GCCcore-10.2.0,1.6.3-GCCcore-12.2.0</td>
</tr>
<tr>
<td>ASE</td>
<td>3.22.1-gfbf-2022b</td>
</tr>
<tr>
<td>ATK</td>
<td>2.36.0-GCCcore-10.2.0,2.38.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>AUGUSTUS</td>
<td>3.4.0-foss-2020b</td>
</tr>
<tr>
<td>Abseil</td>
<td>20230125.2-GCCcore-12.2.0</td>
</tr>
<tr>
<td>AdapterRemoval</td>
<td>2.3.2-GCC-10.2.0</td>
</tr>
<tr>
<td>AlphaFold</td>
<td>2.2.3-fosscuda-2020b-uniclust30_2020_06,2.2.3-fosscuda-2020b,2.2.4-fosscuda-2020b-uniclust30_2020_06,2.2.4-fosscuda-2020b,2.3.2-foss-2020b-CUDA-11.3.1,2.3.2-foss-2022b-CUDA-12.1.1</td>
</tr>
<tr>
<td>AmberTools</td>
<td>23.6-foss-2022b</td>
</tr>
<tr>
<td>Archive-Zip</td>
<td>1.68-GCCcore-10.2.0,1.68-GCCcore-12.2.0</td>
</tr>
<tr>
<td>AreTomo</td>
<td>1.3.4-Feb22_2023</td>
</tr>
<tr>
<td>AreTomo2</td>
<td>1.0.0-GCCcore-12.2.0-CUDA-12.0.0</td>
</tr>
<tr>
<td>Armadillo</td>
<td>10.2.1-foss-2020b,11.4.3-foss-2022b,11.4.3-nvofbf-2023.01</td>
</tr>
<tr>
<td>Arrow</td>
<td>0.17.1-foss-2020b,0.17.1-fosscuda-2020b,6.0.0-foss-2020b,11.0.0-gfbf-2022b,14.0.1-foss-2022b,16.1.0-gfbf-2022b</td>
</tr>
<tr>
<td>Aspera-CLI</td>
<td>3.9.6.1467.159c5b1</td>
</tr>
<tr>
<td>Aspera-Connect</td>
<td>4.2.4.265</td>
</tr>
<tr>
<td>AuthentiCT</td>
<td>1.0.1-foss-2020b</td>
</tr>
<tr>
<td>Autoconf</td>
<td>2.69-GCCcore-10.2.0,2.71-GCCcore-12.2.0,2.72-GCCcore-13.3.0</td>
</tr>
<tr>
<td>Automake</td>
<td>1.16.2-GCCcore-10.2.0,1.16.5-GCCcore-12.2.0,1.16.5-GCCcore-13.3.0</td>
</tr>
<tr>
<td>Autotools</td>
<td>20200321-GCCcore-10.2.0,20220317-GCCcore-12.2.0,20231222-GCCcore-13.3.0</td>
</tr>
<tr>
<td>BBMap</td>
<td>38.90-GCCcore-10.2.0</td>
</tr>
<tr>
<td>BCFtools</td>
<td>1.11-GCC-10.2.0,1.16-GCCcore-10.2.0,1.21-GCC-12.2.0</td>
</tr>
<tr>
<td>BEDOPS</td>
<td>2.4.41</td>
</tr>
<tr>
<td>BEDTools</td>
<td>2.30.0-GCCcore-10.2.0</td>
</tr>
<tr>
<td>BLAST</td>
<td>2.2.26-Linux_x86_64</td>
</tr>
<tr>
<td>BLAST+</td>
<td>2.13.0-gompi-2020b,2.14.1-gompi-2020b,2.15.0-gompi-2022b</td>
</tr>
<tr>
<td>BLAT</td>
<td>3.5-GCC-10.2.0,3.5-GCC-12.2.0</td>
</tr>
<tr>
<td>BLIS</td>
<td>0.9.0-GCC-12.2.0,1.0-GCC-13.3.0</td>
</tr>
<tr>
<td>BLT</td>
<td>20220626-GCCcore-10.2.0</td>
</tr>
<tr>
<td>BWA</td>
<td>0.7.17-GCC-10.2.0,0.7.17-GCCcore-10.2.0,0.7.17-GCCcore-12.2.0</td>
</tr>
<tr>
<td>BamTools</td>
<td>2.5.1-GCC-10.2.0,2.5.1-GCCcore-10.2.0,2.5.2-GCC-12.2.0</td>
</tr>
<tr>
<td>BaseSpaceCLI</td>
<td>1.5.3</td>
</tr>
<tr>
<td>Bazel</td>
<td>3.7.2-GCCcore-10.2.0,5.4.1-GCCcore-12.2.0,6.1.0-GCCcore-12.2.0,6.3.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Beast</td>
<td>2.6.3-fosscuda-2020b,2.6.3-GCCcore-10.2.0,2.6.7-GCC-10.2.0,2.7.4-GCC-12.2.0,2.7.6-GCC-12.2.0-CUDA-12.0.0</td>
</tr>
<tr>
<td>BeautifulSoup</td>
<td>4.11.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Bio-DB-BigFile</td>
<td>1.07-GCC-10.2.0,1.07-GCC-12.2.0</td>
</tr>
<tr>
<td>Bio-DB-HTS</td>
<td>3.01-GCC-10.2.0,3.01-GCC-12.2.0</td>
</tr>
<tr>
<td>BioPP</td>
<td>2.4.1-GCC-10.2.0</td>
</tr>
<tr>
<td>BioPerl</td>
<td>1.7.8-GCCcore-10.2.0,1.7.8-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Biopython</td>
<td>1.78-fosscuda-2020b,1.79-foss-2020b,1.81-foss-2022b,1.83-foss-2022b</td>
</tr>
<tr>
<td>Bismark</td>
<td>0.24.0-GCCcore-10.2.0</td>
</tr>
<tr>
<td>Bison</td>
<td>3.0.4-GCCcore-7.3.0,3.0.4,3.0.5-GCCcore-7.3.0,3.7.1-GCCcore-10.2.0,3.7.1,3.8.2-GCCcore-12.2.0,3.8.2-GCCcore-13.3.0,3.8.2</td>
</tr>
<tr>
<td>Blender</td>
<td>4.0.1-linux-x86_64-CUDA-12.1.1,4.2.1-linux-x86_64-CUDA-12.1.1</td>
</tr>
<tr>
<td>Block</td>
<td>1.5.3-20200525-foss-2022b</td>
</tr>
<tr>
<td>Blosc</td>
<td>1.21.0-GCCcore-10.2.0,1.21.3-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Blosc2</td>
<td>2.8.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Boost</td>
<td>1.74.0-GCC-10.2.0,1.74.0-GCCcore-10.2.0,1.74.0-gompi-2020b,1.74.0-iccifort-2020.4.304,1.74.0-iompi-2020b,1.81.0-GCC-12.2.0,1.81.0-iompi-2022b,1.81.0-NVHPC-23.1-CUDA-12.0.0,1.83.0-GCC-12.2.0,1.85.0-GCC-13.3.0,1.86.0-GCC-12.2.0</td>
</tr>
<tr>
<td>Boost.MPI</td>
<td>1.81.0-gompi-2022b,1.81.0-nvompi-2023.01</td>
</tr>
<tr>
<td>Boost.Python</td>
<td>1.74.0-GCC-10.2.0,1.81.0-GCC-12.2.0</td>
</tr>
<tr>
<td>Boost.Python-NumPy</td>
<td>1.74.0-foss-2020b,1.81.0-foss-2022b</td>
</tr>
<tr>
<td>Bowtie</td>
<td>1.3.0-GCC-10.2.0,1.3.0-GCCcore-10.2.0,1.3.1-GCC-12.2.0</td>
</tr>
<tr>
<td>Bowtie2</td>
<td>2.3.4.3-GCC-10.2.0,2.4.2-GCC-10.2.0,2.4.2-GCCcore-10.2.0,2.5.1-GCC-12.2.0</td>
</tr>
<tr>
<td>Brotli</td>
<td>1.0.9-GCCcore-10.2.0,1.0.9-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Brunsli</td>
<td>0.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Bsoft</td>
<td>2.1.4-GCCcore-10.2.0</td>
</tr>
<tr>
<td>CAMPARI</td>
<td>4.0-intel-2022b</td>
</tr>
<tr>
<td>CCP4</td>
<td>8.0.011,8.0.015-GCCcore-10.2.0</td>
</tr>
<tr>
<td>CD-HIT</td>
<td>4.8.1-GCC-10.2.0</td>
</tr>
<tr>
<td>CDO</td>
<td>2.2.2-iomkl-2022b</td>
</tr>
<tr>
<td>CESM</td>
<td>2.1.3-intel-2022b,2.1.3-iomkl-2022b</td>
</tr>
<tr>
<td>CESM-deps</td>
<td>2-intel-2022b,2-iomkl-2022b</td>
</tr>
<tr>
<td>CFITSIO</td>
<td>3.48-GCCcore-10.2.0,4.2.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>CGAL</td>
<td>4.14.3-gompi-2020b,4.14.3-gompi-2022b,5.2-gompi-2020b,5.2.4-gompi-2022b,5.5.2-GCCcore-12.2.0</td>
</tr>
<tr>
<td>CLHEP</td>
<td>2.4.4.0-GCC-10.2.0,2.4.6.4-GCC-12.2.0</td>
</tr>
<tr>
<td>CMake</td>
<td>3.18.4-GCCcore-10.2.0,3.18.4,3.20.1-GCCcore-10.2.0,3.24.3-GCCcore-12.2.0,3.29.3-GCCcore-13.3.0</td>
</tr>
<tr>
<td>COMSOL</td>
<td>5.2a-classkit,5.2a-research</td>
</tr>
<tr>
<td>CONN</td>
<td>22a</td>
</tr>
<tr>
<td>CP2K</td>
<td>8.1-intel-2020b</td>
</tr>
<tr>
<td>CPPE</td>
<td>0.3.1-GCC-12.2.0</td>
</tr>
<tr>
<td>CTFFIND</td>
<td>4.1.14-foss-2020b,4.1.14-foss-2022b-CUDA-12.0.0,4.1.14-foss-2022b-CUDA-12.1.1,4.1.14-fosscuda-2020b,4.1.14-intel-2020b</td>
</tr>
<tr>
<td>CUDA</td>
<td>10.1.243,11.1.1-GCC-10.2.0,11.3.1,11.8.0,12.0.0,12.1.1,12.6.0</td>
</tr>
<tr>
<td>CUDAcore</td>
<td>11.1.1,11.3.1</td>
</tr>
<tr>
<td>CUnit</td>
<td>2.1-3-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Cartopy</td>
<td>0.20.3-foss-2022b,0.22.0-foss-2022b</td>
</tr>
<tr>
<td>Catch2</td>
<td>2.13.10-GCCcore-13.3.0</td>
</tr>
<tr>
<td>Cbc</td>
<td>2.10.5-foss-2022b</td>
</tr>
<tr>
<td>CellRanger</td>
<td>3.0.2,6.1.2,7.0.0,7.0.1,7.1.0,7.2.0,8.0.1</td>
</tr>
<tr>
<td>CellRanger-ARC</td>
<td>2.0.2</td>
</tr>
<tr>
<td>Cereal</td>
<td>1.3.2-GCCcore-12.2.0,1.3.2</td>
</tr>
<tr>
<td>Cgl</td>
<td>0.60.7-foss-2022b</td>
</tr>
<tr>
<td>CharLS</td>
<td>2.2.0-GCCcore-10.2.0,2.4.2-GCCcore-12.2.0</td>
</tr>
<tr>
<td>CheMPS2</td>
<td>1.8.12-foss-2022b</td>
</tr>
<tr>
<td>Check</td>
<td>0.15.2-GCCcore-10.2.0,0.15.2-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Chimera</td>
<td>1.16-linux_x86_64</td>
</tr>
<tr>
<td>ChimeraX</td>
<td>1.6.1-1.el8,1.7-1.el8,1.8-1-gfbf-2022b-CUDA-12.1.1</td>
</tr>
<tr>
<td>Clang</td>
<td>11.0.1-GCCcore-10.2.0,13.0.1-GCCcore-12.2.0,15.0.5-GCCcore-12.2.0,16.0.4-GCCcore-12.2.0-CUDA-12.0.0,16.0.4-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Clp</td>
<td>1.17.8-foss-2022b</td>
</tr>
<tr>
<td>Code-Server</td>
<td>4.7.0,4.7.0,4.16.1,4.17.0</td>
</tr>
<tr>
<td>CoinUtils</td>
<td>2.11.9-GCC-12.2.0</td>
</tr>
<tr>
<td>Compress-Raw-Zlib</td>
<td>2.202-GCCcore-10.2.0,2.202-GCCcore-12.2.0</td>
</tr>
<tr>
<td>CoordgenLibs</td>
<td>3.0.2-gompi-2022b</td>
</tr>
<tr>
<td>Coot</td>
<td>0.9.7-binary-Linux-x86_64-centos-7-python-gtk2,0.9.8.6</td>
</tr>
<tr>
<td>CppUnit</td>
<td>1.15.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Cufflinks</td>
<td>20190706-GCCcore-10.2.0</td>
</tr>
<tr>
<td>Cython</td>
<td>0.29.22-GCCcore-10.2.0,3.0.8-GCCcore-12.2.0,3.0.10-GCCcore-13.3.0</td>
</tr>
<tr>
<td>DB</td>
<td>18.1.40-GCCcore-10.2.0,18.1.40-GCCcore-12.2.0</td>
</tr>
<tr>
<td>DBD-mysql</td>
<td>4.050-GCC-10.2.0,4.050-GCC-12.2.0</td>
</tr>
<tr>
<td>DB_File</td>
<td>1.855-GCCcore-10.2.0</td>
</tr>
<tr>
<td>DBus</td>
<td>1.13.18-GCCcore-10.2.0,1.15.2-GCCcore-12.2.0</td>
</tr>
<tr>
<td>DIAMOND</td>
<td>2.0.15-GCCcore-10.2.0,2.1.7-GCCcore-10.2.0</td>
</tr>
<tr>
<td>DMTCP</td>
<td>3.0.0-GCCcore-10.2.0,3.0.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>DSSP</td>
<td>4.2.1-GCCcore-10.2.0,4.4.7-GCC-12.2.0</td>
</tr>
<tr>
<td>Dice</td>
<td>20240101-foss-2022b</td>
</tr>
<tr>
<td>Doxygen</td>
<td>1.8.20-GCCcore-10.2.0,1.9.5-GCCcore-12.2.0</td>
</tr>
<tr>
<td>EDirect</td>
<td>20.4.20230912-GCCcore-10.2.0,20.5.20231006-GCCcore-12.2.0,22.8.20241011-GCCcore-12.2.0</td>
</tr>
<tr>
<td>EIGENSOFT</td>
<td>7.2.1-foss-2020b</td>
</tr>
<tr>
<td>ELPA</td>
<td>2020.11.001-foss-2020b,2020.11.001-intel-2020b,2021.11.001-intel-2022b,2022.05.001-intel-2022b</td>
</tr>
<tr>
<td>EMAN</td>
<td>1.9</td>
</tr>
<tr>
<td>EMAN2</td>
<td>2.91-SPHIRE-1.4-SPARX,2.99.47-SPHIRE-1.4-SPARX</td>
</tr>
<tr>
<td>EMBOSS</td>
<td>6.6.0-foss-2020b</td>
</tr>
<tr>
<td>ESM-2</td>
<td>2.0.0-foss-2022b-CUDA-12.0.0</td>
</tr>
<tr>
<td>ESMF</td>
<td>8.3.0-intel-2022b,8.3.0-iomkl-2022b</td>
</tr>
<tr>
<td>EasyBuild</td>
<td>4.6.2,4.7.0,4.7.1,4.7.2,4.8.0,4.8.1,4.8.2,4.9.0,4.9.1,4.9.2,4.9.3</td>
</tr>
<tr>
<td>Eigen</td>
<td>3.3.8-GCCcore-10.2.0,3.3.9-GCCcore-10.2.0,3.4.0-GCCcore-10.2.0,3.4.0-GCCcore-12.2.0,3.4.0-GCCcore-13.3.0</td>
</tr>
<tr>
<td>El-MAVEN</td>
<td>0.12.1beta</td>
</tr>
<tr>
<td>Emacs</td>
<td>28.1-GCCcore-10.2.0,28.2-GCCcore-12.2.0</td>
</tr>
<tr>
<td>ExifTool</td>
<td>12.58-GCCcore-10.2.0,12.70-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Exodus</td>
<td>20240403-foss-2022b-patched,20240403-foss-2022b</td>
</tr>
<tr>
<td>FASTX-Toolkit</td>
<td>0.0.14-GCC-10.2.0</td>
</tr>
<tr>
<td>FFTW</td>
<td>2.1.5-gompi-2020b,2.1.5-gompi-2022b,2.1.5-iomkl-2020b,2.1.5-iomkl-2022b,3.3.8-GCC-10.2.0-serial,3.3.8-gompi-2020b,3.3.8-gompic-2020b,3.3.8-intel-2020b,3.3.8-iomkl-2020b,3.3.10-GCC-12.2.0,3.3.10-GCC-13.3.0,3.3.10-iimpi-2022b,3.3.10-iomkl-2022b,3.3.10-NVHPC-23.1-CUDA-12.0.0</td>
</tr>
<tr>
<td>FFTW.MPI</td>
<td>3.3.10-gompi-2022b,3.3.10-gompi-2024a,3.3.10-nvompi-2023.01</td>
</tr>
<tr>
<td>FFmpeg</td>
<td>4.3.1-GCCcore-10.2.0,5.1.2-GCCcore-12.2.0</td>
</tr>
<tr>
<td>FHI-aims</td>
<td>231212_1-intel-2022b</td>
</tr>
<tr>
<td>FLAC</td>
<td>1.3.3-GCCcore-10.2.0,1.4.2-GCCcore-12.2.0</td>
</tr>
<tr>
<td>FLASH</td>
<td>2.2.00-foss-2020b</td>
</tr>
<tr>
<td>FLTK</td>
<td>1.3.5-GCCcore-10.2.0,1.3.8-GCCcore-12.2.0</td>
</tr>
<tr>
<td>FSL</td>
<td>6.0.5.2-centos7_64,6.0.5.2-fosscuda-2020b,6.0.7.9</td>
</tr>
<tr>
<td>FTGL</td>
<td>2.3-GCCcore-10.2.0,2.4.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Faiss</td>
<td>1.7.4-foss-2022b-CUDA-12.0.0</td>
</tr>
<tr>
<td>FastME</td>
<td>2.1.6.3-GCC-10.2.0</td>
</tr>
<tr>
<td>FastQC</td>
<td>0.11.9-Java-11,0.12.1-Java-11</td>
</tr>
<tr>
<td>FastUniq</td>
<td>1.1-GCCcore-10.2.0</td>
</tr>
<tr>
<td>Fiji</td>
<td>2.14.0-Java-8,20221201-1017,20230801-1717</td>
</tr>
<tr>
<td>Fiona</td>
<td>1.9.2-foss-2022b</td>
</tr>
<tr>
<td>Flask</td>
<td>2.2.3-GCCcore-12.2.0</td>
</tr>
<tr>
<td>FlexiBLAS</td>
<td>3.2.1-GCC-12.2.0,3.2.1-NVHPC-23.1-CUDA-12.0.0,3.4.4-GCC-13.3.0</td>
</tr>
<tr>
<td>FragGeneScan</td>
<td>1.31-GCCcore-10.2.0</td>
</tr>
<tr>
<td>FreeImage</td>
<td>3.18.0-GCCcore-10.2.0,3.18.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>FreeSurfer</td>
<td>dev-20240826-rocky8_x86_64,dev-20240922-rocky8_x86_64,7.3.2-centos8_x86_64,7.4.1-centos8_x86_64</td>
</tr>
<tr>
<td>FreeXL</td>
<td>2.0.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>FriBidi</td>
<td>1.0.10-GCCcore-10.2.0,1.0.12-GCCcore-12.2.0</td>
</tr>
<tr>
<td>GATK</td>
<td>3.8-1-0-gf15c1c3ef-Java-1.8,4.2.0.0-GCCcore-10.2.0-Java-11,4.2.6.1-GCCcore-12.2.0-Java-11,4.4.0.0-GCCcore-10.2.0-Java-17,4.5.0.0,4.6.0.0-GCCcore-12.2.0-Java-17</td>
</tr>
<tr>
<td>GCC</td>
<td>10.2.0,12.2.0,13.3.0</td>
</tr>
<tr>
<td>GCCcore</td>
<td>7.3.0,10.2.0,12.2.0,13.3.0</td>
</tr>
<tr>
<td>GCTA</td>
<td>1.94.1-gfbf-2022b</td>
</tr>
<tr>
<td>GConf</td>
<td>3.2.6-GCCcore-12.2.0</td>
</tr>
<tr>
<td>GDAL</td>
<td>3.2.1-foss-2020b,3.6.2-foss-2022b</td>
</tr>
<tr>
<td>GDB</td>
<td>10.1-GCCcore-10.2.0,13.2-GCCcore-12.2.0</td>
</tr>
<tr>
<td>GDCM</td>
<td>3.0.21-GCCcore-12.2.0</td>
</tr>
<tr>
<td>GDRCopy</td>
<td>2.1-GCCcore-10.2.0-CUDA-11.1.1,2.3-GCCcore-12.2.0,2.3.1-GCCcore-12.2.0,2.4.1-GCCcore-13.3.0</td>
</tr>
<tr>
<td>GEOS</td>
<td>3.9.1-GCC-10.2.0,3.11.1-GCC-12.2.0</td>
</tr>
<tr>
<td>GL2PS</td>
<td>1.4.2-GCCcore-10.2.0,1.4.2-GCCcore-12.2.0</td>
</tr>
<tr>
<td>GLM</td>
<td>0.9.9.8-GCCcore-12.2.0</td>
</tr>
<tr>
<td>GLPK</td>
<td>4.65-GCCcore-10.2.0,5.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>GLib</td>
<td>2.66.1-GCCcore-10.2.0,2.75.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>GLibmm</td>
<td>2.49.7-GCCcore-10.2.0</td>
</tr>
<tr>
<td>GMP</td>
<td>6.2.0-GCCcore-10.2.0,6.2.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>GObject-Introspection</td>
<td>1.66.1-GCCcore-10.2.0-libpng15,1.66.1-GCCcore-10.2.0,1.74.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>GRASS</td>
<td>8.2.0-foss-2022b</td>
</tr>
<tr>
<td>GROMACS</td>
<td>2021.5-foss-2020b-CUDA-11.1.1-PLUMED-2.7.3,2023.3-foss-2022b-CUDA-12.1.1-PLUMED-2.9.2</td>
</tr>
<tr>
<td>GSEA</td>
<td>4.3.2</td>
</tr>
<tr>
<td>GSL</td>
<td>2.5-GCC-12.2.0,2.6-GCC-10.2.0,2.6-GCCcore-10.2.0,2.6-iccifort-2020.4.304,2.7-GCC-12.2.0,2.7-GCCcore-12.2.0,2.7-intel-compilers-2022.2.1</td>
</tr>
<tr>
<td>GST-libav</td>
<td>1.18.4-GCC-10.2.0</td>
</tr>
<tr>
<td>GST-plugins-bad</td>
<td>1.22.5-GCC-12.2.0</td>
</tr>
<tr>
<td>GST-plugins-base</td>
<td>1.18.4-GCC-10.2.0,1.18.4-intel-2020b,1.22.1-GCC-12.2.0,1.22.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>GST-plugins-good</td>
<td>1.18.4-GCC-10.2.0</td>
</tr>
<tr>
<td>GStreamer</td>
<td>1.18.4-GCC-10.2.0,1.18.4-intel-2020b,1.22.1-GCC-12.2.0,1.22.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>GTK+</td>
<td>3.24.23-GCCcore-10.2.0</td>
</tr>
<tr>
<td>GTK2</td>
<td>2.24.33-GCCcore-10.2.0-libpng15</td>
</tr>
<tr>
<td>GTK3</td>
<td>3.24.35-GCCcore-12.2.0</td>
</tr>
<tr>
<td>GTK4</td>
<td>4.11.3-GCC-12.2.0</td>
</tr>
<tr>
<td>GTS</td>
<td>0.7.6-GCCcore-10.2.0</td>
</tr>
<tr>
<td>Garfield++</td>
<td>5.0-foss-2022b</td>
</tr>
<tr>
<td>Gaussian</td>
<td>16-C.01_AVX-patched_GV,16-C.01_AVX</td>
</tr>
<tr>
<td>Gctf</td>
<td>1.18-CUDA-12.0.0,1.18-GCCcore-10.2.0</td>
</tr>
<tr>
<td>Gdk-Pixbuf</td>
<td>2.40.0-GCCcore-10.2.0-libpng15,2.40.0-GCCcore-10.2.0,2.42.10-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Geant4</td>
<td>10.7.1-GCC-10.2.0</td>
</tr>
<tr>
<td>GenomeTools</td>
<td>1.6.1-GCC-10.2.0</td>
</tr>
<tr>
<td>Ghostscript</td>
<td>9.53.3-GCCcore-10.2.0,10.0.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>GitPython</td>
<td>3.1.31-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Globus-CLI</td>
<td>3.18.0-GCCcore-12.2.0,3.30.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>GnuTLS</td>
<td>3.7.8-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Go</td>
<td>1.17.6,1.21.1,1.21.4,1.22.1</td>
</tr>
<tr>
<td>Grace</td>
<td>5.1.25-foss-2020b</td>
</tr>
<tr>
<td>Gradle</td>
<td>8.6-Java-17</td>
</tr>
<tr>
<td>Graphene</td>
<td>1.10.8-GCCcore-12.2.0</td>
</tr>
<tr>
<td>GraphicsMagick</td>
<td>1.3.36-foss-2020b</td>
</tr>
<tr>
<td>Graphviz</td>
<td>2.47.0-GCCcore-10.2.0-Java-11</td>
</tr>
<tr>
<td>Guile</td>
<td>2.2.7-GCCcore-10.2.0,3.0.9-GCCcore-10.2.0,3.0.9-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Gurobi</td>
<td>9.1.2-GCCcore-10.2.0,10.0.3-GCCcore-12.2.0</td>
</tr>
<tr>
<td>HDF</td>
<td>4.2.15-GCCcore-10.2.0,4.2.15-GCCcore-12.2.0</td>
</tr>
<tr>
<td>HDF5</td>
<td>1.10.7-gompi-2020b,1.10.7-gompic-2020b,1.10.7-iimpi-2020b,1.10.7-iompi-2020b,1.14.0-gompi-2022b,1.14.0-iimpi-2022b,1.14.0-iompi-2022b,1.14.0-nvompi-2023.01</td>
</tr>
<tr>
<td>HDFView</td>
<td>3.3.1-centos7_64</td>
</tr>
<tr>
<td>HH-suite</td>
<td>3.3.0-gompi-2020b,3.3.0-gompi-2022b,3.3.0-gompic-2020b</td>
</tr>
<tr>
<td>HISAT-3N</td>
<td>20221013-gompi-2020b</td>
</tr>
<tr>
<td>HISAT2</td>
<td>2.2.1-gompi-2020b</td>
</tr>
<tr>
<td>HMMER</td>
<td>3.3.2-gompi-2020b,3.3.2-gompic-2020b,3.4-gompi-2022b</td>
</tr>
<tr>
<td>HOOMD-blue</td>
<td>4.9.1-foss-2024a-CUDA-12.6.0,4.9.1-foss-2024a</td>
</tr>
<tr>
<td>HPCG</td>
<td>3.1-foss-2020b,3.1-foss-2022b,3.1-intel-2020b,3.1-intel-2022b</td>
</tr>
<tr>
<td>HPL</td>
<td>2.3-foss-2020b,2.3-foss-2022b,2.3-intel-2020b,2.3-intel-2022b</td>
</tr>
<tr>
<td>HTSeq</td>
<td>0.13.5-foss-2020b-Python-3.8.6</td>
</tr>
<tr>
<td>HTSlib</td>
<td>1.11-GCC-10.2.0,1.11-GCCcore-10.2.0,1.12-GCCcore-10.2.0,1.16-GCCcore-10.2.0,1.17-GCC-12.2.0,1.21-GCC-12.2.0</td>
</tr>
<tr>
<td>HarfBuzz</td>
<td>2.6.7-GCCcore-10.2.0,5.3.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Harminv</td>
<td>1.4.1-foss-2020b,1.4.2-foss-2020b</td>
</tr>
<tr>
<td>HepMC3</td>
<td>3.2.6-GCC-12.2.0</td>
</tr>
<tr>
<td>Highway</td>
<td>1.0.3-GCCcore-12.2.0</td>
</tr>
<tr>
<td>HyPhy</td>
<td>2.5.62-gompi-2022b</td>
</tr>
<tr>
<td>Hypre</td>
<td>2.20.0-foss-2020b,2.27.0-foss-2022b</td>
</tr>
<tr>
<td>ICU</td>
<td>67.1-GCCcore-10.2.0,72.1-GCCcore-12.2.0,75.1-GCCcore-13.3.0</td>
</tr>
<tr>
<td>IDBA-UD</td>
<td>1.1.3-GCC-10.2.0</td>
</tr>
<tr>
<td>IGV</td>
<td>2.16.0-Java-11,2.16.2,2.17.4-Java-17</td>
</tr>
<tr>
<td>IMOD</td>
<td>4.11.15-fosscuda-2020b,4.11.16-fosscuda-2020b,4.11.24_RHEL7-64_CUDA10.1,4.11.24-fosscuda-2020b,4.12.56_RHEL7-64_CUDA12.1,4.12.62_RHEL8-64_CUDA12.0</td>
</tr>
<tr>
<td>IOR</td>
<td>4.0.0-gompi-2022b,4.0.0-iimpi-2022b</td>
</tr>
<tr>
<td>IPython</td>
<td>7.18.1-GCCcore-10.2.0,8.14.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>IQ-TREE</td>
<td>2.1.2-gompi-2020b</td>
</tr>
<tr>
<td>ISA-L</td>
<td>2.30.0-GCCcore-10.2.0</td>
</tr>
<tr>
<td>ISL</td>
<td>0.23-GCCcore-10.2.0,0.26-GCCcore-12.2.0</td>
</tr>
<tr>
<td>ImageMagick</td>
<td>7.0.10-35-GCCcore-10.2.0,7.1.0-53-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Imath</td>
<td>3.1.6-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Infernal</td>
<td>1.1.4-foss-2020b</td>
</tr>
<tr>
<td>IsoNet</td>
<td>0.2.1-fosscuda-2020b</td>
</tr>
<tr>
<td>JAGS</td>
<td>4.3.0-foss-2020b,4.3.2-foss-2022b</td>
</tr>
<tr>
<td>Jansson</td>
<td>2.14-GCC-12.2.0</td>
</tr>
<tr>
<td>JasPer</td>
<td>2.0.24-GCCcore-10.2.0,4.0.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Java</td>
<td>1.8.345,8.345,11.0.16,17.0.4</td>
</tr>
<tr>
<td>JsonCpp</td>
<td>1.9.4-GCCcore-10.2.0,1.9.5-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Judy</td>
<td>1.0.5-GCCcore-10.2.0,1.0.5-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Julia</td>
<td>1.8.2-linux-x86_64,1.8.5-linux-x86_64,1.9.2-linux-x86_64,1.10.0-linux-x86_64,1.10.2-linux-x86_64,1.10.4-linux-x86_64,1.11.1-linux-x86_64</td>
</tr>
<tr>
<td>Jupyter-bundle</td>
<td>20230823-GCCcore-12.2.0</td>
</tr>
<tr>
<td>JupyterHub</td>
<td>4.0.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>JupyterLab</td>
<td>2.2.8-GCCcore-10.2.0,4.0.3-GCCcore-12.2.0</td>
</tr>
<tr>
<td>JupyterNotebook</td>
<td>7.0.3-GCCcore-12.2.0</td>
</tr>
<tr>
<td>KaHIP</td>
<td>3.14-gompi-2022b</td>
</tr>
<tr>
<td>Kalign</td>
<td>3.3.1-GCCcore-10.2.0,3.4.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Kent_tools</td>
<td>411-GCC-10.2.0,461-GCC-12.2.0</td>
</tr>
<tr>
<td>Knitro</td>
<td>12.0.0,14.0.0</td>
</tr>
<tr>
<td>Kraken2</td>
<td>2.1.3-gompi-2020b</td>
</tr>
<tr>
<td>LAME</td>
<td>3.100-GCCcore-10.2.0,3.100-GCCcore-12.2.0</td>
</tr>
<tr>
<td>LAMMPS</td>
<td>2Aug2023-foss-2022b-kokkos,23Jun2022-foss-2020b-kokkos</td>
</tr>
<tr>
<td>LDC</td>
<td>0.17.6-x86_64,1.25.1-GCCcore-10.2.0</td>
</tr>
<tr>
<td>LERC</td>
<td>4.0.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>LHAPDF</td>
<td>6.5.4-GCC-12.2.0</td>
</tr>
<tr>
<td>LLVM</td>
<td>11.0.0-GCCcore-10.2.0,14.0.6-GCCcore-12.2.0-llvmlite,15.0.5-GCCcore-12.2.0,16.0.4-GCCcore-12.2.0</td>
</tr>
<tr>
<td>LMDB</td>
<td>0.9.24-GCCcore-10.2.0,0.9.29-GCCcore-12.2.0</td>
</tr>
<tr>
<td>LSD2</td>
<td>2.2-GCCcore-10.2.0</td>
</tr>
<tr>
<td>LZO</td>
<td>2.10-GCCcore-10.2.0,2.10-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Leptonica</td>
<td>1.83.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>LibSoup</td>
<td>3.0.8-GCC-12.2.0</td>
</tr>
<tr>
<td>LibTIFF</td>
<td>4.1.0-GCCcore-10.2.0,4.2.0-GCCcore-10.2.0,4.4.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Libint</td>
<td>2.6.0-iccifort-2020.4.304-lmax-6-cp2k</td>
</tr>
<tr>
<td>LittleCMS</td>
<td>2.11-GCCcore-10.2.0,2.14-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Lua</td>
<td>5.4.2-GCCcore-10.2.0,5.4.4-GCCcore-12.2.0</td>
</tr>
<tr>
<td>M4</td>
<td>1.4.17,1.4.18-GCCcore-7.3.0,1.4.18-GCCcore-10.2.0,1.4.18,1.4.19-GCCcore-12.2.0,1.4.19-GCCcore-13.3.0,1.4.19</td>
</tr>
<tr>
<td>MACS2</td>
<td>2.2.7.1-foss-2020b-Python-3.8.6,2.2.9.1-foss-2020b,2.2.9.1-foss-2022b</td>
</tr>
<tr>
<td>MACS3</td>
<td>3.0.1-foss-2022b</td>
</tr>
<tr>
<td>MAFFT</td>
<td>7.475-gompi-2020b-with-extensions,7.505-GCC-10.2.0-with-extensions</td>
</tr>
<tr>
<td>MAGeCK</td>
<td>0.5.9.5-gfbf-2022b</td>
</tr>
<tr>
<td>MATIO</td>
<td>1.5.23-GCCcore-12.2.0</td>
</tr>
<tr>
<td>MATLAB</td>
<td>2018b,2020b,2022a,2022b,2023a,2023b</td>
</tr>
<tr>
<td>MCL</td>
<td>14.137-GCCcore-10.2.0</td>
</tr>
<tr>
<td>MCR</td>
<td>R2019b.8,R2020b.5,R2021b.6,R2022a.6,R2023a</td>
</tr>
<tr>
<td>MDI</td>
<td>1.4.16-gompi-2022b</td>
</tr>
<tr>
<td>MEME</td>
<td>5.4.1-GCCcore-10.2.0-Python-3.8.6</td>
</tr>
<tr>
<td>METIS</td>
<td>5.1.0-GCCcore-10.2.0,5.1.0-GCCcore-12.2.0,5.1.0-GCCcore-12.2.0-32bit</td>
</tr>
<tr>
<td>MINC</td>
<td>2.4.06-foss-2022b</td>
</tr>
<tr>
<td>MMseqs2</td>
<td>13-45111-20220809-gompi-2020b,14-7e284-gompi-2022b</td>
</tr>
<tr>
<td>MPB</td>
<td>1.11.1-foss-2020b</td>
</tr>
<tr>
<td>MPC</td>
<td>1.2.1-GCCcore-10.2.0,1.3.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>MPFR</td>
<td>4.1.0-GCCcore-10.2.0,4.2.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>MPICH</td>
<td>4.2.1-GCC-12.2.0</td>
</tr>
<tr>
<td>MRIcron</td>
<td>1.0.20190902</td>
</tr>
<tr>
<td>MRtrix3</td>
<td>3.0.2-foss-2020b</td>
</tr>
<tr>
<td>MUMPS</td>
<td>5.3.5-foss-2020b-metis,5.6.1-foss-2022b-metis</td>
</tr>
<tr>
<td>MUSCLE</td>
<td>5.1-GCCcore-10.2.0</td>
</tr>
<tr>
<td>MadGraph5_aMC</td>
<td>2.9.16-gompi-2022b</td>
</tr>
<tr>
<td>MafFilter</td>
<td>1.3.1-GCC-10.2.0</td>
</tr>
<tr>
<td>Mako</td>
<td>1.1.3-GCCcore-10.2.0,1.2.4-GCCcore-12.2.0</td>
</tr>
<tr>
<td>MariaDB</td>
<td>10.5.8-GCC-10.2.0,10.11.2-GCC-12.2.0</td>
</tr>
<tr>
<td>Markdown</td>
<td>3.6-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Mathematica</td>
<td>13.0.1</td>
</tr>
<tr>
<td>Maven</td>
<td>3.9.2</td>
</tr>
<tr>
<td>MaxBin</td>
<td>2.2.7-gompi-2020b</td>
</tr>
<tr>
<td>MaxQuant</td>
<td>2.4.2.0-GCCcore-10.2.0,2.4.2.0-GCCcore-12.2.0,2.6.1.0</td>
</tr>
<tr>
<td>Meep</td>
<td>1.24.0-foss-2020b,1.26.0-foss-2020b</td>
</tr>
<tr>
<td>Mercurial</td>
<td>5.7.1-GCCcore-10.2.0</td>
</tr>
<tr>
<td>Mesa</td>
<td>20.2.1-GCCcore-10.2.0,21.3.3-GCCcore-10.2.0,22.2.4-GCCcore-12.2.0</td>
</tr>
<tr>
<td>MeshLab</td>
<td>2023.12-GCC-12.2.0</td>
</tr>
<tr>
<td>Meson</td>
<td>0.55.3-GCCcore-10.2.0,0.62.1-GCCcore-10.2.0,0.64.0-GCCcore-12.2.0,1.3.1-GCCcore-12.2.0,1.4.0-GCCcore-13.3.0</td>
</tr>
<tr>
<td>Metal</td>
<td>2020-05-05-GCC-10.2.0</td>
</tr>
<tr>
<td>MitoGraph</td>
<td>3.0-foss-2020b</td>
</tr>
<tr>
<td>Mono</td>
<td>6.8.0.105-GCCcore-10.2.0,6.8.0.123-GCCcore-12.2.0</td>
</tr>
<tr>
<td>MotionCor2</td>
<td>1.5.0-GCCcore-10.2.0,1.6.4-GCCcore-12.2.0</td>
</tr>
<tr>
<td>MotionCor3</td>
<td>1.0.1-GCCcore-12.2.0-CUDA-12.0.0</td>
</tr>
<tr>
<td>MrBayes</td>
<td>3.2.7-gompi-2020b</td>
</tr>
<tr>
<td>MultiQC</td>
<td>1.10.1-foss-2020b-Python-3.8.6</td>
</tr>
<tr>
<td>NAG</td>
<td>29</td>
</tr>
<tr>
<td>NAMD</td>
<td>2.14-foss-2020b-mpi,2.14-foss-2022b-mpi,2.14-fosscuda-2020b,2.14-multicore</td>
</tr>
<tr>
<td>NASM</td>
<td>2.15.05-GCCcore-10.2.0,2.15.05-GCCcore-12.2.0</td>
</tr>
<tr>
<td>NBO</td>
<td>7.0</td>
</tr>
<tr>
<td>NCCL</td>
<td>2.8.3-GCCcore-10.2.0-CUDA-11.1.1,2.8.4-CUDA-11.1.1,2.10.3-GCCcore-10.2.0-CUDA-11.3.1,2.16.2-GCCcore-12.2.0-CUDA-11.8.0,2.16.2-GCCcore-12.2.0-CUDA-12.0.0,2.16.2-GCCcore-12.2.0-CUDA-12.1.1,2.18.3-GCCcore-12.2.0-CUDA-12.1.1,2.23.4-GCCcore-13.3.0-CUDA-12.6.0</td>
</tr>
<tr>
<td>NCO</td>
<td>5.2.1-iomkl-2022b</td>
</tr>
<tr>
<td>NECI</td>
<td>20230620-foss-2022b</td>
</tr>
<tr>
<td>NEdit</td>
<td>5.7-GCCcore-10.2.0</td>
</tr>
<tr>
<td>NGS</td>
<td>2.10.9-GCCcore-10.2.0</td>
</tr>
<tr>
<td>NIfTI</td>
<td>2.0.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>NLopt</td>
<td>2.6.2-GCCcore-10.2.0-MATLAB2020b,2.6.2-GCCcore-10.2.0,2.7.0-GCCcore-10.2.0,2.7.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>NSPR</td>
<td>4.29-GCCcore-10.2.0,4.35-GCCcore-12.2.0</td>
</tr>
<tr>
<td>NSS</td>
<td>3.57-GCCcore-10.2.0,3.85-GCCcore-12.2.0</td>
</tr>
<tr>
<td>NVHPC</td>
<td>21.11-CUDA-11.3.1,21.11,23.1-CUDA-12.0.0,24.9-CUDA-12.6.0</td>
</tr>
<tr>
<td>Net-core</td>
<td>3.1.101</td>
</tr>
<tr>
<td>NetLogo</td>
<td>6.4.0-64</td>
</tr>
<tr>
<td>Netpbm</td>
<td>10.86.41-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Nextflow</td>
<td>22.10.6,23.04.2,23.10.1,24.04.2,24.04.4</td>
</tr>
<tr>
<td>Ninja</td>
<td>1.10.1-GCCcore-10.2.0,1.11.1-GCCcore-12.2.0,1.12.1-GCCcore-13.3.0</td>
</tr>
<tr>
<td>ORCA</td>
<td>5.0.3-gompi-2020b,5.0.3-gompi-2022b,5.0.4-gompi-2020b,5.0.4-gompi-2022b,6.0.0-gompi-2022b,6.0.1-gompi-2022b</td>
</tr>
<tr>
<td>OSU-Micro-Benchmarks</td>
<td>5.7-gompi-2020b,5.7-iimpi-2020b,6.2-gompi-2022b,6.2-iimpi-2022b</td>
</tr>
<tr>
<td>OligoArray</td>
<td>2.1-GCCcore-10.2.0-Java-11</td>
</tr>
<tr>
<td>OligoArrayAux</td>
<td>3.8-GCCcore-10.2.0</td>
</tr>
<tr>
<td>OpenBLAS</td>
<td>0.3.12-GCC-10.2.0,0.3.21-GCC-12.2.0,0.3.21-NVHPC-23.1-CUDA-12.0.0,0.3.27-GCC-13.3.0</td>
</tr>
<tr>
<td>OpenBabel</td>
<td>3.1.1-gompi-2022b</td>
</tr>
<tr>
<td>OpenCV</td>
<td>4.5.1-foss-2020b-contrib,4.8.0-foss-2022b-CUDA-12.0.0-contrib</td>
</tr>
<tr>
<td>OpenEXR</td>
<td>2.5.5-GCCcore-10.2.0,3.1.5-GCCcore-12.2.0</td>
</tr>
<tr>
<td>OpenFOAM</td>
<td>v2012-foss-2020b,v2206-foss-2020b,v2212-foss-2022b</td>
</tr>
<tr>
<td>OpenJPEG</td>
<td>2.4.0-GCCcore-10.2.0,2.5.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>OpenLibm</td>
<td>0.7.5-GCC-10.2.0</td>
</tr>
<tr>
<td>OpenMM</td>
<td>7.5.0-fosscuda-2020b,7.5.1-foss-2020b-CUDA-11.3.1-AlphaFold-patch,7.5.1-foss-2020b,7.5.1-fosscuda-2020b,7.7.0-foss-2020b-CUDA-11.3.1,8.0.0-foss-2022b-CUDA-12.1.1</td>
</tr>
<tr>
<td>OpenMPI</td>
<td>4.0.5-GCC-10.2.0-CUDA-11.3.1,4.0.5-GCC-10.2.0,4.0.5-gcccuda-2020b,4.0.5-iccifort-2020.4.304,4.0.5-NVHPC-21.11-CUDA-11.1.1,4.1.4-intel-compilers-2022.2.1,4.1.4-NVHPC-23.1-CUDA-12.0.0,4.1.4-GCC-12.2.0</td>
</tr>
<tr>
<td>OpenPGM</td>
<td>5.2.122-GCCcore-10.2.0,5.2.122-GCCcore-12.2.0</td>
</tr>
<tr>
<td>OpenSSL</td>
<td>1.0,1.1,3</td>
</tr>
<tr>
<td>OpenSlide</td>
<td>3.4.1-GCCcore-12.2.0-largefiles</td>
</tr>
<tr>
<td>OpenSlide-Java</td>
<td>0.12.4-GCCcore-12.2.0-Java-17</td>
</tr>
<tr>
<td>OrthoFinder</td>
<td>2.5.4-foss-2020b</td>
</tr>
<tr>
<td>Osi</td>
<td>0.108.8-GCC-12.2.0</td>
</tr>
<tr>
<td>PALEOMIX</td>
<td>1.3.8-foss-2020b</td>
</tr>
<tr>
<td>PAML</td>
<td>4.10.7-GCCcore-12.2.0</td>
</tr>
<tr>
<td>PBZIP2</td>
<td>1.1.13-GCCcore-12.2.0</td>
</tr>
<tr>
<td>PCRE</td>
<td>8.44-GCCcore-10.2.0,8.45-GCCcore-12.2.0</td>
</tr>
<tr>
<td>PCRE2</td>
<td>10.35-GCCcore-10.2.0,10.40-GCCcore-12.2.0</td>
</tr>
<tr>
<td>PDBFixer</td>
<td>1.7-fosscuda-2020b</td>
</tr>
<tr>
<td>PEAR</td>
<td>0.9.11-GCCcore-10.2.0</td>
</tr>
<tr>
<td>PEET</td>
<td>1.15.0,1.16.0a</td>
</tr>
<tr>
<td>PETSc</td>
<td>3.15.0-foss-2020b,3.17.4-foss-2022b,3.20.3-foss-2022b</td>
</tr>
<tr>
<td>PGI</td>
<td>18.10-GCCcore-10.2.0,18.10-GCCcore-12.2.0</td>
</tr>
<tr>
<td>PIPseeker</td>
<td>2.1.4</td>
</tr>
<tr>
<td>PKTOOLS</td>
<td>2.6.7.6-foss-2020b,2.6.7.6-foss-2022b</td>
</tr>
<tr>
<td>PLINK</td>
<td>1.9b_6.21-x86_64,2_avx2_20221024</td>
</tr>
<tr>
<td>PLUMED</td>
<td>2.6.2-intel-2020b,2.7.0-foss-2020b,2.7.3-foss-2020b,2.9.0-foss-2022b,2.9.2-foss-2022b</td>
</tr>
<tr>
<td>PMIx</td>
<td>5.0.2-GCCcore-13.3.0</td>
</tr>
<tr>
<td>POV-Ray</td>
<td>3.7.0.8-GCC-10.2.0,3.7.0.10-GCC-12.2.0</td>
</tr>
<tr>
<td>PRINSEQ</td>
<td>0.20.4-foss-2020b-Perl-5.32.0</td>
</tr>
<tr>
<td>PROJ</td>
<td>7.2.1-GCCcore-10.2.0,9.1.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>PRRTE</td>
<td>3.0.5-GCCcore-13.3.0</td>
</tr>
<tr>
<td>PYTHIA</td>
<td>8.309-foss-2022b</td>
</tr>
<tr>
<td>Pandoc</td>
<td>2.13,3.1.2</td>
</tr>
<tr>
<td>Pango</td>
<td>1.47.0-GCCcore-10.2.0,1.50.12-GCCcore-12.2.0</td>
</tr>
<tr>
<td>ParMETIS</td>
<td>4.0.3-gompi-2022b</td>
</tr>
<tr>
<td>ParaView</td>
<td>5.8.1-foss-2020b-mpi,5.11.0-foss-2022b-mpi</td>
</tr>
<tr>
<td>PartitionFinder</td>
<td>2.1.1-foss-2020b-Python-2.7.18</td>
</tr>
<tr>
<td>Perl</td>
<td>5.28.0-GCCcore-7.3.0,5.32.0-GCCcore-10.2.0-minimal,5.32.0-GCCcore-10.2.0,5.32.1-GCCcore-10.2.0,5.36.0-GCCcore-12.2.0-minimal,5.36.0-GCCcore-12.2.0,5.36.1-GCCcore-12.2.0,5.38.0,5.38.2-GCCcore-13.3.0</td>
</tr>
<tr>
<td>Perl-bundle-CPAN</td>
<td>5.36.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Phenix</td>
<td>1.20.1-4487-cryo_fit,1.20.1-4487</td>
</tr>
<tr>
<td>PhyloBayes</td>
<td>4.1e-GCCcore-10.2.0</td>
</tr>
<tr>
<td>Pillow</td>
<td>8.0.1-GCCcore-10.2.0,9.4.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Pillow-SIMD</td>
<td>7.1.2-GCCcore-10.2.0,9.5.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Pint</td>
<td>0.22-GCCcore-12.2.0</td>
</tr>
<tr>
<td>PnetCDF</td>
<td>1.12.2-gompic-2020b,1.12.3-gompi-2022b,1.13.0-iimpi-2022b,1.13.0-iompi-2022b</td>
</tr>
<tr>
<td>PostgreSQL</td>
<td>13.2-GCCcore-10.2.0,15.2-GCCcore-12.2.0</td>
</tr>
<tr>
<td>PuLP</td>
<td>2.7.0-foss-2022b</td>
</tr>
<tr>
<td>PyBLP</td>
<td>1.1.0-foss-2020b</td>
</tr>
<tr>
<td>PyBerny</td>
<td>0.6.3-foss-2022b</td>
</tr>
<tr>
<td>PyCairo</td>
<td>1.24.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>PyCharm</td>
<td>2022.3.2</td>
</tr>
<tr>
<td>PyCheMPS2</td>
<td>1.8.12-foss-2022b</td>
</tr>
<tr>
<td>PyGObject</td>
<td>3.44.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>PyInstaller</td>
<td>6.3.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>PyOpenGL</td>
<td>3.1.5-GCCcore-10.2.0,3.1.6-GCCcore-12.2.0</td>
</tr>
<tr>
<td>PyQt5</td>
<td>5.15.4-GCCcore-10.2.0,5.15.7-GCCcore-12.2.0</td>
</tr>
<tr>
<td>PySCF</td>
<td>2.4.0-foss-2022b</td>
</tr>
<tr>
<td>PyTables</td>
<td>3.5.2-foss-2020b-Python-2.7.18,3.8.0-foss-2022b</td>
</tr>
<tr>
<td>PyTorch</td>
<td>1.9.0-fosscuda-2020b,1.13.1-foss-2022b-CUDA-12.0.0,2.1.2-foss-2022b-CUDA-12.0.0,2.1.2-foss-2022b-CUDA-12.1.1</td>
</tr>
<tr>
<td>PyYAML</td>
<td>5.3.1-GCCcore-10.2.0,6.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>PycURL</td>
<td>7.45.2-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Pylada-light</td>
<td>2023Oct13-gfbf-2022b</td>
</tr>
<tr>
<td>Pysam</td>
<td>0.16.0.1-GCC-10.2.0-Python-2.7.18,0.16.0.1-GCC-10.2.0,0.16.0.1-GCCcore-10.2.0,0.21.0-GCC-12.2.0</td>
</tr>
<tr>
<td>Python</td>
<td>2.7.18-GCCcore-10.2.0,2.7.18-GCCcore-12.2.0-bare,3.8.6-GCCcore-10.2.0-clean,3.8.6-GCCcore-10.2.0,3.10.8-GCCcore-12.2.0-bare,3.10.8-GCCcore-12.2.0-minimal,3.10.8-GCCcore-12.2.0-noclikit,3.10.8-GCCcore-12.2.0,3.12.3-GCCcore-13.3.0</td>
</tr>
<tr>
<td>Python-bundle-PyPI</td>
<td>2023.06-GCCcore-12.2.0,2024.06-GCCcore-13.3.0</td>
</tr>
<tr>
<td>Q-Chem</td>
<td>5.4-openmp</td>
</tr>
<tr>
<td>QCA</td>
<td>2.3.5-GCCcore-12.2.0</td>
</tr>
<tr>
<td>QScintilla</td>
<td>2.11.6-GCCcore-12.2.0</td>
</tr>
<tr>
<td>QTLtools</td>
<td>1.3.1-foss-2020b</td>
</tr>
<tr>
<td>Qhull</td>
<td>2020.2-GCCcore-10.2.0,2020.2-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Qt5</td>
<td>5.14.2-GCCcore-10.2.0,5.15.7-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Qt5Webkit</td>
<td>5.212.0-alpha4-GCCcore-10.2.0,5.212.0-alpha4-GCCcore-12.2.0</td>
</tr>
<tr>
<td>QtKeychain</td>
<td>0.13.2-GCCcore-12.2.0</td>
</tr>
<tr>
<td>QtPy</td>
<td>2.3.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Qtconsole</td>
<td>5.4.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>QuPath</td>
<td>0.5.0-GCCcore-12.2.0-Java-17,0.5.1-GCCcore-12.2.0-Java-17</td>
</tr>
<tr>
<td>QuantumESPRESSO</td>
<td>6.8-intel-2020b,7.0-intel-2020b,7.2-intel-2020b</td>
</tr>
<tr>
<td>Quip</td>
<td>1.1.8-GCCcore-10.2.0,1.1.8-GCCcore-12.2.0,20171217-GCCcore-10.2.0</td>
</tr>
<tr>
<td>Qwt</td>
<td>6.1.5-GCCcore-10.2.0,6.2.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>R</td>
<td>4.2.0-foss-2020b,4.3.2-foss-2022b-bare,4.3.2-foss-2022b,4.4.1-foss-2022b-bare,4.4.1-foss-2022b</td>
</tr>
<tr>
<td>R-INLA</td>
<td>24.01.18-foss-2022b</td>
</tr>
<tr>
<td>R-bundle-Bioconductor</td>
<td>3.15-foss-2020b-R-4.2.0,3.16-foss-2022b-R-4.2.3,3.18-foss-2022b-R-4.3.2,3.19-foss-2022b-R-4.4.1</td>
</tr>
<tr>
<td>R-bundle-CRAN</td>
<td>2023.12-foss-2022b,2024.06-foss-2022b</td>
</tr>
<tr>
<td>RE2</td>
<td>2023-03-01-GCCcore-12.2.0</td>
</tr>
<tr>
<td>RECON</td>
<td>1.08-GCC-10.2.0</td>
</tr>
<tr>
<td>RELION</td>
<td>3.0.8-fosscuda-2020b,3.1.4-foss-2020b,3.1.4-fosscuda-2020b,3.1.4-intel-2020b,4.0.0-fosscuda-2020b,4.0.1-foss-2022b-CUDA-12.0.0,4.0.1-fosscuda-2020b,5beta-foss-2022b-CUDA-12.0.0-2024-03-13,5beta-2024-05-03-foss-2022b-CUDA-12.0.0</td>
</tr>
<tr>
<td>RMBlast</td>
<td>2.11.0-gompi-2020b</td>
</tr>
<tr>
<td>ROOT</td>
<td>6.26.06-foss-2020b,6.26.10-foss-2022b</td>
</tr>
<tr>
<td>RSEM</td>
<td>1.3.3-foss-2020b</td>
</tr>
<tr>
<td>RStudio</td>
<td>2022.07.2-576,2022.12.0-353,2024.04.2-764</td>
</tr>
<tr>
<td>RStudio-Server</td>
<td>2024.04.1+748-foss-2022b-Java-11-R-4.3.2</td>
</tr>
<tr>
<td>RapidJSON</td>
<td>1.1.0-GCCcore-10.2.0,1.1.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>RepeatMasker</td>
<td>4.1.2-p1-foss-2020b</td>
</tr>
<tr>
<td>RepeatScout</td>
<td>1.0.6-GCC-10.2.0</td>
</tr>
<tr>
<td>ResMap</td>
<td>1.95-cuda-Centos7x64</td>
</tr>
<tr>
<td>RevBayes</td>
<td>1.1.1-GCC-10.2.0,1.2.1-GCC-10.2.0,1.2.2-GCC-12.2.0,1.2.2-gompi-2022b</td>
</tr>
<tr>
<td>Rivet</td>
<td>3.1.9-gompi-2022b-HepMC3-3.2.6</td>
</tr>
<tr>
<td>Rmath</td>
<td>4.0.4-foss-2020b,4.4.1-foss-2022b</td>
</tr>
<tr>
<td>Rosetta</td>
<td>3.12</td>
</tr>
<tr>
<td>Ruby</td>
<td>2.7.2-GCCcore-10.2.0,3.0.5-GCCcore-12.2.0,3.2.2-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Rust</td>
<td>1.52.1-GCCcore-10.2.0,1.65.0-GCCcore-12.2.0,1.70.0-GCCcore-12.2.0,1.75.0-GCCcore-12.2.0,1.78.0-GCCcore-13.3.0</td>
</tr>
<tr>
<td>SAMtools</td>
<td>1.11-GCC-10.2.0,1.11-GCCcore-10.2.0,1.16-GCCcore-10.2.0,1.16.1-GCCcore-10.2.0,1.18-GCC-12.2.0,1.20-GCC-12.2.0,1.21-GCC-12.2.0</td>
</tr>
<tr>
<td>SAS</td>
<td>9.4M8,9.4</td>
</tr>
<tr>
<td>SBGrid</td>
<td>2.11.2,2.11.11</td>
</tr>
<tr>
<td>SCOTCH</td>
<td>6.1.0-gompi-2020b,7.0.3-gompi-2022b</td>
</tr>
<tr>
<td>SCons</td>
<td>4.0.1-GCCcore-10.2.0,4.5.2-GCCcore-12.2.0</td>
</tr>
<tr>
<td>SDL2</td>
<td>2.0.14-GCCcore-10.2.0,2.26.3-GCCcore-12.2.0</td>
</tr>
<tr>
<td>SHAPEIT</td>
<td>2.r904.glibcv2.17</td>
</tr>
<tr>
<td>SHAPEIT4</td>
<td>4.2.2-foss-2022b</td>
</tr>
<tr>
<td>SLEPc</td>
<td>3.15.0-foss-2020b,3.17.2-foss-2022b</td>
</tr>
<tr>
<td>SMRT-Link</td>
<td>11.1.0.166339-cli-tools-only,12.0.0-cli-tools</td>
</tr>
<tr>
<td>SOCI</td>
<td>4.0.3-GCC-12.2.0-Boost-1.83,4.0.3-GCC-12.2.0</td>
</tr>
<tr>
<td>SPAdes</td>
<td>3.15.1-GCCcore-10.2.0-Python-3.8.6</td>
</tr>
<tr>
<td>SPM</td>
<td>12.5_r7771</td>
</tr>
<tr>
<td>SQLite</td>
<td>3.33.0-GCCcore-10.2.0,3.39.4-GCCcore-12.2.0,3.45.3-GCCcore-13.3.0</td>
</tr>
<tr>
<td>SRA-Toolkit</td>
<td>2.10.9-gompi-2020b,3.0.10-gompi-2022b,3.1.1-centos_linux64,3.1.1-gompi-2022b</td>
</tr>
<tr>
<td>STAR</td>
<td>2.7.6a-GCC-10.2.0,2.7.7a-GCCcore-10.2.0,2.7.8a-GCC-10.2.0,2.7.9a-GCC-10.2.0,2.7.11a-GCC-10.2.0,2.7.11a-GCC-12.2.0</td>
</tr>
<tr>
<td>STREAM</td>
<td>5.10-intel-2022b</td>
</tr>
<tr>
<td>SWIG</td>
<td>4.0.2-GCCcore-10.2.0,4.1.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Salmon</td>
<td>1.4.0-gompi-2020b</td>
</tr>
<tr>
<td>Sambamba</td>
<td>0.8.0-GCCcore-10.2.0</td>
</tr>
<tr>
<td>ScaFaCoS</td>
<td>1.0.1-foss-2020b,1.0.4-foss-2022b</td>
</tr>
<tr>
<td>ScaLAPACK</td>
<td>2.1.0-gompi-2020b,2.1.0-gompic-2020b,2.2.0-gompi-2022b-fb,2.2.0-gompi-2024a-fb,2.2.0-nvompi-2023.01-fb</td>
</tr>
<tr>
<td>SciPy-bundle</td>
<td>2020.11-foss-2020b-Python-2.7.18,2020.11-foss-2020b,2020.11-fosscuda-2020b,2020.11-intel-2020b,2020.11-iomkl-2020b,2021.05-foss-2020b,2023.02-gfbf-2022b,2024.05-gfbf-2024a</td>
</tr>
<tr>
<td>Seaborn</td>
<td>0.12.2-foss-2022b,0.13.2-gfbf-2022b</td>
</tr>
<tr>
<td>Seq-Gen</td>
<td>1.3.4-GCCcore-12.2.0</td>
</tr>
<tr>
<td>SeqKit</td>
<td>2.3.1,2.8.1</td>
</tr>
<tr>
<td>Serf</td>
<td>1.3.9-GCCcore-10.2.0,1.3.9-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Shapely</td>
<td>1.8.5.post1-foss-2022b,2.0.1-foss-2022b</td>
</tr>
<tr>
<td>Sherpa</td>
<td>3.0.0-foss-2022b</td>
</tr>
<tr>
<td>Slicer</td>
<td>5.6.2-GCCcore-12.2.0</td>
</tr>
<tr>
<td>SpaceRanger</td>
<td>2.1.1-GCC-12.2.0</td>
</tr>
<tr>
<td>Spark</td>
<td>3.1.1-foss-2020b,3.1.1-fosscuda-2020b,3.5.0-foss-2022b-Scala-2.13,3.5.0-foss-2022b,3.5.1-foss-2022b-Scala-2.13,3.5.3-foss-2022b-Scala-2.13</td>
</tr>
<tr>
<td>SpectrA</td>
<td>1.0.0-GCC-10.2.0,1.0.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Stacks</td>
<td>2.59-GCCcore-10.2.0</td>
</tr>
<tr>
<td>Stata</td>
<td>17</td>
</tr>
<tr>
<td>StringTie</td>
<td>2.1.4-GCCcore-10.2.0</td>
</tr>
<tr>
<td>Subread</td>
<td>2.0.3-GCC-10.2.0</td>
</tr>
<tr>
<td>Subversion</td>
<td>1.14.0-GCCcore-10.2.0,1.14.3-GCCcore-12.2.0</td>
</tr>
<tr>
<td>SuiteSparse</td>
<td>5.8.1-foss-2020b-METIS-5.1.0,5.13.0-foss-2022b-METIS-5.1.0</td>
</tr>
<tr>
<td>Summovie</td>
<td>1.0.2</td>
</tr>
<tr>
<td>SuperLU_DIST</td>
<td>8.1.2-foss-2022b</td>
</tr>
<tr>
<td>Szip</td>
<td>2.1.1-GCCcore-10.2.0,2.1.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>TOMO3D</td>
<td>01-2015</td>
</tr>
<tr>
<td>TOPAS</td>
<td>3.9-foss-2022b</td>
</tr>
<tr>
<td>TRF</td>
<td>4.09.1-GCCcore-10.2.0</td>
</tr>
<tr>
<td>TRUST4</td>
<td>1.0.7-GCC-12.2.0</td>
</tr>
<tr>
<td>TWL-NINJA</td>
<td>0.97-cluster_only-GCC-10.2.0</td>
</tr>
<tr>
<td>Tcl</td>
<td>8.6.10-GCCcore-10.2.0,8.6.12-GCCcore-12.2.0,8.6.14-GCCcore-13.3.0</td>
</tr>
<tr>
<td>TensorFlow</td>
<td>2.5.0-fosscuda-2020b,2.7.1-foss-2020b-CUDA-11.3.1,2.13.0-foss-2022b,2.15.1-foss-2022b-CUDA-12.1.1</td>
</tr>
<tr>
<td>TensorRT</td>
<td>8.6.1-foss-2022b-CUDA-12.0.0</td>
</tr>
<tr>
<td>Tk</td>
<td>8.6.10-GCCcore-10.2.0,8.6.12-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Tkinter</td>
<td>3.8.6-GCCcore-10.2.0,3.10.8-GCCcore-12.2.0</td>
</tr>
<tr>
<td>TopHat</td>
<td>2.1.2-GCC-10.2.0-Python-2.7.18,2.1.2-GCCcore-10.2.0</td>
</tr>
<tr>
<td>TotalView</td>
<td>2023.3.10</td>
</tr>
<tr>
<td>TreeMix</td>
<td>1.13-GCC-10.2.0</td>
</tr>
<tr>
<td>Trilinos</td>
<td>13.4.1-foss-2022b-zoltan</td>
</tr>
<tr>
<td>Trim_Galore</td>
<td>0.6.7-GCCcore-10.2.0</td>
</tr>
<tr>
<td>Trimmomatic</td>
<td>0.39-Java-11</td>
</tr>
<tr>
<td>UCC</td>
<td>1.1.0-GCCcore-12.2.0,1.3.0-GCCcore-13.3.0</td>
</tr>
<tr>
<td>UCC-CUDA</td>
<td>1.1.0-GCCcore-12.2.0-CUDA-12.0.0,1.1.0-GCCcore-12.2.0-CUDA-12.1.1,1.3.0-GCCcore-13.3.0-CUDA-12.6.0</td>
</tr>
<tr>
<td>UCX</td>
<td>1.9.0-GCCcore-10.2.0-CUDA-11.1.1,1.9.0-GCCcore-10.2.0,1.10.0-GCCcore-10.2.0,1.13.1-GCCcore-12.2.0,1.16.0-GCCcore-13.3.0</td>
</tr>
<tr>
<td>UCX-CUDA</td>
<td>1.10.0-GCCcore-10.2.0-CUDA-11.3.1,1.13.1-GCCcore-12.2.0-CUDA-11.8.0,1.13.1-GCCcore-12.2.0-CUDA-12.0.0,1.13.1-GCCcore-12.2.0-CUDA-12.1.1,1.16.0-GCCcore-13.3.0-CUDA-12.6.0</td>
</tr>
<tr>
<td>UDUNITS</td>
<td>2.2.26-GCCcore-10.2.0,2.2.28-GCCcore-12.2.0</td>
</tr>
<tr>
<td>USEARCH</td>
<td>11.0.667-i86linux32</td>
</tr>
<tr>
<td>UnZip</td>
<td>6.0-GCCcore-10.2.0,6.0-GCCcore-12.2.0,6.0-GCCcore-13.3.0</td>
</tr>
<tr>
<td>Unblur</td>
<td>1.0.2</td>
</tr>
<tr>
<td>VASP</td>
<td>5.4.1-iomkl-2020b,5.4.4-iomkl-2020b-VTST,5.4.4-iomkl-2020b,6.3.0-iomkl-2020b,6.4.2-iomkl-2020b-Wannier90</td>
</tr>
<tr>
<td>VASPsol</td>
<td>5.4.1-1.0-iomkl-2020b</td>
</tr>
<tr>
<td>VCFtools</td>
<td>0.1.16-GCCcore-10.2.0</td>
</tr>
<tr>
<td>VDJtools</td>
<td>1.2.1</td>
</tr>
<tr>
<td>VEP</td>
<td>107-GCC-10.2.0,110-GCC-10.2.0,112-GCC-12.2.0,112.0</td>
</tr>
<tr>
<td>VESTA</td>
<td>3.5.8-gtk3</td>
</tr>
<tr>
<td>VMD</td>
<td>1.9.4a57-foss-2022b</td>
</tr>
<tr>
<td>VSCode</td>
<td>1.95.3</td>
</tr>
<tr>
<td>VTK</td>
<td>9.0.1-foss-2020b,9.0.1-fosscuda-2020b,9.2.6-foss-2022b</td>
</tr>
<tr>
<td>VTune</td>
<td>2023.2.0</td>
</tr>
<tr>
<td>Valgrind</td>
<td>3.16.1-gompi-2020b,3.21.0-gompi-2022b</td>
</tr>
<tr>
<td>ViennaRNA</td>
<td>2.5.1-foss-2020b</td>
</tr>
<tr>
<td>Vim</td>
<td>9.0.1434-GCCcore-12.2.0</td>
</tr>
<tr>
<td>VisPy</td>
<td>0.12.2-foss-2022b</td>
</tr>
<tr>
<td>Voro++</td>
<td>0.4.6-GCCcore-10.2.0,0.4.6-GCCcore-12.2.0</td>
</tr>
<tr>
<td>WRF</td>
<td>4.4.1-foss-2022b-dmpar</td>
</tr>
<tr>
<td>Wannier90</td>
<td>3.1.0-foss-2022b-serial,3.1.0-iomkl-2020b</td>
</tr>
<tr>
<td>Wayland</td>
<td>1.22.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Waylandpp</td>
<td>1.0.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>WebKitGTK+</td>
<td>2.40.4-GCC-12.2.0</td>
</tr>
<tr>
<td>X11</td>
<td>20201008-GCCcore-10.2.0,20221110-GCCcore-12.2.0</td>
</tr>
<tr>
<td>XCFun</td>
<td>2.1.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>XGBoost</td>
<td>2.1.1-gfbf-2022b-CUDA-12.1.1,2.1.1-gfbf-2022b</td>
</tr>
<tr>
<td>XML-LibXML</td>
<td>2.0206-GCCcore-10.2.0,2.0208-GCCcore-12.2.0</td>
</tr>
<tr>
<td>XZ</td>
<td>5.2.5-GCCcore-10.2.0,5.2.7-GCCcore-12.2.0,5.4.5-GCCcore-13.3.0</td>
</tr>
<tr>
<td>Xerces-C++</td>
<td>3.1.4-GCCcore-6.4.0,3.2.3-GCCcore-10.2.0,3.2.4-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Xvfb</td>
<td>1.20.9-GCCcore-10.2.0,21.1.6-GCCcore-12.2.0</td>
</tr>
<tr>
<td>YODA</td>
<td>1.9.9-GCC-12.2.0</td>
</tr>
<tr>
<td>Yasm</td>
<td>1.3.0-GCCcore-10.2.0,1.3.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Z3</td>
<td>4.8.10-GCCcore-10.2.0,4.10.2-GCCcore-12.2.0,4.12.2-GCCcore-12.2.0-Python-3.10.8,4.12.2-GCCcore-12.2.0</td>
</tr>
<tr>
<td>ZeroMQ</td>
<td>4.3.3-GCCcore-10.2.0,4.3.4-GCCcore-12.2.0</td>
</tr>
<tr>
<td>Zip</td>
<td>3.0-GCCcore-10.2.0,3.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>aiohttp</td>
<td>3.8.5-GCCcore-12.2.0</td>
</tr>
<tr>
<td>alibuild</td>
<td>1.17.11-GCCcore-12.2.0</td>
</tr>
<tr>
<td>angsd</td>
<td>0.940-GCC-12.2.0</td>
</tr>
<tr>
<td>anndata</td>
<td>0.10.5.post1-foss-2022b</td>
</tr>
<tr>
<td>annovar</td>
<td>2019Oct24-GCCcore-10.2.0-Perl-5.32.0,20200607-GCCcore-12.2.0-Perl-5.36.1</td>
</tr>
<tr>
<td>ant</td>
<td>1.10.9-Java-11,1.10.12-Java-11,1.10.12-Java-17</td>
</tr>
<tr>
<td>archspec</td>
<td>0.1.2-GCCcore-10.2.0-Python-3.8.6,0.2.0-GCCcore-12.2.0-Python-3.10.8</td>
</tr>
<tr>
<td>aria2</td>
<td>1.35.0-gompi-2020b,1.36.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>arpack-ng</td>
<td>3.8.0-foss-2020b,3.8.0-foss-2022b,3.8.0-nvofbf-2023.01</td>
</tr>
<tr>
<td>arrow-R</td>
<td>6.0.0.2-foss-2020b-R-4.2.0,11.0.0.3-foss-2022b-R-4.2.3,14.0.0.2-foss-2022b-R-4.3.2,16.1.0-foss-2022b-R-4.4.1</td>
</tr>
<tr>
<td>at-spi2-atk</td>
<td>2.38.0-GCCcore-10.2.0,2.38.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>at-spi2-core</td>
<td>2.38.0-GCCcore-10.2.0,2.46.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>attr</td>
<td>2.4.48-GCCcore-10.2.0</td>
</tr>
<tr>
<td>attrdict3</td>
<td>2.0.2-GCCcore-12.2.0</td>
</tr>
<tr>
<td>awscli</td>
<td>2.1.23-GCCcore-10.2.0-Python-3.8.6,2.13.20-GCCcore-12.2.0,2.15.2-GCCcore-12.2.0</td>
</tr>
<tr>
<td>bases2Fastq</td>
<td>v1.5.1,v1.5.1,v2.0.0</td>
</tr>
<tr>
<td>bcl2fastq2</td>
<td>2.20.0-GCC-12.2.0,2.20.0-GCCcore-10.2.0</td>
</tr>
<tr>
<td>beagle-lib</td>
<td>3.1.2-fosscuda-2020b,3.1.2-GCC-10.2.0,3.1.2-GCCcore-10.2.0,3.1.2-GCCcore-12.2.0,4.0.0-GCC-12.2.0,4.0.1-GCC-12.2.0-CUDA-12.0.0</td>
</tr>
<tr>
<td>binutils</td>
<td>2.28,2.30-GCCcore-7.3.0,2.30,2.35-GCCcore-10.2.0,2.35,2.39-GCCcore-12.2.0,2.39,2.40,2.42-GCCcore-13.3.0,2.42</td>
</tr>
<tr>
<td>biswebnode</td>
<td>1.3.0-GCCcore-10.2.0</td>
</tr>
<tr>
<td>bokeh</td>
<td>2.2.3-foss-2020b,2.2.3-fosscuda-2020b,3.2.1-foss-2022b</td>
</tr>
<tr>
<td>boto3</td>
<td>1.20.13-GCCcore-10.2.0,1.26.163-GCCcore-12.2.0</td>
</tr>
<tr>
<td>breseq</td>
<td>0.35.5-foss-2020b-R-4.2.0,0.38.0-foss-2020b-R-4.2.0,0.38.1-foss-2020b-R-4.2.0</td>
</tr>
<tr>
<td>bsddb3</td>
<td>6.2.9-GCCcore-10.2.0,6.2.9-GCCcore-12.2.0</td>
</tr>
<tr>
<td>bzip2</td>
<td>1.0.8-GCCcore-10.2.0,1.0.8-GCCcore-12.2.0,1.0.8-GCCcore-13.3.0</td>
</tr>
<tr>
<td>c-ares</td>
<td>1.19.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>cURL</td>
<td>7.55.1-GCCcore-6.4.0,7.72.0-GCCcore-10.2.0,7.86.0-GCCcore-10.2.0,7.86.0-GCCcore-12.2.0,8.7.1-GCCcore-13.3.0</td>
</tr>
<tr>
<td>cairo</td>
<td>1.16.0-GCCcore-10.2.0-libpng15,1.16.0-GCCcore-10.2.0,1.17.4-GCCcore-12.2.0</td>
</tr>
<tr>
<td>ccache</td>
<td>4.6.3-GCCcore-12.2.0</td>
</tr>
<tr>
<td>cffi</td>
<td>1.16.0-GCCcore-13.3.0</td>
</tr>
<tr>
<td>code-server</td>
<td>4.91.1,4.95.3</td>
</tr>
<tr>
<td>configurable-http-proxy</td>
<td>4.5.5-GCCcore-12.2.0</td>
</tr>
<tr>
<td>cppy</td>
<td>1.2.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>cromwell</td>
<td>86</td>
</tr>
<tr>
<td>cryptography</td>
<td>41.0.1-GCCcore-12.2.0,42.0.8-GCCcore-13.3.0</td>
</tr>
<tr>
<td>cuDNN</td>
<td>8.0.5.39-CUDA-11.1.1,8.2.1.32-CUDA-11.3.1,8.7.0.84-CUDA-11.8.0,8.8.0.121-CUDA-12.0.0,8.9.2.26-CUDA-12.1.1,9.5.0.50-CUDA-12.6.0</td>
</tr>
<tr>
<td>cuTENSOR</td>
<td>1.7.0.1-CUDA-12.0.0,2.0.2.5-CUDA-12.6.0</td>
</tr>
<tr>
<td>cutadapt</td>
<td>3.4-GCCcore-10.2.0</td>
</tr>
<tr>
<td>cxxopts</td>
<td>3.0.0</td>
</tr>
<tr>
<td>cyrus-sasl</td>
<td>2.1.28-GCCcore-12.2.0</td>
</tr>
<tr>
<td>dSQ</td>
<td>1.05</td>
</tr>
<tr>
<td>dask</td>
<td>2021.2.0-foss-2020b,2021.2.0-fosscuda-2020b,2023.7.1-foss-2022b</td>
</tr>
<tr>
<td>dbus-glib</td>
<td>0.112-GCCcore-12.2.0</td>
</tr>
<tr>
<td>dcm2niix</td>
<td>1.0.20211006-GCCcore-10.2.0,1.0.20230411-GCCcore-12.2.0</td>
</tr>
<tr>
<td>dedalus</td>
<td>3.0.2-foss-2022b</td>
</tr>
<tr>
<td>deepTools</td>
<td>3.5.1-foss-2020b,3.5.5-foss-2022b</td>
</tr>
<tr>
<td>deml</td>
<td>1.1.4-GCCcore-10.2.0</td>
</tr>
<tr>
<td>dill</td>
<td>0.3.7-GCCcore-12.2.0</td>
</tr>
<tr>
<td>dlib</td>
<td>19.22-foss-2020b-CUDA-11.3.1,19.22-foss-2022b-CUDA-12.0.0</td>
</tr>
<tr>
<td>dorado</td>
<td>0.5.3-foss-2022b-CUDA-12.0.0</td>
</tr>
<tr>
<td>dotNET-Core</td>
<td>7.0.410</td>
</tr>
<tr>
<td>dotNET-SDK</td>
<td>3.1.300-linux-x64</td>
</tr>
<tr>
<td>double-conversion</td>
<td>3.1.5-GCCcore-10.2.0,3.2.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>dtcmp</td>
<td>1.1.2-gompi-2020b</td>
</tr>
<tr>
<td>ecBuild</td>
<td>3.8.0</td>
</tr>
<tr>
<td>ecCodes</td>
<td>2.31.0-iomkl-2022b</td>
</tr>
<tr>
<td>einops</td>
<td>0.7.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>elbencho</td>
<td>2.0-3-GCC-10.2.0,3.0-19-GCC-12.2.0</td>
</tr>
<tr>
<td>elfutils</td>
<td>0.183-GCCcore-10.2.0,0.189-GCCcore-12.2.0</td>
</tr>
<tr>
<td>eman</td>
<td></td>
</tr>
<tr>
<td>enchant-2</td>
<td>2.3.3-GCCcore-12.2.0</td>
</tr>
<tr>
<td>ensmallen</td>
<td>2.21.1-foss-2022b</td>
</tr>
<tr>
<td>exiv2</td>
<td>0.27.5-GCCcore-12.2.0,0.28.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>expat</td>
<td>2.2.5-GCCcore-7.3.0,2.2.9-GCCcore-10.2.0,2.4.9-GCCcore-12.2.0,2.6.2-GCCcore-13.3.0</td>
</tr>
<tr>
<td>expecttest</td>
<td>0.1.3-GCCcore-12.2.0</td>
</tr>
<tr>
<td>fastjet</td>
<td>3.4.0-gompi-2022b</td>
</tr>
<tr>
<td>fastjet-contrib</td>
<td>1.049-gompi-2022b</td>
</tr>
<tr>
<td>fastp</td>
<td>0.23.2-GCCcore-10.2.0</td>
</tr>
<tr>
<td>ffnvcodec</td>
<td>11.1.5.2</td>
</tr>
<tr>
<td>file</td>
<td>5.39-GCCcore-10.2.0,5.43-GCCcore-12.2.0</td>
</tr>
<tr>
<td>flatbuffers</td>
<td>1.12.0-GCCcore-10.2.0,23.1.4-GCCcore-12.2.0,23.5.26-GCCcore-12.2.0</td>
</tr>
<tr>
<td>flatbuffers-python</td>
<td>1.12-GCCcore-10.2.0,2.0-GCCcore-12.2.0,23.1.4-GCCcore-12.2.0,23.5.26-GCCcore-12.2.0</td>
</tr>
<tr>
<td>flex</td>
<td>2.6.3,2.6.4-GCCcore-7.3.0,2.6.4-GCCcore-10.2.0,2.6.4-GCCcore-12.2.0,2.6.4-GCCcore-13.3.0,2.6.4</td>
</tr>
<tr>
<td>flit</td>
<td>3.9.0-GCCcore-12.2.0,3.9.0-GCCcore-13.3.0</td>
</tr>
<tr>
<td>fmriprep</td>
<td>23.1.0,23.1.4,23.2.1,24.1.0</td>
</tr>
<tr>
<td>fontconfig</td>
<td>2.13.92-GCCcore-10.2.0,2.14.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>foss</td>
<td>2020b,2022b,2024a</td>
</tr>
<tr>
<td>fosscuda</td>
<td>2020b</td>
</tr>
<tr>
<td>freeglut</td>
<td>3.2.1-GCCcore-10.2.0,3.4.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>freetype</td>
<td>2.10.3-GCCcore-10.2.0-libpng15,2.10.3-GCCcore-10.2.0,2.12.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>gc</td>
<td>8.0.4-GCCcore-10.2.0,8.2.2-GCCcore-10.2.0,8.2.4-GCCcore-12.2.0</td>
</tr>
<tr>
<td>gcccuda</td>
<td>2020b,2022b</td>
</tr>
<tr>
<td>gcloud</td>
<td>382.0.0,494.0.0</td>
</tr>
<tr>
<td>gettext</td>
<td>0.19.8.1,0.21-GCCcore-10.2.0,0.21,0.21.1-GCCcore-12.2.0,0.21.1,0.22.5-GCCcore-13.3.0,0.22.5</td>
</tr>
<tr>
<td>gfbf</td>
<td>2022b,2024a</td>
</tr>
<tr>
<td>gflags</td>
<td>2.2.2-GCCcore-12.2.0</td>
</tr>
<tr>
<td>giflib</td>
<td>5.2.1-GCCcore-10.2.0,5.2.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>git</td>
<td>2.28.0-GCCcore-10.2.0-nodocs,2.30.0-GCCcore-10.2.0-nodocs,2.38.1-GCCcore-12.2.0-nodocs,2.45.1-GCCcore-13.3.0</td>
</tr>
<tr>
<td>git-lfs</td>
<td>3.2.0,3.5.1</td>
</tr>
<tr>
<td>glew</td>
<td>2.1.0-GCCcore-10.2.0,2.2.0-GCCcore-12.2.0-egl</td>
</tr>
<tr>
<td>glib-networking</td>
<td>2.72.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>glibc</td>
<td>2.34-GCCcore-10.2.0</td>
</tr>
<tr>
<td>gmpy2</td>
<td>2.1.0b5-GCC-10.2.0,2.1.5-GCC-12.2.0</td>
</tr>
<tr>
<td>gmsh</td>
<td>4.11.1-foss-2020b,4.11.1-foss-2022b</td>
</tr>
<tr>
<td>gnuplot</td>
<td>5.4.1-GCCcore-10.2.0,5.4.6-GCCcore-12.2.0</td>
</tr>
<tr>
<td>gomkl</td>
<td>2022b</td>
</tr>
<tr>
<td>gompi</td>
<td>2020b,2022b,2024a</td>
</tr>
<tr>
<td>gompic</td>
<td>2020b</td>
</tr>
<tr>
<td>googletest</td>
<td>1.10.0-GCCcore-10.2.0,1.12.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>gperf</td>
<td>3.1-GCCcore-10.2.0,3.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>gperftools</td>
<td>2.14-GCCcore-12.2.0</td>
</tr>
<tr>
<td>gpu_burn</td>
<td>20231110-GCCcore-12.2.0-CUDA-12.0.0</td>
</tr>
<tr>
<td>graphite2</td>
<td>1.3.14-GCCcore-10.2.0,1.3.14-GCCcore-12.2.0</td>
</tr>
<tr>
<td>groff</td>
<td>1.22.4-GCCcore-10.2.0,1.22.4-GCCcore-12.2.0</td>
</tr>
<tr>
<td>grpcio</td>
<td>1.59.3-GCCcore-12.2.0</td>
</tr>
<tr>
<td>gsutil</td>
<td>4.42,5.10-GCCcore-12.2.0</td>
</tr>
<tr>
<td>gzip</td>
<td>1.10-GCCcore-10.2.0,1.12-GCCcore-12.2.0,1.13-GCCcore-13.3.0</td>
</tr>
<tr>
<td>h5py</td>
<td>3.1.0-foss-2020b,3.1.0-fosscuda-2020b,3.2.1-foss-2020b,3.8.0-foss-2022b</td>
</tr>
<tr>
<td>hatchling</td>
<td>1.18.0-GCCcore-12.2.0,1.24.2-GCCcore-13.3.0</td>
</tr>
<tr>
<td>help2man</td>
<td>1.47.4-GCCcore-7.3.0,1.47.16-GCCcore-10.2.0,1.49.2-GCCcore-12.2.0,1.49.3-GCCcore-13.3.0</td>
</tr>
<tr>
<td>hiredis</td>
<td>1.2.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>hmmlearn</td>
<td>0.3.0-foss-2022b</td>
</tr>
<tr>
<td>hunspell</td>
<td>1.7.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>hwloc</td>
<td>2.2.0-GCCcore-10.2.0,2.8.0-GCCcore-12.2.0,2.10.0-GCCcore-13.3.0</td>
</tr>
<tr>
<td>hypothesis</td>
<td>5.41.2-GCCcore-10.2.0,5.41.5-GCCcore-10.2.0,6.1.1-GCCcore-10.2.0,6.68.2-GCCcore-12.2.0,6.103.1-GCCcore-13.3.0</td>
</tr>
<tr>
<td>iccifort</td>
<td>2020.4.304</td>
</tr>
<tr>
<td>igraph</td>
<td>0.9.5-foss-2022b,0.10.4-foss-2020b,0.10.4-NVHPC-21.11,0.10.6-NVHPC-23.1-CUDA-12.0.0,0.10.6-nvofbf-2023.01,0.10.10-foss-2022b</td>
</tr>
<tr>
<td>iimkl</td>
<td>2022b</td>
</tr>
<tr>
<td>iimpi</td>
<td>2020b,2022b,2024a</td>
</tr>
<tr>
<td>imageio</td>
<td>2.9.0-foss-2020b,2.31.1-foss-2022b</td>
</tr>
<tr>
<td>imgaug</td>
<td>0.4.0-foss-2022b-CUDA-12.0.0</td>
</tr>
<tr>
<td>imkl</td>
<td>2020.4.304-gompi-2020b,2020.4.304-iimpi-2020b,2020.4.304-iompi-2020b,2022.2.1-gompi-2022b,2022.2.1,2024.2.0</td>
</tr>
<tr>
<td>imkl-FFTW</td>
<td>2022.2.1-iimpi-2022b,2024.2.0-iimpi-2024a</td>
</tr>
<tr>
<td>impi</td>
<td>2019.9.304-iccifort-2020.4.304,2021.7.1-intel-compilers-2022.2.1,2021.13.0-intel-compilers-2024.2.0</td>
</tr>
<tr>
<td>inih</td>
<td>57-GCCcore-12.2.0</td>
</tr>
<tr>
<td>intel</td>
<td>2020b,2022b,2024a</td>
</tr>
<tr>
<td>intel-compilers</td>
<td>2022.2.1,2024.2.0</td>
</tr>
<tr>
<td>intltool</td>
<td>0.51.0-GCCcore-10.2.0,0.51.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>iomkl</td>
<td>2020b,2022b</td>
</tr>
<tr>
<td>iompi</td>
<td>2020b,2022b</td>
</tr>
<tr>
<td>jax</td>
<td>0.2.19-fosscuda-2020b,0.3.25-foss-2020b-CUDA-11.3.1,0.4.25-foss-2022b,0.4.25-gfbf-2022b-CUDA-12.1.1</td>
</tr>
<tr>
<td>jbigkit</td>
<td>2.1-GCCcore-10.2.0,2.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>jemalloc</td>
<td>5.2.1-GCCcore-10.2.0,5.3.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>json-c</td>
<td>0.16-GCCcore-12.2.0</td>
</tr>
<tr>
<td>jupyter-resource-usage</td>
<td>1.0.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>jupyter-server</td>
<td>2.7.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>jupyter-server-proxy</td>
<td>3.2.2-GCCcore-12.2.0</td>
</tr>
<tr>
<td>jupyterlmod</td>
<td>4.0.3-GCCcore-12.2.0</td>
</tr>
<tr>
<td>kallisto</td>
<td>0.48.0-foss-2020b</td>
</tr>
<tr>
<td>kim-api</td>
<td>2.2.1-GCCcore-10.2.0,2.3.0-GCC-12.2.0</td>
</tr>
<tr>
<td>kineto</td>
<td>0.4.0-GCC-12.2.0</td>
</tr>
<tr>
<td>leidenalg</td>
<td>0.8.8-foss-2022b,0.10.2-foss-2022b</td>
</tr>
<tr>
<td>lftp</td>
<td>4.9.2-GCCcore-10.2.0</td>
</tr>
<tr>
<td>libGDSII</td>
<td>0.21-GCCcore-10.2.0</td>
</tr>
<tr>
<td>libGLU</td>
<td>9.0.1-GCCcore-10.2.0,9.0.2-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libGridXC</td>
<td>0.9.6-gompi-2020b</td>
</tr>
<tr>
<td>libPSML</td>
<td>1.1.10-GCC-10.2.0</td>
</tr>
<tr>
<td>libRmath</td>
<td>4.1.0-GCCcore-10.2.0</td>
</tr>
<tr>
<td>libXp</td>
<td>1.0.3-foss-2020b</td>
</tr>
<tr>
<td>libaec</td>
<td>1.0.6-GCCcore-10.2.0,1.0.6-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libaio</td>
<td>0.3.112-GCCcore-10.2.0,0.3.113-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libarchive</td>
<td>3.4.3-GCCcore-10.2.0,3.6.1-GCCcore-12.2.0,3.7.4-GCCcore-13.3.0</td>
</tr>
<tr>
<td>libavif</td>
<td>0.11.1-foss-2022b,0.11.1-GCC-12.2.0</td>
</tr>
<tr>
<td>libcerf</td>
<td>1.14-GCCcore-10.2.0,2.3-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libcifpp</td>
<td>5.0.6-GCCcore-10.2.0,7.0.3-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libcint</td>
<td>5.5.0-gfbf-2022b</td>
</tr>
<tr>
<td>libcircle</td>
<td>0.3-gompi-2020b</td>
</tr>
<tr>
<td>libctl</td>
<td>4.5.1-GCCcore-10.2.0</td>
</tr>
<tr>
<td>libdap</td>
<td>3.20.11-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libdeflate</td>
<td>1.7-GCCcore-10.2.0,1.15-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libdrm</td>
<td>2.4.102-GCCcore-10.2.0,2.4.114-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libepoxy</td>
<td>1.5.4-GCCcore-10.2.0,1.5.10-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libev</td>
<td>4.33-GCC-12.2.0</td>
</tr>
<tr>
<td>libevent</td>
<td>2.1.12-GCCcore-10.2.0,2.1.12-GCCcore-12.2.0,2.1.12-GCCcore-13.3.0</td>
</tr>
<tr>
<td>libexif</td>
<td>0.6.24-GCCcore-10.2.0,0.6.24-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libfabric</td>
<td>1.11.0-GCCcore-10.2.0,1.16.1-GCCcore-12.2.0,1.21.0-GCCcore-13.3.0</td>
</tr>
<tr>
<td>libffi</td>
<td>3.3-GCCcore-10.2.0,3.4.4-GCCcore-12.2.0,3.4.5-GCCcore-13.3.0</td>
</tr>
<tr>
<td>libgcrypt</td>
<td>1.10.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libgd</td>
<td>2.3.0-GCCcore-10.2.0,2.3.1-GCCcore-10.2.0,2.3.3-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libgdiplus</td>
<td>6.1-GCCcore-10.2.0,6.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libgeotiff</td>
<td>1.6.0-GCCcore-10.2.0,1.7.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libgit2</td>
<td>1.1.0-GCCcore-10.2.0,1.5.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libglvnd</td>
<td>1.3.2-GCCcore-10.2.0,1.6.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libgpg-error</td>
<td>1.46-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libharu</td>
<td>2.3.0-GCCcore-10.2.0</td>
</tr>
<tr>
<td>libiconv</td>
<td>1.16-GCCcore-10.2.0,1.17-GCCcore-12.2.0,1.17-GCCcore-13.3.0</td>
</tr>
<tr>
<td>libidn</td>
<td>1.41-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libidn2</td>
<td>2.3.0-GCCcore-10.2.0,2.3.2-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libjpeg-turbo</td>
<td>2.0.5-GCCcore-10.2.0,2.1.4-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libleidenalg</td>
<td>0.11.1-foss-2022b,0.11.1-NVHPC-23.1-CUDA-12.0.0,0.11.1-nvofbf-2023.01</td>
</tr>
<tr>
<td>libmcfp</td>
<td>1.2.2-GCCcore-10.2.0,1.3.3-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libnsl</td>
<td>2.0.0-GCCcore-10.2.0</td>
</tr>
<tr>
<td>libogg</td>
<td>1.3.4-GCCcore-10.2.0,1.3.5-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libopus</td>
<td>1.3.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libpci</td>
<td>3.7.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libpciaccess</td>
<td>0.16-GCCcore-10.2.0,0.17-GCCcore-12.2.0,0.18.1-GCCcore-13.3.0</td>
</tr>
<tr>
<td>libpng</td>
<td>1.2.59,1.5.30,1.6.37-GCCcore-10.2.0,1.6.38-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libpsl</td>
<td>0.21.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libreadline</td>
<td>8.0-GCCcore-10.2.0,8.2-GCCcore-12.2.0,8.2-GCCcore-13.3.0</td>
</tr>
<tr>
<td>librsvg</td>
<td>2.51.2-GCCcore-10.2.0</td>
</tr>
<tr>
<td>librttopo</td>
<td>1.1.0-GCC-12.2.0</td>
</tr>
<tr>
<td>libsigc++</td>
<td>2.10.8-GCCcore-10.2.0</td>
</tr>
<tr>
<td>libsndfile</td>
<td>1.0.28-GCCcore-10.2.0,1.2.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libsodium</td>
<td>1.0.18-GCCcore-10.2.0,1.0.18-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libspatialindex</td>
<td>1.9.3-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libspatialite</td>
<td>5.0.1-GCC-12.2.0</td>
</tr>
<tr>
<td>libtasn1</td>
<td>4.19.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libtirpc</td>
<td>1.3.1-GCCcore-10.2.0,1.3.3-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libtool</td>
<td>2.4.6-GCCcore-10.2.0,2.4.7-GCCcore-12.2.0,2.4.7-GCCcore-13.3.0</td>
</tr>
<tr>
<td>libunistring</td>
<td>0.9.10-GCCcore-10.2.0,1.1-GCCcore-10.2.0,1.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libunwind</td>
<td>1.4.0-GCCcore-10.2.0,1.6.2-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libvorbis</td>
<td>1.3.7-GCCcore-10.2.0,1.3.7-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libwebkitgtk-1.0</td>
<td>1.2.4.9-3</td>
</tr>
<tr>
<td>libwebp</td>
<td>1.1.0-GCCcore-10.2.0,1.3.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libwpe</td>
<td>1.14.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libxc</td>
<td>4.3.4-GCC-10.2.0,4.3.4-iccifort-2020.4.304,5.1.2-iccifort-2020.4.304,5.1.5-iccifort-2020.4.304,6.1.0-GCC-12.2.0,6.1.0-intel-compilers-2022.2.1</td>
</tr>
<tr>
<td>libxml++</td>
<td>2.40.1-GCCcore-10.2.0</td>
</tr>
<tr>
<td>libxml2</td>
<td>2.9.10-GCCcore-10.2.0,2.9.14-GCCcore-12.2.0,2.10.3-GCCcore-12.2.0,2.12.7-GCCcore-13.3.0</td>
</tr>
<tr>
<td>libxslt</td>
<td>1.1.34-GCCcore-10.2.0,1.1.37-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libxsmm</td>
<td>1.16.1-iccifort-2020.4.304</td>
</tr>
<tr>
<td>libyaml</td>
<td>0.2.5-GCCcore-10.2.0,0.2.5-GCCcore-12.2.0</td>
</tr>
<tr>
<td>libzip</td>
<td>1.9.2-GCCcore-12.2.0</td>
</tr>
<tr>
<td>liftOver</td>
<td>2023-05-23</td>
</tr>
<tr>
<td>lpsolve</td>
<td>5.5.2.11-GCC-10.2.0</td>
</tr>
<tr>
<td>lwgrp</td>
<td>1.0.3-gompi-2020b</td>
</tr>
<tr>
<td>lxml</td>
<td>4.9.2-GCCcore-12.2.0</td>
</tr>
<tr>
<td>lz4</td>
<td>1.9.2-GCCcore-10.2.0,1.9.4-GCCcore-12.2.0,1.9.4-GCCcore-13.3.0</td>
</tr>
<tr>
<td>maeparser</td>
<td>1.3.1-gompi-2022b</td>
</tr>
<tr>
<td>magma</td>
<td>2.5.4-fosscuda-2020b,2.7.1-foss-2022b-CUDA-12.0.0,2.7.1-foss-2022b-CUDA-12.1.1</td>
</tr>
<tr>
<td>make</td>
<td>4.3-GCCcore-10.2.0,4.3-GCCcore-12.2.0,4.4.1-GCCcore-12.2.0,4.4.1-GCCcore-13.3.0</td>
</tr>
<tr>
<td>makeinfo</td>
<td>6.7-GCCcore-10.2.0-minimal,6.7-GCCcore-10.2.0,7.0.3-GCCcore-12.2.0</td>
</tr>
<tr>
<td>mapDamage</td>
<td>2.2.1-foss-2020b</td>
</tr>
<tr>
<td>matlab-proxy</td>
<td>0.12.1-GCCcore-12.2.0,0.13.1-GCCcore-12.2.0,0.14.0-GCCcore-12.2.0,0.15.1-GCCcore-12.2.0,0.18.2-GCCcore-12.2.0,0.19.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>matplotlib</td>
<td>3.3.3-foss-2020b,3.3.3-fosscuda-2020b,3.3.3-intel-2020b,3.7.0-gfbf-2022b</td>
</tr>
<tr>
<td>maturin</td>
<td>1.1.0-GCCcore-12.2.0,1.4.0-GCCcore-12.2.0-Rust-1.75.0,1.6.0-GCCcore-13.3.0</td>
</tr>
<tr>
<td>meson-python</td>
<td>0.11.0-GCCcore-12.2.0,0.15.0-GCCcore-12.2.0,0.16.0-GCCcore-13.3.0</td>
</tr>
<tr>
<td>mfold_util</td>
<td>4.7-GCCcore-10.2.0</td>
</tr>
<tr>
<td>miniconda</td>
<td>22.9.0,22.11.1,23.1.0,23.3.1,23.5.2,24.3.0-miniforge,24.3.0,24.7.1,24.9.2</td>
</tr>
<tr>
<td>minimap2</td>
<td>2.22-GCCcore-10.2.0</td>
</tr>
<tr>
<td>minizip</td>
<td>1.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>ml_dtypes</td>
<td>0.3.1-gfbf-2022b</td>
</tr>
<tr>
<td>mlpack</td>
<td>4.3.0-foss-2022b</td>
</tr>
<tr>
<td>mm-common</td>
<td>1.0.4-GCCcore-10.2.0</td>
</tr>
<tr>
<td>mongolite</td>
<td>20240424-foss-2022b-R-4.2.3,20240424-foss-2022b-R-4.3.2</td>
</tr>
<tr>
<td>motif</td>
<td>2.3.8-GCCcore-10.2.0,2.3.8-GCCcore-12.2.0</td>
</tr>
<tr>
<td>mpi4py</td>
<td>3.1.4-gompi-2022b</td>
</tr>
<tr>
<td>mpifileutils</td>
<td>0.11.1-gompi-2020b</td>
</tr>
<tr>
<td>mrc</td>
<td>1.3.6-GCCcore-10.2.0,1.3.13-GCCcore-12.2.0</td>
</tr>
<tr>
<td>mrcfile</td>
<td>1.3.0-fosscuda-2020b,1.5.0-foss-2022b</td>
</tr>
<tr>
<td>muParser</td>
<td>2.3.4-GCCcore-12.2.0</td>
</tr>
<tr>
<td>nanobind</td>
<td>2.1.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>napari</td>
<td>0.4.18-foss-2022b</td>
</tr>
<tr>
<td>nbclassic</td>
<td>1.0.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>ncbi-vdb</td>
<td>2.10.9-gompi-2020b,3.0.10-gompi-2022b,3.1.1-gompi-2022b</td>
</tr>
<tr>
<td>ncdu</td>
<td>1.18-GCC-12.2.0</td>
</tr>
<tr>
<td>ncompress</td>
<td>4.2.4.6-GCCcore-10.2.0</td>
</tr>
<tr>
<td>ncurses</td>
<td>5.9-GCC-10.2.0,5.9,6.0,6.2-GCCcore-10.2.0,6.2,6.3-GCCcore-12.2.0,6.3,6.5-GCCcore-13.3.0,6.5</td>
</tr>
<tr>
<td>ncview</td>
<td>2.1.8-gompi-2022b,2.1.8-iompi-2022b</td>
</tr>
<tr>
<td>nedit-ng</td>
<td>2020.1-GCCcore-10.2.0</td>
</tr>
<tr>
<td>netCDF</td>
<td>4.6.1-iomkl-2020b,4.7.4-gompi-2020b,4.7.4-gompic-2020b,4.7.4-iimpi-2020b,4.7.4-iompi-2020b,4.9.0-gompi-2022b,4.9.0-iimpi-2022b,4.9.0-iompi-2022b</td>
</tr>
<tr>
<td>netCDF-C++</td>
<td>4.2-iomkl-2020b</td>
</tr>
<tr>
<td>netCDF-C++4</td>
<td>4.3.1-iimpi-2022b,4.3.1-iompi-2022b</td>
</tr>
<tr>
<td>netCDF-Fortran</td>
<td>4.4.4-iomkl-2020b,4.5.3-gompi-2020b,4.5.3-gompic-2020b,4.5.3-iimpi-2020b,4.5.3-iompi-2020b,4.6.0-gompi-2022b,4.6.0-iimpi-2022b,4.6.0-iompi-2022b</td>
</tr>
<tr>
<td>netcdf4-python</td>
<td>1.6.3-foss-2022b</td>
</tr>
<tr>
<td>nettle</td>
<td>3.6-GCCcore-10.2.0,3.8.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>networkx</td>
<td>2.5-foss-2020b,2.5-fosscuda-2020b,2.5.1-foss-2020b,3.0-gfbf-2022b</td>
</tr>
<tr>
<td>nf-core</td>
<td>2.14.1-foss-2022b</td>
</tr>
<tr>
<td>nghttp2</td>
<td>1.48.0-GCC-12.2.0</td>
</tr>
<tr>
<td>nghttp3</td>
<td>0.6.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>ngtcp2</td>
<td>0.7.0-GCC-12.2.0</td>
</tr>
<tr>
<td>nlohmann_json</td>
<td>3.11.2-GCCcore-12.2.0</td>
</tr>
<tr>
<td>nodejs</td>
<td>12.19.0-GCCcore-10.2.0,18.12.1-GCCcore-12.2.0,20.11.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>nsync</td>
<td>1.24.0-GCCcore-10.2.0,1.26.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>numactl</td>
<td>2.0.13-GCCcore-10.2.0,2.0.16-GCCcore-12.2.0,2.0.18-GCCcore-13.3.0</td>
</tr>
<tr>
<td>numba</td>
<td>0.58.1-foss-2022b</td>
</tr>
<tr>
<td>nvofbf</td>
<td>2023.01</td>
</tr>
<tr>
<td>nvompi</td>
<td>2023.01</td>
</tr>
<tr>
<td>occt</td>
<td>7.5.0p1-foss-2020b,7.5.0p1-foss-2022b</td>
</tr>
<tr>
<td>p11-kit</td>
<td>0.24.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>p7zip</td>
<td>17.04-GCCcore-12.2.0</td>
</tr>
<tr>
<td>pam-devel</td>
<td>1.3.1</td>
</tr>
<tr>
<td>parallel</td>
<td>20210322-GCCcore-10.2.0</td>
</tr>
<tr>
<td>parameterized</td>
<td>0.9.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>patchelf</td>
<td>0.12-GCCcore-10.2.0,0.17.2-GCCcore-12.2.0,0.18.0-GCCcore-13.3.0</td>
</tr>
<tr>
<td>phonopy</td>
<td>2.27.0-foss-2022b</td>
</tr>
<tr>
<td>phyx</td>
<td>1.3-foss-2020b</td>
</tr>
<tr>
<td>picard</td>
<td>2.18.14-Java-1.8,2.25.6-Java-11</td>
</tr>
<tr>
<td>pigz</td>
<td>2.6-GCCcore-10.2.0,2.7-GCCcore-12.2.0</td>
</tr>
<tr>
<td>pixman</td>
<td>0.40.0-GCCcore-10.2.0,0.42.2-GCCcore-12.2.0</td>
</tr>
<tr>
<td>pkg-config</td>
<td>0.29.2-GCCcore-10.2.0,0.29.2-GCCcore-12.2.0</td>
</tr>
<tr>
<td>pkgconf</td>
<td>1.8.0-GCCcore-10.2.0,1.8.0,1.9.3-GCCcore-12.2.0,2.2.0-GCCcore-13.3.0</td>
</tr>
<tr>
<td>pkgconfig</td>
<td>1.5.1-GCCcore-10.2.0-python,1.5.5-GCCcore-12.2.0-python</td>
</tr>
<tr>
<td>plotly.py</td>
<td>4.14.3-GCCcore-10.2.0,5.13.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>pocl</td>
<td>1.6-GCC-10.2.0,1.8-GCC-12.2.0,5.0-GCC-12.2.0-CUDA-12.0.0</td>
</tr>
<tr>
<td>poetry</td>
<td>1.5.1-GCCcore-12.2.0,1.7.1-GCCcore-12.2.0,1.8.3-GCCcore-13.3.0</td>
</tr>
<tr>
<td>poppler</td>
<td>21.06.1-GCC-10.2.0,21.06.1-intel-2020b,22.12.0-GCC-12.2.0</td>
</tr>
<tr>
<td>popt</td>
<td>1.16-GCC-10.2.0</td>
</tr>
<tr>
<td>postgis</td>
<td>3.4.2-foss-2022b</td>
</tr>
<tr>
<td>printproto</td>
<td>1.0.5-foss-2020b</td>
</tr>
<tr>
<td>prompt-toolkit</td>
<td>3.0.36-GCCcore-12.2.0</td>
</tr>
<tr>
<td>protobuf</td>
<td>3.14.0-GCCcore-10.2.0,3.19.4-GCCcore-12.2.0,23.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>protobuf-python</td>
<td>3.14.0-GCCcore-10.2.0,3.19.4-GCCcore-12.2.0,4.23.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>psycopg2</td>
<td>2.9.9-GCCcore-12.2.0</td>
</tr>
<tr>
<td>pugixml</td>
<td>1.12.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>py-cpuinfo</td>
<td>9.0.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>py3Dmol</td>
<td>2.0.1.post1-GCCcore-10.2.0,2.1.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>pyFFTW</td>
<td>0.13.1-foss-2022b</td>
</tr>
<tr>
<td>pybind11</td>
<td>2.6.0-GCCcore-10.2.0,2.6.2-GCCcore-10.2.0,2.10.3-GCCcore-12.2.0,2.12.0-GCC-13.3.0,2.12.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>pydantic</td>
<td>2.5.3-GCCcore-12.2.0</td>
</tr>
<tr>
<td>pyfaidx</td>
<td>0.7.2.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>pyproj</td>
<td>3.5.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>pytest</td>
<td>7.4.2-GCCcore-12.2.0</td>
</tr>
<tr>
<td>pytest-flakefinder</td>
<td>1.1.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>pytest-rerunfailures</td>
<td>12.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>pytest-shard</td>
<td>0.1.2-GCCcore-12.2.0</td>
</tr>
<tr>
<td>pytest-workflow</td>
<td>2.0.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>pytest-xdist</td>
<td>2.3.0-GCCcore-10.2.0,3.3.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>python-igraph</td>
<td>0.9.8-foss-2022b,0.11.4-foss-2022b</td>
</tr>
<tr>
<td>python-isal</td>
<td>0.11.1-GCCcore-10.2.0</td>
</tr>
<tr>
<td>qrupdate</td>
<td>1.1.2-GCCcore-10.2.0</td>
</tr>
<tr>
<td>rMATS-turbo</td>
<td>4.1.1-foss-2020b,4.1.2-foss-2020b,4.2.0-foss-2022b</td>
</tr>
<tr>
<td>rasterio</td>
<td>1.3.8-foss-2022b</td>
</tr>
<tr>
<td>re2c</td>
<td>2.0.3-GCCcore-10.2.0,3.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>rpmrebuild</td>
<td>2.16,2.18</td>
</tr>
<tr>
<td>ruamel.yaml</td>
<td>0.17.21-GCCcore-10.2.0,0.17.21-GCCcore-12.2.0</td>
</tr>
<tr>
<td>samblaster</td>
<td>0.1.26-GCCcore-10.2.0</td>
</tr>
<tr>
<td>scanpy</td>
<td>1.9.8-foss-2022b</td>
</tr>
<tr>
<td>scikit-build</td>
<td>0.11.1-foss-2020b,0.11.1-fosscuda-2020b,0.17.2-GCCcore-12.2.0,0.17.6-GCCcore-13.3.0</td>
</tr>
<tr>
<td>scikit-build-core</td>
<td>0.9.3-GCCcore-12.2.0</td>
</tr>
<tr>
<td>scikit-image</td>
<td>0.18.1-foss-2020b,0.18.1-fosscuda-2020b,0.18.3-foss-2020b,0.21.0-foss-2022b</td>
</tr>
<tr>
<td>scikit-learn</td>
<td>0.20.4-foss-2020b-Python-2.7.18,0.23.2-foss-2020b,0.23.2-fosscuda-2020b,0.24.1-foss-2020b,1.2.1-gfbf-2022b</td>
</tr>
<tr>
<td>segemehl</td>
<td>0.3.4-GCC-10.2.0</td>
</tr>
<tr>
<td>seqtk</td>
<td>1.3-GCC-10.2.0</td>
</tr>
<tr>
<td>setuptools</td>
<td>64.0.3-GCCcore-12.2.0</td>
</tr>
<tr>
<td>setuptools-rust</td>
<td>1.9.0-GCCcore-13.3.0</td>
</tr>
<tr>
<td>shRNA</td>
<td>0.1-GCC-10.2.0</td>
</tr>
<tr>
<td>siscone</td>
<td>3.0.5-GCCcore-12.2.0</td>
</tr>
<tr>
<td>slurm-drmaa</td>
<td>1.1.3-GCCcore-10.2.0</td>
</tr>
<tr>
<td>snakemake</td>
<td>7.32.3-foss-2022b</td>
</tr>
<tr>
<td>snappy</td>
<td>1.1.8-GCCcore-10.2.0,1.1.9-GCCcore-12.2.0,1.1.10-GCCcore-12.2.0</td>
</tr>
<tr>
<td>sparsehash</td>
<td>2.0.4-GCCcore-10.2.0</td>
</tr>
<tr>
<td>spglib-python</td>
<td>2.0.2-gfbf-2022b,2.3.1-gfbf-2022b</td>
</tr>
<tr>
<td>statsmodels</td>
<td>0.12.1-foss-2020b,0.14.0-gfbf-2022b</td>
</tr>
<tr>
<td>sympy</td>
<td>1.7.1-foss-2020b,1.12-gfbf-2022b</td>
</tr>
<tr>
<td>t-SNE-CUDA</td>
<td>3.0.1-foss-2022b-CUDA-12.0.0</td>
</tr>
<tr>
<td>tabix</td>
<td>0.2.6-GCCcore-10.2.0</td>
</tr>
<tr>
<td>tbb</td>
<td>2020.3-GCCcore-10.2.0,2021.9.0-GCCcore-12.2.0,2021.10.0-GCCcore-12.2.0,2021.13.0-GCCcore-13.3.0</td>
</tr>
<tr>
<td>tcsh</td>
<td>6.22.03-GCCcore-10.2.0,6.24.07-GCCcore-12.2.0</td>
</tr>
<tr>
<td>tensorboard</td>
<td>2.15.1-gfbf-2022b</td>
</tr>
<tr>
<td>tesseract</td>
<td>5.3.0-GCCcore-12.2.0-CUDA-12.0.0,5.3.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>texlive</td>
<td>20220321-GCC-10.2.0,20220321-GCC-12.2.0,20220321-intel-2020b</td>
</tr>
<tr>
<td>time</td>
<td>1.9-GCCcore-12.2.0</td>
</tr>
<tr>
<td>tmux</td>
<td>3.4-GCCcore-12.2.0</td>
</tr>
<tr>
<td>topaz</td>
<td>0.2.5-fosscuda-2020b,0.2.5.20240417-foss-2022b-CUDA-12.0.0</td>
</tr>
<tr>
<td>torchvision</td>
<td>0.10.0-fosscuda-2020b-PyTorch-1.9.0,0.16.0-foss-2022b-CUDA-12.0.0</td>
</tr>
<tr>
<td>tqdm</td>
<td>4.56.2-GCCcore-10.2.0,4.60.0-GCCcore-10.2.0,4.64.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>ttyd</td>
<td>1.7.7</td>
</tr>
<tr>
<td>typing-extensions</td>
<td>3.7.4.3-GCCcore-10.2.0,4.9.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>umap-learn</td>
<td>0.5.3-foss-2022b</td>
</tr>
<tr>
<td>unifdef</td>
<td>2.12-GCCcore-12.2.0</td>
</tr>
<tr>
<td>unrar</td>
<td>7.0.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>utf8proc</td>
<td>2.5.0-GCCcore-10.2.0,2.8.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>util-linux</td>
<td>2.36-GCCcore-10.2.0,2.38.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>virtualenv</td>
<td>20.23.1-GCCcore-12.2.0,20.26.2-GCCcore-13.3.0</td>
</tr>
<tr>
<td>watershed-workflow</td>
<td>1.4.0-foss-2022b-patched,1.4.0-foss-2022b,1.5.0-foss-2022b</td>
</tr>
<tr>
<td>wget</td>
<td>1.20.3-GCCcore-10.2.0</td>
</tr>
<tr>
<td>wpebackend-fdo</td>
<td>1.14.1-GCCcore-12.2.0</td>
</tr>
<tr>
<td>wrapt</td>
<td>1.15.0-gfbf-2022b</td>
</tr>
<tr>
<td>wxPython</td>
<td>4.2.1-foss-2022b</td>
</tr>
<tr>
<td>wxWidgets</td>
<td>3.1.4-GCC-10.2.0,3.1.4-intel-2020b,3.2.0-GCC-10.2.0,3.2.2.1-GCC-12.2.0</td>
</tr>
<tr>
<td>x264</td>
<td>20201026-GCCcore-10.2.0,20230226-GCCcore-12.2.0</td>
</tr>
<tr>
<td>x265</td>
<td>3.3-GCCcore-10.2.0,3.5-GCCcore-12.2.0</td>
</tr>
<tr>
<td>xarray</td>
<td>2023.4.2-foss-2022b,2023.4.2-gfbf-2022b</td>
</tr>
<tr>
<td>xextproto</td>
<td>7.3.0-foss-2020b</td>
</tr>
<tr>
<td>xmlf90</td>
<td>1.5.4-GCC-10.2.0</td>
</tr>
<tr>
<td>xorg-macros</td>
<td>1.19.2-GCCcore-10.2.0,1.19.3-GCCcore-12.2.0,1.20.1-GCCcore-13.3.0</td>
</tr>
<tr>
<td>xprop</td>
<td>1.2.5-GCCcore-10.2.0,1.2.5-GCCcore-12.2.0</td>
</tr>
<tr>
<td>xtb</td>
<td>6.5.1-foss-2020b,6.6.0-foss-2020b,6.6.1-gfbf-2022b</td>
</tr>
<tr>
<td>xxd</td>
<td>8.2.4220-GCCcore-10.2.0,9.0.1696-GCCcore-12.2.0</td>
</tr>
<tr>
<td>yaml-cpp</td>
<td>0.7.0-GCCcore-10.2.0,0.7.0-GCCcore-12.2.0</td>
</tr>
<tr>
<td>ycga-public</td>
<td>1.6.0,1.7.2-GCCcore-12.2.0,1.7.3-GCCcore-12.2.0,1.7.4-GCCcore-12.2.0,1.7.5-GCCcore-12.2.0,1.7.6-GCCcore-12.2.0,1.7.7-GCCcore-12.2.0</td>
</tr>
<tr>
<td>zlib</td>
<td>1.2.11-GCCcore-7.3.0,1.2.11-GCCcore-10.2.0,1.2.11,1.2.12-GCCcore-12.2.0,1.2.12,1.2.13,1.3.1-GCCcore-13.3.0,1.3.1</td>
</tr>
<tr>
<td>zstd</td>
<td>1.4.5-GCCcore-10.2.0,1.5.2-GCCcore-12.2.0,1.5.6-GCCcore-13.3.0</td>
</tr>
</tbody>
</table>
</details>
<h2 id="partitions-and-hardware">Partitions and Hardware</h2>
<p>McCleary is made up of several kinds of compute nodes. We group them into (sometimes overlapping) <a href="/clusters-at-yale/job-scheduling">Slurm partitions</a> meant to serve different purposes. By combining the <code>--partition</code> and <a href="/clusters-at-yale/job-scheduling/resource-requests#features-and-constraints"><code>--constraint</code></a> Slurm options you can more finely control what nodes your jobs can run on.</p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>YCGA sequence data user?  To avoid being charged for your cpu usage for YCGA-related work, make sure to submit jobs to the ycga partition with -p ycga.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Job Submission Limits</p>
<ul>
<li>
<p>You are limited to 4 interactive app instances (of any type) at one time. 
Additional instances will be rejected until you delete older open instances. 
For OnDemand jobs, closing the window does not terminate the interactive app job.
To terminate the job, click the "Delete" button in your "My Interactive Apps" page in the web portal.</p>
</li>
<li>
<p>Job submissions are limited to <strong>200 jobs per hour</strong>. See the Rate Limits section in the <a href="/clusters-at-yale/job-scheduling/common-job-failures/#rate-limits">Common Job Failures</a> page for more info.</p>
</li>
</ul>
</div>
<h3 id="public-partitions">Public Partitions</h3>
<p>See each tab below for more information about the available common use partitions.</p>
<div class="tabbed-set" data-tabs="1:10"><input checked="checked" id="__tabbed_1_1" name="__tabbed_1" type="radio" /><label for="__tabbed_1_1">day</label><div class="tabbed-content">
<p>Use the day partition for most batch jobs. This is the default if you don't specify one with <code>--partition</code>.</p>
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the day partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>1-00:00:00</code></td>
</tr>
<tr>
<td>Maximum CPUs per group</td>
<td><code>512</code></td>
</tr>
<tr>
<td>Maximum memory per group</td>
<td><code>6000G</code></td>
</tr>
<tr>
<td>Maximum CPUs per user</td>
<td><code>256</code></td>
</tr>
<tr>
<td>Maximum memory per user</td>
<td><code>3000G</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>26</td>
<td>8358</td>
<td>64</td>
<td>983</td>
<td>icelake, avx512, 8358, nogpu, bigtmp, common</td>
</tr>
<tr>
<td>5</td>
<td>6240</td>
<td>36</td>
<td>180</td>
<td>cascadelake, avx512, 6240, nogpu, bigtmp, standard, oldest, common</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_1_2" name="__tabbed_1" type="radio" /><label for="__tabbed_1_2">devel</label><div class="tabbed-content">
<p>Use the devel partition to jobs with which you need ongoing interaction. For example, exploratory analyses or debugging compilation.</p>
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the devel partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>06:00:00</code></td>
</tr>
<tr>
<td>Maximum CPUs per user</td>
<td><code>4</code></td>
</tr>
<tr>
<td>Maximum memory per user</td>
<td><code>32G</code></td>
</tr>
<tr>
<td>Maximum submitted jobs per user</td>
<td><code>1</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>7</td>
<td>6240</td>
<td>36</td>
<td>180</td>
<td>cascadelake, avx512, 6240, nogpu, bigtmp, standard, oldest, common</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_1_3" name="__tabbed_1" type="radio" /><label for="__tabbed_1_3">week</label><div class="tabbed-content">
<p>Use the week partition for jobs that need a longer runtime than day allows.</p>
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the week partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>7-00:00:00</code></td>
</tr>
<tr>
<td>Maximum CPUs per group</td>
<td><code>192</code></td>
</tr>
<tr>
<td>Maximum memory per group</td>
<td><code>2949G</code></td>
</tr>
<tr>
<td>Maximum CPUs per user</td>
<td><code>192</code></td>
</tr>
<tr>
<td>Maximum memory per user</td>
<td><code>2949G</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>14</td>
<td>8358</td>
<td>64</td>
<td>983</td>
<td>icelake, avx512, 8358, nogpu, bigtmp, common</td>
</tr>
<tr>
<td>2</td>
<td>6240</td>
<td>36</td>
<td>180</td>
<td>cascadelake, avx512, 6240, nogpu, bigtmp, standard, oldest, common</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_1_4" name="__tabbed_1" type="radio" /><label for="__tabbed_1_4">long</label><div class="tabbed-content">
<p>Use the long partition for jobs that need a longer runtime than week allows.</p>
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=7-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the long partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>28-00:00:00</code></td>
</tr>
<tr>
<td>Maximum CPUs per group</td>
<td><code>36</code></td>
</tr>
<tr>
<td>Maximum CPUs per user</td>
<td><code>36</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>3</td>
<td>6240</td>
<td>36</td>
<td>180</td>
<td>cascadelake, avx512, 6240, nogpu, bigtmp, standard, oldest, common</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_1_5" name="__tabbed_1" type="radio" /><label for="__tabbed_1_5">transfer</label><div class="tabbed-content">
<p>Use the transfer partition to stage data for your jobs to and from <a href="/clusters-at-yale/data/#staging-data">cluster storage</a>.</p>
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the transfer partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>1-00:00:00</code></td>
</tr>
<tr>
<td>Maximum CPUs per user</td>
<td><code>4</code></td>
</tr>
<tr>
<td>Maximum running jobs per user</td>
<td><code>4</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>72</td>
<td>8</td>
<td>227</td>
<td>milan, 72F3, nogpu, standard, common</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_1_6" name="__tabbed_1" type="radio" /><label for="__tabbed_1_6">gpu</label><div class="tabbed-content">
<p>Use the gpu partition for jobs that make use of GPUs. You must <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus">request GPUs explicitly</a> with the <code>--gpus</code> option in order to use them. For example, <code>--gpus=gtx1080ti:2</code> would request 2 GeForce GTX 1080Ti GPUs per node.</p>
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the gpu partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>2-00:00:00</code></td>
</tr>
<tr>
<td>Maximum GPUs per group</td>
<td><code>24</code></td>
</tr>
<tr>
<td>Maximum GPUs per user</td>
<td><code>12</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>9</td>
<td>6326</td>
<td>32</td>
<td>206</td>
<td>a5000</td>
<td>4</td>
<td>24</td>
<td>icelake, avx512, 6326, doubleprecision, a5000, common</td>
</tr>
<tr>
<td>1</td>
<td>8358</td>
<td>64</td>
<td>983</td>
<td>a100</td>
<td>4</td>
<td>80</td>
<td>icelake, avx512, 8358, doubleprecision, bigtmp, common, a100, a100-80g</td>
</tr>
<tr>
<td>1</td>
<td>8358</td>
<td>64</td>
<td>983</td>
<td>a100</td>
<td>4</td>
<td>80</td>
<td>icelake, avx512, 8358, gpu, bigtmp, common, doubleprecision, a100, a100-80g</td>
</tr>
<tr>
<td>1</td>
<td>8358</td>
<td>64</td>
<td>984</td>
<td>a100</td>
<td>4</td>
<td>80</td>
<td>icelake, avx512, 8358, doubleprecision, bigtmp, common, a100, a100-80g</td>
</tr>
<tr>
<td>3</td>
<td>5222</td>
<td>8</td>
<td>163</td>
<td>rtx3090</td>
<td>4</td>
<td>24</td>
<td>cascadelake, avx512, 5222, doubleprecision, common, rtx3090</td>
</tr>
<tr>
<td>4</td>
<td>5222</td>
<td>8</td>
<td>163</td>
<td>rtx5000</td>
<td>4</td>
<td>16</td>
<td>cascadelake, avx512, 5222, doubleprecision, common, bigtmp, rtx5000</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_1_7" name="__tabbed_1" type="radio" /><label for="__tabbed_1_7">gpu_devel</label><div class="tabbed-content">
<p>Use the gpu_devel partition to debug jobs that make use of GPUs, or to develop GPU-enabled code.</p>
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the gpu_devel partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>06:00:00</code></td>
</tr>
<tr>
<td>Maximum CPUs per user</td>
<td><code>10</code></td>
</tr>
<tr>
<td>Maximum GPUs per user</td>
<td><code>2</code></td>
</tr>
<tr>
<td>Maximum submitted jobs per user</td>
<td><code>2</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>3</td>
<td>6326</td>
<td>32</td>
<td>206</td>
<td>a5000</td>
<td>4</td>
<td>24</td>
<td>icelake, avx512, 6326, doubleprecision, a5000, common</td>
</tr>
<tr>
<td>2</td>
<td>5222</td>
<td>8</td>
<td>163</td>
<td>rtx5000</td>
<td>4</td>
<td>16</td>
<td>cascadelake, avx512, 5222, doubleprecision, common, bigtmp, rtx5000</td>
</tr>
<tr>
<td>1</td>
<td>5222</td>
<td>8</td>
<td>163</td>
<td>rtx3090</td>
<td>4</td>
<td>24</td>
<td>cascadelake, avx512, 5222, doubleprecision, common, rtx3090</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>352</td>
<td>a100</td>
<td>4</td>
<td>40</td>
<td>cascadelake, avx512, 6240, doubleprecision, common, bigtmp, oldest, a100, a100-40g</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_1_8" name="__tabbed_1" type="radio" /><label for="__tabbed_1_8">bigmem</label><div class="tabbed-content">
<p>Use the bigmem partition for jobs that have memory requirements other partitions can't handle.</p>
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the bigmem partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>1-00:00:00</code></td>
</tr>
<tr>
<td>Maximum CPUs per user</td>
<td><code>32</code></td>
</tr>
<tr>
<td>Maximum memory per user</td>
<td><code>3960G</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>4</td>
<td>6346</td>
<td>32</td>
<td>3960</td>
<td>icelake, avx512, 6346, nogpu, bigtmp, common</td>
</tr>
<tr>
<td>3</td>
<td>6240</td>
<td>36</td>
<td>1486</td>
<td>cascadelake, avx512, 6240, nogpu, pi, bigtmp, oldest</td>
</tr>
<tr>
<td>2</td>
<td>6234</td>
<td>16</td>
<td>1486</td>
<td>cascadelake, avx512, 6234, nogpu, common, bigtmp</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_1_9" name="__tabbed_1" type="radio" /><label for="__tabbed_1_9">scavenge</label><div class="tabbed-content">
<p>Use the scavenge partition to run preemptable jobs on more resources than normally allowed. For more information about scavenge, see the <a href="/clusters-at-yale/job-scheduling/scavenge">Scavenge documentation</a>.</p>
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the scavenge partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>1-00:00:00</code></td>
</tr>
<tr>
<td>Maximum CPUs per user</td>
<td><code>1000</code></td>
</tr>
<tr>
<td>Maximum memory per user</td>
<td><code>20000G</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>48</td>
<td>8362</td>
<td>64</td>
<td>479</td>
<td></td>
<td></td>
<td></td>
<td>icelake, avx512, 8362, nogpu, standard, pi</td>
</tr>
<tr>
<td>1</td>
<td>8358</td>
<td>64</td>
<td>1007</td>
<td>a5000</td>
<td>8</td>
<td>24</td>
<td>icelake, avx512, 8358, doubleprecision, bigtmp, pi, a5000</td>
</tr>
<tr>
<td>17</td>
<td>6326</td>
<td>32</td>
<td>206</td>
<td>a5000</td>
<td>4</td>
<td>24</td>
<td>icelake, avx512, 6326, doubleprecision, a5000, common</td>
</tr>
<tr>
<td>2</td>
<td>6326</td>
<td>32</td>
<td>984</td>
<td>a100</td>
<td>4</td>
<td>80</td>
<td>icelake, avx512, 6326, doubleprecision, pi, a100, a100-80g</td>
</tr>
<tr>
<td>40</td>
<td>8358</td>
<td>64</td>
<td>983</td>
<td></td>
<td></td>
<td></td>
<td>icelake, avx512, 8358, nogpu, bigtmp, common</td>
</tr>
<tr>
<td>1</td>
<td>8358</td>
<td>64</td>
<td>983</td>
<td>a100</td>
<td>4</td>
<td>80</td>
<td>icelake, avx512, 8358, doubleprecision, bigtmp, common, a100, a100-80g</td>
</tr>
<tr>
<td>1</td>
<td>8358</td>
<td>64</td>
<td>983</td>
<td>a100</td>
<td>4</td>
<td>80</td>
<td>icelake, avx512, 8358, doubleprecision, bigtmp, pi, a100, a100-80g</td>
</tr>
<tr>
<td>1</td>
<td>8358</td>
<td>64</td>
<td>983</td>
<td>a100</td>
<td>4</td>
<td>80</td>
<td>icelake, avx512, 8358, gpu, bigtmp, common, doubleprecision, a100, a100-80g</td>
</tr>
<tr>
<td>1</td>
<td>8358</td>
<td>64</td>
<td>984</td>
<td>a100</td>
<td>4</td>
<td>80</td>
<td>icelake, avx512, 8358, doubleprecision, bigtmp, common, a100, a100-80g</td>
</tr>
<tr>
<td>4</td>
<td>6346</td>
<td>32</td>
<td>1991</td>
<td></td>
<td></td>
<td></td>
<td>icelake, avx512, 6346, nogpu, pi</td>
</tr>
<tr>
<td>4</td>
<td>6326</td>
<td>32</td>
<td>479</td>
<td>a5000</td>
<td>4</td>
<td>24</td>
<td>icelake, avx512, 6326, doubleprecision, a5000, pi</td>
</tr>
<tr>
<td>1</td>
<td>8358</td>
<td>64</td>
<td>1007</td>
<td>l40s</td>
<td>8</td>
<td>48</td>
<td>icelake, avx512, 8358, doubleprecision, pi, bigtmp, l40s,</td>
</tr>
<tr>
<td>4</td>
<td>6346</td>
<td>32</td>
<td>3960</td>
<td></td>
<td></td>
<td></td>
<td>icelake, avx512, 6346, nogpu, bigtmp, common</td>
</tr>
<tr>
<td>41</td>
<td>6240</td>
<td>36</td>
<td>180</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx512, 6240, nogpu, bigtmp, standard, pi</td>
</tr>
<tr>
<td>4</td>
<td>6240</td>
<td>36</td>
<td>730</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx512, 6240, nogpu, bigtmp, standard, pi</td>
</tr>
<tr>
<td>4</td>
<td>6240</td>
<td>36</td>
<td>352</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx512, 6240, nogpu, bigtmp, standard, pi</td>
</tr>
<tr>
<td>12</td>
<td>6240</td>
<td>36</td>
<td>180</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx512, 6240, nogpu, bigtmp, standard, oldest, common</td>
</tr>
<tr>
<td>9</td>
<td>6240</td>
<td>36</td>
<td>163</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx512, 6240, nogpu, bigtmp, standard, pi</td>
</tr>
<tr>
<td>2</td>
<td>6240</td>
<td>36</td>
<td>166</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx512, 6240, nogpu, bigtmp, standard, pi</td>
</tr>
<tr>
<td>3</td>
<td>5222</td>
<td>8</td>
<td>163</td>
<td>rtx3090</td>
<td>4</td>
<td>24</td>
<td>cascadelake, avx512, 5222, doubleprecision, common, rtx3090</td>
</tr>
<tr>
<td>6</td>
<td>6240</td>
<td>36</td>
<td>1486</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx512, 6240, nogpu, pi, bigtmp, oldest</td>
</tr>
<tr>
<td>10</td>
<td>8268</td>
<td>48</td>
<td>352</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx512, 8268, nogpu, bigtmp, pi</td>
</tr>
<tr>
<td>1</td>
<td>6248r</td>
<td>48</td>
<td>352</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx512, 6248r, nogpu, pi, bigtmp</td>
</tr>
<tr>
<td>2</td>
<td>6234</td>
<td>16</td>
<td>1486</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx512, 6234, nogpu, common, bigtmp</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>352</td>
<td>v100</td>
<td>4</td>
<td>16</td>
<td>cascadelake, avx512, 6240, pi, oldest, v100</td>
</tr>
<tr>
<td>4</td>
<td>5222</td>
<td>8</td>
<td>163</td>
<td>rtx5000</td>
<td>4</td>
<td>16</td>
<td>cascadelake, avx512, 5222, doubleprecision, common, bigtmp, rtx5000</td>
</tr>
<tr>
<td>8</td>
<td>5222</td>
<td>8</td>
<td>163</td>
<td>rtx5000</td>
<td>4</td>
<td>16</td>
<td>cascadelake, avx512, 5222, doubleprecision, pi, bigtmp, rtx5000</td>
</tr>
<tr>
<td>2</td>
<td>6240</td>
<td>36</td>
<td>352</td>
<td>a100</td>
<td>4</td>
<td>40</td>
<td>cascadelake, avx512, 6240, doubleprecision, pi, bigtmp, oldest, a100, a100-40g</td>
</tr>
<tr>
<td>1</td>
<td>6226r</td>
<td>32</td>
<td>163</td>
<td>rtx3090</td>
<td>4</td>
<td>24</td>
<td>cascadelake, avx512, 6226r, doubleprecision, pi, rtx3090</td>
</tr>
<tr>
<td>2</td>
<td>6240</td>
<td>36</td>
<td>163</td>
<td>rtx2080ti</td>
<td>4</td>
<td>11</td>
<td>cascadelake, avx512, 6240, singleprecision, pi, bigtmp, oldest, rtx2080ti</td>
</tr>
<tr>
<td>1</td>
<td>6242</td>
<td>32</td>
<td>981</td>
<td>rtx8000</td>
<td>2</td>
<td>48</td>
<td>cascadelake, avx512, 6242, doubleprecision, pi, bigtmp, oldest, rtx8000</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>352</td>
<td>rtx3090</td>
<td>8</td>
<td>24</td>
<td>cascadelake, avx512, 6240, doubleprecision, pi, bigtmp, oldest, rtx3090</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>730</td>
<td>a100</td>
<td>4</td>
<td>40</td>
<td>cascadelake, avx512, 6240, doubleprecision, pi, bigtmp, oldest, a100, a100-40g</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>163</td>
<td>rtx3090</td>
<td>4</td>
<td>24</td>
<td>cascadelake, avx512, 6240, doubleprecision, pi, bigtmp, oldest, rtx3090</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>163</td>
<td>rtx3090</td>
<td>8</td>
<td>24</td>
<td>cascadelake, avx512, 6240, doubleprecision, pi, bigtmp, oldest, rtx3090</td>
</tr>
<tr>
<td>1</td>
<td>6132</td>
<td>28</td>
<td>730</td>
<td></td>
<td></td>
<td></td>
<td>skylake, avx512, 6132, nogpu, standard, bigtmp, pi</td>
</tr>
<tr>
<td>2</td>
<td>6132</td>
<td>28</td>
<td>163</td>
<td></td>
<td></td>
<td></td>
<td>skylake, avx512, 6132, nogpu, standard, bigtmp, pi</td>
</tr>
<tr>
<td>2</td>
<td>5122</td>
<td>8</td>
<td>163</td>
<td>rtx2080</td>
<td>4</td>
<td>8</td>
<td>skylake, avx512, 5122, singleprecision, pi, rtx2080</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_1_10" name="__tabbed_1" type="radio" /><label for="__tabbed_1_10">scavenge_gpu</label><div class="tabbed-content">
<p>Use the scavenge_gpu partition to run preemptable jobs on more GPU resources than normally allowed. For more information about scavenge, see the <a href="/clusters-at-yale/job-scheduling/scavenge">Scavenge documentation</a>.</p>
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the scavenge_gpu partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>1-00:00:00</code></td>
</tr>
<tr>
<td>Maximum GPUs per group</td>
<td><code>100</code></td>
</tr>
<tr>
<td>Maximum GPUs per user</td>
<td><code>64</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>8358</td>
<td>64</td>
<td>1007</td>
<td>a5000</td>
<td>8</td>
<td>24</td>
<td>icelake, avx512, 8358, doubleprecision, bigtmp, pi, a5000</td>
</tr>
<tr>
<td>17</td>
<td>6326</td>
<td>32</td>
<td>206</td>
<td>a5000</td>
<td>4</td>
<td>24</td>
<td>icelake, avx512, 6326, doubleprecision, a5000, common</td>
</tr>
<tr>
<td>2</td>
<td>6326</td>
<td>32</td>
<td>984</td>
<td>a100</td>
<td>4</td>
<td>80</td>
<td>icelake, avx512, 6326, doubleprecision, pi, a100, a100-80g</td>
</tr>
<tr>
<td>1</td>
<td>8358</td>
<td>64</td>
<td>983</td>
<td>a100</td>
<td>4</td>
<td>80</td>
<td>icelake, avx512, 8358, doubleprecision, bigtmp, common, a100, a100-80g</td>
</tr>
<tr>
<td>1</td>
<td>8358</td>
<td>64</td>
<td>983</td>
<td>a100</td>
<td>4</td>
<td>80</td>
<td>icelake, avx512, 8358, doubleprecision, bigtmp, pi, a100, a100-80g</td>
</tr>
<tr>
<td>1</td>
<td>8358</td>
<td>64</td>
<td>983</td>
<td>a100</td>
<td>4</td>
<td>80</td>
<td>icelake, avx512, 8358, gpu, bigtmp, common, doubleprecision, a100, a100-80g</td>
</tr>
<tr>
<td>1</td>
<td>8358</td>
<td>64</td>
<td>984</td>
<td>a100</td>
<td>4</td>
<td>80</td>
<td>icelake, avx512, 8358, doubleprecision, bigtmp, common, a100, a100-80g</td>
</tr>
<tr>
<td>4</td>
<td>6326</td>
<td>32</td>
<td>479</td>
<td>a5000</td>
<td>4</td>
<td>24</td>
<td>icelake, avx512, 6326, doubleprecision, a5000, pi</td>
</tr>
<tr>
<td>1</td>
<td>8358</td>
<td>64</td>
<td>1007</td>
<td>l40s</td>
<td>8</td>
<td>48</td>
<td>icelake, avx512, 8358, doubleprecision, pi, bigtmp, l40s,</td>
</tr>
<tr>
<td>3</td>
<td>5222</td>
<td>8</td>
<td>163</td>
<td>rtx3090</td>
<td>4</td>
<td>24</td>
<td>cascadelake, avx512, 5222, doubleprecision, common, rtx3090</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>352</td>
<td>v100</td>
<td>4</td>
<td>16</td>
<td>cascadelake, avx512, 6240, pi, oldest, v100</td>
</tr>
<tr>
<td>4</td>
<td>5222</td>
<td>8</td>
<td>163</td>
<td>rtx5000</td>
<td>4</td>
<td>16</td>
<td>cascadelake, avx512, 5222, doubleprecision, common, bigtmp, rtx5000</td>
</tr>
<tr>
<td>8</td>
<td>5222</td>
<td>8</td>
<td>163</td>
<td>rtx5000</td>
<td>4</td>
<td>16</td>
<td>cascadelake, avx512, 5222, doubleprecision, pi, bigtmp, rtx5000</td>
</tr>
<tr>
<td>2</td>
<td>6240</td>
<td>36</td>
<td>352</td>
<td>a100</td>
<td>4</td>
<td>40</td>
<td>cascadelake, avx512, 6240, doubleprecision, pi, bigtmp, oldest, a100, a100-40g</td>
</tr>
<tr>
<td>1</td>
<td>6226r</td>
<td>32</td>
<td>163</td>
<td>rtx3090</td>
<td>4</td>
<td>24</td>
<td>cascadelake, avx512, 6226r, doubleprecision, pi, rtx3090</td>
</tr>
<tr>
<td>2</td>
<td>6240</td>
<td>36</td>
<td>163</td>
<td>rtx2080ti</td>
<td>4</td>
<td>11</td>
<td>cascadelake, avx512, 6240, singleprecision, pi, bigtmp, oldest, rtx2080ti</td>
</tr>
<tr>
<td>1</td>
<td>6242</td>
<td>32</td>
<td>981</td>
<td>rtx8000</td>
<td>2</td>
<td>48</td>
<td>cascadelake, avx512, 6242, doubleprecision, pi, bigtmp, oldest, rtx8000</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>352</td>
<td>rtx3090</td>
<td>8</td>
<td>24</td>
<td>cascadelake, avx512, 6240, doubleprecision, pi, bigtmp, oldest, rtx3090</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>730</td>
<td>a100</td>
<td>4</td>
<td>40</td>
<td>cascadelake, avx512, 6240, doubleprecision, pi, bigtmp, oldest, a100, a100-40g</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>163</td>
<td>rtx3090</td>
<td>4</td>
<td>24</td>
<td>cascadelake, avx512, 6240, doubleprecision, pi, bigtmp, oldest, rtx3090</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>163</td>
<td>rtx3090</td>
<td>8</td>
<td>24</td>
<td>cascadelake, avx512, 6240, doubleprecision, pi, bigtmp, oldest, rtx3090</td>
</tr>
<tr>
<td>2</td>
<td>5122</td>
<td>8</td>
<td>163</td>
<td>rtx2080</td>
<td>4</td>
<td>8</td>
<td>skylake, avx512, 5122, singleprecision, pi, rtx2080</td>
</tr>
</tbody>
</table>
</div>
</div>
<h3 id="private-partitions">Private Partitions</h3>
<p>With few exceptions, jobs submitted to private partitions are not considered when calculating your group's <a href="/clusters-at-yale/job-scheduling/fairshare/">Fairshare</a>. Your group can purchase additional hardware for private use, which we will make available as a <code>pi_groupname</code> partition. These nodes are purchased by you, but supported and administered by us. After vendor support expires, we retire compute nodes. Compute nodes can range from $10K to upwards of $50K depending on your requirements. If you are interested in purchasing nodes for your group, please <a href="/#get-help">contact us</a>.</p>
<details class="summary">
<summary>PI Partitions (click to expand)</summary>
<div class="tabbed-set" data-tabs="2:32"><input checked="checked" id="__tabbed_2_1" name="__tabbed_2" type="radio" /><label for="__tabbed_2_1">pi_bunick</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_bunick partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>7-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>352</td>
<td>a100</td>
<td>4</td>
<td>40</td>
<td>cascadelake, avx512, 6240, doubleprecision, pi, bigtmp, oldest, a100, a100-40g</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_2" name="__tabbed_2" type="radio" /><label for="__tabbed_2_2">pi_butterwick</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_butterwick partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>7-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>352</td>
<td>a100</td>
<td>4</td>
<td>40</td>
<td>cascadelake, avx512, 6240, doubleprecision, pi, bigtmp, oldest, a100, a100-40g</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_3" name="__tabbed_2" type="radio" /><label for="__tabbed_2_3">pi_chenlab</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_chenlab partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>8268</td>
<td>48</td>
<td>352</td>
<td>cascadelake, avx512, 8268, nogpu, bigtmp, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_4" name="__tabbed_2" type="radio" /><label for="__tabbed_2_4">pi_cryo_realtime</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_cryo_realtime partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
<tr>
<td>Maximum GPUs per user</td>
<td><code>12</code></td>
</tr>
<tr>
<td>Maximum running jobs per user</td>
<td><code>2</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6326</td>
<td>32</td>
<td>206</td>
<td>a5000</td>
<td>4</td>
<td>24</td>
<td>icelake, avx512, 6326, doubleprecision, a5000, common</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_5" name="__tabbed_2" type="radio" /><label for="__tabbed_2_5">pi_cryoem</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_cryoem partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>4-00:00:00</code></td>
</tr>
<tr>
<td>Maximum CPUs per user</td>
<td><code>32</code></td>
</tr>
<tr>
<td>Maximum GPUs per user</td>
<td><code>12</code></td>
</tr>
<tr>
<td>Maximum running jobs per user</td>
<td><code>2</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>6</td>
<td>6326</td>
<td>32</td>
<td>206</td>
<td>a5000</td>
<td>4</td>
<td>24</td>
<td>icelake, avx512, 6326, doubleprecision, a5000, common</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_6" name="__tabbed_2" type="radio" /><label for="__tabbed_2_6">pi_dewan</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_dewan partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>7-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>6240</td>
<td>36</td>
<td>163</td>
<td>cascadelake, avx512, 6240, nogpu, bigtmp, standard, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_7" name="__tabbed_2" type="radio" /><label for="__tabbed_2_7">pi_dijk</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_dijk partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>7-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>352</td>
<td>rtx3090</td>
<td>8</td>
<td>24</td>
<td>cascadelake, avx512, 6240, doubleprecision, pi, bigtmp, oldest, rtx3090</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_8" name="__tabbed_2" type="radio" /><label for="__tabbed_2_8">pi_dunn</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_dunn partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>7-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>163</td>
<td>cascadelake, avx512, 6240, nogpu, bigtmp, standard, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_9" name="__tabbed_2" type="radio" /><label for="__tabbed_2_9">pi_edwards</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_edwards partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>7-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>163</td>
<td>cascadelake, avx512, 6240, nogpu, bigtmp, standard, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_10" name="__tabbed_2" type="radio" /><label for="__tabbed_2_10">pi_falcone</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_falcone partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>7-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>163</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx512, 6240, nogpu, bigtmp, standard, pi</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>1486</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx512, 6240, nogpu, pi, bigtmp, oldest</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>352</td>
<td>v100</td>
<td>4</td>
<td>16</td>
<td>cascadelake, avx512, 6240, pi, oldest, v100</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_11" name="__tabbed_2" type="radio" /><label for="__tabbed_2_11">pi_galvani</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_galvani partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>7-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>7</td>
<td>8268</td>
<td>48</td>
<td>352</td>
<td>cascadelake, avx512, 8268, nogpu, bigtmp, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_12" name="__tabbed_2" type="radio" /><label for="__tabbed_2_12">pi_gerstein</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_gerstein partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>7-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6132</td>
<td>28</td>
<td>730</td>
<td>skylake, avx512, 6132, nogpu, standard, bigtmp, pi</td>
</tr>
<tr>
<td>2</td>
<td>6132</td>
<td>28</td>
<td>163</td>
<td>skylake, avx512, 6132, nogpu, standard, bigtmp, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_13" name="__tabbed_2" type="radio" /><label for="__tabbed_2_13">pi_gerstein_gpu</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_gerstein_gpu partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>7-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>8358</td>
<td>64</td>
<td>983</td>
<td>a100</td>
<td>4</td>
<td>80</td>
<td>icelake, avx512, 8358, doubleprecision, bigtmp, pi, a100, a100-80g</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>163</td>
<td>rtx3090</td>
<td>4</td>
<td>24</td>
<td>cascadelake, avx512, 6240, doubleprecision, pi, bigtmp, oldest, rtx3090</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>163</td>
<td>rtx3090</td>
<td>8</td>
<td>24</td>
<td>cascadelake, avx512, 6240, doubleprecision, pi, bigtmp, oldest, rtx3090</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_14" name="__tabbed_2" type="radio" /><label for="__tabbed_2_14">pi_hall</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_hall partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>28-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>6326</td>
<td>32</td>
<td>984</td>
<td>a100</td>
<td>4</td>
<td>80</td>
<td>icelake, avx512, 6326, doubleprecision, pi, a100, a100-80g</td>
</tr>
<tr>
<td>39</td>
<td>6240</td>
<td>36</td>
<td>180</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx512, 6240, nogpu, bigtmp, standard, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_15" name="__tabbed_2" type="radio" /><label for="__tabbed_2_15">pi_hall_bigmem</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_hall_bigmem partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>28-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>6240</td>
<td>36</td>
<td>1486</td>
<td>cascadelake, avx512, 6240, nogpu, pi, bigtmp, oldest</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_16" name="__tabbed_2" type="radio" /><label for="__tabbed_2_16">pi_jetz</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_jetz partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>7-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>8358</td>
<td>64</td>
<td>1991</td>
<td>icelake, avx512, 8358, nogpu, bigtmp, pi</td>
</tr>
<tr>
<td>4</td>
<td>6240</td>
<td>36</td>
<td>730</td>
<td>cascadelake, avx512, 6240, nogpu, bigtmp, standard, pi</td>
</tr>
<tr>
<td>4</td>
<td>6240</td>
<td>36</td>
<td>352</td>
<td>cascadelake, avx512, 6240, nogpu, bigtmp, standard, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_17" name="__tabbed_2" type="radio" /><label for="__tabbed_2_17">pi_kleinstein</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_kleinstein partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>7-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>6240</td>
<td>36</td>
<td>163</td>
<td>cascadelake, avx512, 6240, nogpu, bigtmp, standard, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_18" name="__tabbed_2" type="radio" /><label for="__tabbed_2_18">pi_krishnaswamy</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_krishnaswamy partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>7-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>730</td>
<td>a100</td>
<td>4</td>
<td>40</td>
<td>cascadelake, avx512, 6240, doubleprecision, pi, bigtmp, oldest, a100, a100-40g</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_19" name="__tabbed_2" type="radio" /><label for="__tabbed_2_19">pi_ma</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_ma partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>7-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>8268</td>
<td>48</td>
<td>352</td>
<td>cascadelake, avx512, 8268, nogpu, bigtmp, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_20" name="__tabbed_2" type="radio" /><label for="__tabbed_2_20">pi_medzhitov</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_medzhitov partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>7-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>6240</td>
<td>36</td>
<td>166</td>
<td>cascadelake, avx512, 6240, nogpu, bigtmp, standard, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_21" name="__tabbed_2" type="radio" /><label for="__tabbed_2_21">pi_miranker</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_miranker partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>7-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6248r</td>
<td>48</td>
<td>352</td>
<td>cascadelake, avx512, 6248r, nogpu, pi, bigtmp</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_22" name="__tabbed_2" type="radio" /><label for="__tabbed_2_22">pi_ohern</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_ohern partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>7-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>4</td>
<td>8358</td>
<td>64</td>
<td>984</td>
<td>icelake, avx512, 8358, nogpu, bigtmp, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_23" name="__tabbed_2" type="radio" /><label for="__tabbed_2_23">pi_reinisch</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_reinisch partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>7-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>5122</td>
<td>8</td>
<td>163</td>
<td>rtx2080</td>
<td>4</td>
<td>8</td>
<td>skylake, avx512, 5122, singleprecision, pi, rtx2080</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_24" name="__tabbed_2" type="radio" /><label for="__tabbed_2_24">pi_sestan</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_sestan partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>7-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>8358</td>
<td>64</td>
<td>1991</td>
<td>icelake, avx512, 8358, nogpu, bigtmp, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_25" name="__tabbed_2" type="radio" /><label for="__tabbed_2_25">pi_sigworth</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_sigworth partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>7-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>163</td>
<td>rtx2080ti</td>
<td>4</td>
<td>11</td>
<td>cascadelake, avx512, 6240, singleprecision, pi, bigtmp, oldest, rtx2080ti</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_26" name="__tabbed_2" type="radio" /><label for="__tabbed_2_26">pi_sindelar</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_sindelar partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>7-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>163</td>
<td>rtx2080ti</td>
<td>4</td>
<td>11</td>
<td>cascadelake, avx512, 6240, singleprecision, pi, bigtmp, oldest, rtx2080ti</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_27" name="__tabbed_2" type="radio" /><label for="__tabbed_2_27">pi_tomography</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_tomography partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>4-00:00:00</code></td>
</tr>
<tr>
<td>Maximum CPUs per user</td>
<td><code>32</code></td>
</tr>
<tr>
<td>Maximum GPUs per user</td>
<td><code>24</code></td>
</tr>
<tr>
<td>Maximum running jobs per user</td>
<td><code>2</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>8</td>
<td>5222</td>
<td>8</td>
<td>163</td>
<td>rtx5000</td>
<td>4</td>
<td>16</td>
<td>cascadelake, avx512, 5222, doubleprecision, pi, bigtmp, rtx5000</td>
</tr>
<tr>
<td>1</td>
<td>6242</td>
<td>32</td>
<td>981</td>
<td>rtx8000</td>
<td>2</td>
<td>48</td>
<td>cascadelake, avx512, 6242, doubleprecision, pi, bigtmp, oldest, rtx8000</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_28" name="__tabbed_2" type="radio" /><label for="__tabbed_2_28">pi_townsend</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_townsend partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>7-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>6240</td>
<td>36</td>
<td>180</td>
<td>cascadelake, avx512, 6240, nogpu, bigtmp, standard, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_29" name="__tabbed_2" type="radio" /><label for="__tabbed_2_29">pi_tsang</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_tsang partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>7-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>4</td>
<td>8358</td>
<td>64</td>
<td>983</td>
<td>icelake, avx512, 8358, nogpu, bigtmp, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_30" name="__tabbed_2" type="radio" /><label for="__tabbed_2_30">pi_ya-chi_ho</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_ya-chi_ho partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>7-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>8268</td>
<td>48</td>
<td>352</td>
<td>cascadelake, avx512, 8268, nogpu, bigtmp, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_31" name="__tabbed_2" type="radio" /><label for="__tabbed_2_31">pi_yong_xiong</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_yong_xiong partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>7-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>8358</td>
<td>64</td>
<td>1007</td>
<td>a5000</td>
<td>8</td>
<td>24</td>
<td>icelake, avx512, 8358, doubleprecision, bigtmp, pi, a5000</td>
</tr>
<tr>
<td>4</td>
<td>6326</td>
<td>32</td>
<td>479</td>
<td>a5000</td>
<td>4</td>
<td>24</td>
<td>icelake, avx512, 6326, doubleprecision, a5000, pi</td>
</tr>
<tr>
<td>1</td>
<td>8358</td>
<td>64</td>
<td>1007</td>
<td>l40s</td>
<td>8</td>
<td>48</td>
<td>icelake, avx512, 8358, doubleprecision, pi, bigtmp, l40s,</td>
</tr>
<tr>
<td>1</td>
<td>6226r</td>
<td>32</td>
<td>163</td>
<td>rtx3090</td>
<td>4</td>
<td>24</td>
<td>cascadelake, avx512, 6226r, doubleprecision, pi, rtx3090</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_32" name="__tabbed_2" type="radio" /><label for="__tabbed_2_32">pi_zhao</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_zhao partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>7-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>6240</td>
<td>36</td>
<td>163</td>
<td>cascadelake, avx512, 6240, nogpu, bigtmp, standard, pi</td>
</tr>
</tbody>
</table>
</div>
</div>
</details>
<h3 id="ycga-partitions">YCGA Partitions</h3>
<p>The following partitions are intended for projects related to the <a href="http://ycga.yale.edu/">Yale Center for Genome Analysis</a>. Please do not use these partitions for other proejcts. Access is granted on a group basis. If you need access to these partitions, please <a href="/#get-help">contact us</a> to get approved and added.</p>
<details class="summary">
<summary>YCGA Partitions (click to expand)</summary>
<div class="tabbed-set" data-tabs="3:4"><input checked="checked" id="__tabbed_3_1" name="__tabbed_3" type="radio" /><label for="__tabbed_3_1">ycga</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the ycga partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>2-00:00:00</code></td>
</tr>
<tr>
<td>Maximum CPUs per group</td>
<td><code>1024</code></td>
</tr>
<tr>
<td>Maximum memory per group</td>
<td><code>3934G</code></td>
</tr>
<tr>
<td>Maximum CPUs per user</td>
<td><code>256</code></td>
</tr>
<tr>
<td>Maximum memory per user</td>
<td><code>1916G</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>40</td>
<td>8362</td>
<td>64</td>
<td>479</td>
<td>icelake, avx512, 8362, nogpu, standard, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_3_2" name="__tabbed_3" type="radio" /><label for="__tabbed_3_2">ycga_admins</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>8362</td>
<td>64</td>
<td>479</td>
<td>icelake, avx512, 8362, nogpu, standard, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_3_3" name="__tabbed_3" type="radio" /><label for="__tabbed_3_3">ycga_bigmem</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the ycga_bigmem partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>4-00:00:00</code></td>
</tr>
<tr>
<td>Maximum CPUs per user</td>
<td><code>64</code></td>
</tr>
<tr>
<td>Maximum memory per user</td>
<td><code>1991G</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>4</td>
<td>6346</td>
<td>32</td>
<td>1991</td>
<td>icelake, avx512, 6346, nogpu, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_3_4" name="__tabbed_3" type="radio" /><label for="__tabbed_3_4">ycga_long</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the ycga_long partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
<tr>
<td>Maximum CPUs per group</td>
<td><code>64</code></td>
</tr>
<tr>
<td>Maximum memory per group</td>
<td><code>479G</code></td>
</tr>
<tr>
<td>Maximum CPUs per user</td>
<td><code>32</code></td>
</tr>
<tr>
<td>Maximum memory per user</td>
<td><code>239G</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>6</td>
<td>8362</td>
<td>64</td>
<td>479</td>
<td>icelake, avx512, 8362, nogpu, standard, pi</td>
</tr>
</tbody>
</table>
</div>
</div>
</details>
<h2 id="public-datasets">Public Datasets</h2>
<p>We host datasets of general interest in a loosely organized directory tree in <code>/gpfs/gibbs/data</code>:</p>
<div class="highlight"><pre><span></span><code>├── alphafold-2.3
├── alphafold-2.2 (deprecated)
├── alphafold-2.0 (deprecated)
├── annovar
│   └── humandb
├── cryoem
├── db
│   ├── annovar
│   ├── blast
│   ├── busco
│   └── Pfam
└── genomes
    ├── 1000Genomes
    ├── 10xgenomics
    ├── Aedes_aegypti
    ├── Bos_taurus
    ├── Chelonoidis_nigra
    ├── Danio_rerio
    ├── Drosophila_melanogaster
    ├── Gallus_gallus
    ├── hisat2
    ├── Homo_sapiens
    ├── Macaca_mulatta
    ├── Mus_musculus
    ├── Monodelphis_domestica
    ├── PhiX
    └── Saccharomyces_cerevisiae
    └── tmp
└── hisat2
    └── mouse
</code></pre></div>
<p>If you would like us to host a dataset or questions about what is currently available, please <a href="/#get-help">contact us</a>.</p>
<h2 id="ycga-data">YCGA Data</h2>
<p>Data associated with YCGA projects and sequenceers are located on the YCGA storage system, accessible at <code>/gpfs/ycga</code>.</p>
<p>For more information on accessing this data as well as sequencing data retention polices, see the <a href="/data/ycga-data">YCGA Data documentation</a>.</p>
<h2 id="storage">Storage</h2>
<p>McCleary has access to a number of GPFS filesystems. <code>/vast/palmer</code> is McCleary's primary filesystem where Home and Scratch60 directories are located. Every group on McCleary also has access to a Project allocation on the Gibbs filesytem on <code>/gpfs/gibbs</code>. For more details on the different storage spaces, see our <a href="/data/hpc-storage">Cluster Storage</a> documentation.</p>
<p>You can check your current storage usage &amp; limits by running the <code>getquota</code> command. Your <code>~/project</code> and <code>~/palmer_scratch</code> directories are shortcuts. Get a list of the absolute paths to your directories with the <code>mydirectories</code> command. If you want to share data in your Project or Scratch directory, see the <a href="/data/permissions/">permissions</a> page.</p>
<p>For information on data recovery, see the <a href="/data/backups">Backups and Snapshots</a> documentation.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Files stored in <code>palmer_scratch</code> are purged if they are older than 60 days. You will receive an email alert one week before they are deleted. Artificial extension of scratch file expiration is forbidden without explicit approval from the YCRC. Please <a href="/data/#purchase-additional-storage">purchase storage</a> if you need additional longer term storage.</p>
</div>
<table>
<thead>
<tr>
<th>Partition</th>
<th>Root Directory</th>
<th>Storage</th>
<th>File Count</th>
<th>Backups</th>
<th>Snapshots</th>
</tr>
</thead>
<tbody>
<tr>
<td>home</td>
<td><code>/vast/palmer/home.mccleary</code></td>
<td>125GiB/user</td>
<td>500,000</td>
<td>Yes</td>
<td>&gt;=2 days</td>
</tr>
<tr>
<td>project</td>
<td><code>/gpfs/gibbs/project</code></td>
<td>1TiB/group, increase to 4TiB on request</td>
<td>5,000,000</td>
<td>No</td>
<td>&gt;=2 days</td>
</tr>
<tr>
<td>scratch</td>
<td><code>/vast/palmer/scratch</code></td>
<td>10TiB/group</td>
<td>15,000,000</td>
<td>No</td>
<td>No</td>
</tr>
</tbody>
</table>
                
                  
                    

<hr>
<div class="md-source-date">
  <small>
    
      Last update: <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">February 12, 2025</span>
      
    
  </small>
</div>
                  
                
              
              
                


              
            </article>
          </div>
        </div>
        
      </main>
      
        <!--
  Copyright (c) 2016-2020 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->



<!-- Application footer -->
<footer class="md-footer">
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div>
        <a href="https://www.yale.edu" title="Yale" target="_blank">
          <img alt="Yale Logo" src="/img/yale-white.png" height="60" style="padding: 8px; height: 60px;">
	</a>
      </div>
 
      <!-- Accessability/Privacy information -->
      <div class="md-footer-copyright">
        <a href="https://usability.yale.edu/web-accessibility/accessibility-yale">Accessibility at Yale</a>
           · 
        <a title="Yale Privacy policy" href="http://www.yale.edu/privacy-policy">Privacy policy</a>
          
      </div>

      <!-- Social links -->
      
  <div class="md-footer-social">
    
      
      
        
        
      
      <a href="https://ycrc.yale.edu" target="_blank" rel="noopener" title="ycrc.yale.edu" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19.07 4.93C17.22 3 14.66 1.96 12 2c-2.66-.04-5.21 1-7.06 2.93C3 6.78 1.96 9.34 2 12c-.04 2.66 1 5.21 2.93 7.06C6.78 21 9.34 22.04 12 22c2.66.04 5.21-1 7.06-2.93C21 17.22 22.04 14.66 22 12c.04-2.66-1-5.22-2.93-7.07M17 12v6h-3.5v-5h-3v5H7v-6H5l7-7 7.5 7H17z"/></svg>
      </a>
    
      
      
        
        
      
      <a href="http://help.ycrc.yale.edu/" target="_blank" rel="noopener" title="help.ycrc.yale.edu" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m15.07 11.25-.9.92C13.45 12.89 13 13.5 13 15h-2v-.5c0-1.11.45-2.11 1.17-2.83l1.24-1.26c.37-.36.59-.86.59-1.41a2 2 0 0 0-2-2 2 2 0 0 0-2 2H8a4 4 0 0 1 4-4 4 4 0 0 1 4 4 3.2 3.2 0 0 1-.93 2.25M13 19h-2v-2h2M12 2A10 10 0 0 0 2 12a10 10 0 0 0 10 10 10 10 0 0 0 10-10c0-5.53-4.5-10-10-10z"/></svg>
      </a>
    
      
      
        
        
      
      <a href="https://research.computing.yale.edu/system-status" target="_blank" rel="noopener" title="research.computing.yale.edu" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M7.5 4A5.5 5.5 0 0 0 2 9.5c0 .5.09 1 .22 1.5H6.3l1.27-3.37c.3-.8 1.48-.88 1.86 0L11.5 13l.59-1.42c.13-.33.48-.58.91-.58h8.78c.13-.5.22-1 .22-1.5A5.5 5.5 0 0 0 16.5 4c-1.86 0-3.5.93-4.5 2.34C11 4.93 9.36 4 7.5 4M3 12.5a1 1 0 0 0-1 1 1 1 0 0 0 1 1h2.44L11 20c1 .9 1 .9 2 0l5.56-5.5H21a1 1 0 0 0 1-1 1 1 0 0 0-1-1h-7.6l-.93 2.3c-.4 1.01-1.55.87-1.92.03L8.5 9.5l-.96 2.33c-.15.38-.49.67-.94.67H3z"/></svg>
      </a>
    
      
      
        
        
      
      <a href="https://github.com/ycrc" target="_blank" rel="noopener" title="github.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
      </a>
    
      
      
        
        
      
      <a href="https://twitter.com/yalecrc" target="_blank" rel="noopener" title="twitter.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
      </a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.tabs.sticky", "navigation.sections"], "search": "../../assets/javascripts/workers/search.fcfe8b6d.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.b1047164.min.js"></script>
      
        <script src="../../_static/js/extra.js"></script>
      
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
      
    
  </body>
</html>