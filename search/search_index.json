{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>YCRC Welcomes Ruth Marinshaw as the New Executive Director</p> <p>YCRC Team is delighted to welcome Ruth Marinshaw as our new Executive Director. Ruth joined Yale mid-November to serve as the university's primary technologist to support the computing needs of Yale's research community. She will work with colleagues across campus to implement and operate computational technologies that support Yale faculty, students, and staff in conducting cutting-edge research. See the announcement for more about Ruth.</p> <p>Announcing the Bouchet HPC Cluster</p> <p>The Bouchet HPC cluster, YCRC first installation at MGHPCC, will be available in beta Fall 2024. The first installation of nodes, approximately 4,000 direct-liquid-cooled cores, will be dedicated to tightly coupled parallel workflows, such as those run in the \u201cmpi\u201d partition on the Grace cluster. Later on this year we will be acquiring and installing a large number of general purpose compute nodes as well as GPU-enabled compute nodes. At that point Bouchet will be available to all Yale researchers for computational work involving low-risk data. See the Bouchet page for more information and updates.</p>"},{"location":"#introduction","title":"Introduction","text":"<p>The Yale Center for Research Computing provides support for research computing at Yale University. Our most active area for support is High Performance Computing, however we also support other computationally intensive research.  In addition, we work with faculty and research groups across disciplines to design and maintain cost-effective computing capabilities.</p>"},{"location":"#get-help","title":"Get Help","text":"<p>To best serve the research community, we provide one-on-one consulting and use a support tracking system.</p> <p>Troubleshooting Login Issues</p> <p>If you are experiencing issues logging into one of the clusters, please first check the current System Status for known issues and check the Troubleshoot Login guide first before seeking additional assistance.</p>"},{"location":"#web-and-email-support","title":"Web and Email Support","text":"<p>To submit requests, issues, or questions please send us an email at hpc@yale.edu or sign on to our online support system at help.ycrc.yale.edu. Your login credentials there are your email and a password of your choosing, not your CAS password.</p> <p>Once received, our system will send an automated response with a link to a ticket. From there we'll track your ticket and make sure it's handled properly by the right person. Replies via email or the online support system go to the same place and are interchangeable. Constructive feedback is much appreciated.</p>"},{"location":"#office-hours-via-zoom","title":"Office Hours via Zoom","text":"<p>The YCRC hosts weekly office hours via Zoom on Wednesdays at 11am-12pm EST. Every Wednesday, Research support team members will be available to answer questions about the HPC clusters, data storage, cluster usage, etc. No appointments are necessary.</p> <p>Link: https://yale.zoom.us/my/ycrcsupport</p> <p>Phone: 203-432-9666 (2-ZOOM if on-campus)or 646 568 7788; Meeting ID: 224 666 8665</p>"},{"location":"#youtube-channel","title":"YouTube Channel","text":"<p>The YCRC YouTube channel features recorded tutorials and workshops that cover a wide range of computing topics. New videos are added regularly and suggestions for topics can be submitted by emailing research.computing@yale.edu.</p>"},{"location":"#one-on-one-support","title":"One-on-One Support","text":"<p>Research support team members are available by appointment for one-on-one support.  See the table below for information about each person's area of particular focus. Please send requests for appointments with a team member to research.computing@yale.edu.  If you have a general question or are unsure about who to meet with,  include as much detail as possible about your request and we'll find the right person for you.</p> Specialist Cluster(s) Areas of Focus Kathleen McKiernan All Getting Started Rob Bjornson, Ph.D. McCleary Life Sciences, Bioinformatics, Python, R Tom Langford, Ph.D. Grace/Bouchet/Milgram Physics, EPS dept, Python, MPI Aya Nawano, Ph.D. Grace/Bouchet Molecular Dynamics, Matlab, C/C++, MPI Kaylea Nelson, Ph.D. Grace/Milgram Astronomy, EPS dept, MPI, Python Mike Rothberg, Ph.D. Grace/McCleary AI/ML, Computational Chemistry, Python, Matlab Michael Strickler, Ph.D. McCleary Life Sciences, Structural Biology Ping Luo Misha/Milgram Wu Tsai Institute, Psychology dept, Open OnDemand Misha Guy, Ph.D. SRSC Software and Mathematica (email at mikhael.guy@yale.edu for appointment)"},{"location":"#qa-platform","title":"Q&amp;A Platform","text":"<p>The YCRC hosts a Q&amp;A platform at ask.cyberinfrastructure.org. Post questions about the clusters and receive answers from YCRC staff or even your peers! The sub-site for YCRC related questions is available at ask.cyberinfrastructure.org/g/Yale.</p>"},{"location":"#acknowledge-the-ycrc","title":"Acknowledge the YCRC","text":"<p>If publishing work performed on a YCRC cluster or with assistance from YCRC staff, we greatly appreciate acknowledgement of our staff and computing time in your publication. A list of YCRC staff can be found on our Staff page, and the clusters are summarized on our HPC Resources page. Example acknowledgement below: </p> <p>We thank the Yale Center for Research Computing, specifically [YCRC staff member name(s)], for guidance and assistance in computation run on the [cluster name here] cluster.</p> <p>Additionally, if you would be willing to send the publication information to research.computing@yale.edu, that would assist our efforts to capture work performed on YCRC resources and we can promote your work on our research.computing.yale.edu website.</p>"},{"location":"glossary/","title":"Glossary","text":"<p>To help clarify the way we refer to certain terms in our user documentation, here is a brief list of some of the words that regularly come up in our documents. Please reach out to us at hpc@yale.edu if there are any other words that need to be added.</p> <p>Account - used to authenticate and grant permission to access resources</p> <p>Account (Slurm) - an accounting mechanism to keep track of a group's computing usage</p> <p>Activate - making something operational</p> <p>Array - a data structure across a series of memory locations consisting of elements organized in an index</p> <p>Array (job) - a series of jobs that all request the same resources and run the same batch script</p> <p>Array Task ID - a unique sequential number with an appended number that refers to an individual task within the set of submitted jobs</p> <p>Channel - Community-led collections of packages created by a group or lab installed with conda to allow for a homogenous environment across systems</p> <p>CLI - Command Line Interface processes commands to a computer program in the form of lines of text in a window</p> <p>Cluster - a set of computers, called nodes) networked together so nodes can perform the tasks facilitated by a scheduling software</p> <p>Command - a specific order from a computer to execute a service with either an application or the operating system</p> <p>Compute Node - the nodes that work runs on to perform computational work</p> <p>Container - A stack of software, libraries and operating system that is independent of the host computer and can be accessed on other computers</p> <p>Container Image - Self-contained read-only files used to run applications</p> <p>CPU - Central Processing Units are the components of a system that perform basic operations and exchange data with the system\u2019s memory (also known as a processor)</p> <p>Data - items of information collected together for reference or analysis</p> <p>Database - a collection of structured data held within a computer</p> <p>Deactivate - making something de-operational</p> <p>Environment - a collection of hardware, software, data storage and networks that work together in facilitating the processing and exchange of information</p> <p>Extension - Suffix at the end of a filename to indicate the file type</p> <p>Fileset - a section of a storage device that is given a designated purpose</p> <p>Filesystem - a process that manages how and where data is stored</p> <p>Flag - (see Options)</p> <p>GPU - Graphics Processing Units are specialized circuits designed to rapidly manipulate memory and create images in a frame buffer for a displayed output</p> <p>GridFTP - an extension of the Fire Transfer Protocol for grid computing that allows users to transfer and save data on a different account such as Google Drive or other off network memory</p> <p>Group - a collection of users who can all be given the same permissions on a system</p> <p>GUI - Graphical User Interface allows users to interact with devices and applications through a visual window that can commonly display icons and predetermined fields</p> <p>Hardware - the physical parts of a computer</p> <p>Host - (ie. Host Computer) A device connected to a computer network that offers resources, services and applications to users on the network</p> <p>Image - (See Container Image)</p> <p>Index - a method of sorting data by creating keywords or a listing of data</p> <p>Interface - a boundary across which two or more computer system components can exchange information</p> <p>Job - a unit of work given to an operating system by a scheduler</p> <p>Job Array - a way to submit multiple similar jobs by associating each subjob with an index value based on an array task id</p> <p>Key - a variable value applied using an algorithm to a block of unencrypted text to produce encrypted text</p> <p>Load - transfer a program or data into memory or into the CPU</p> <p>Login Node - a node that users log in on to access the cluster</p> <p>Memory - (see RAM)</p> <p>Metadata - A set of data that describes and gives basic information about other data</p> <p>Module - a number of distinct but interrelated units that build up or into a program</p> <p>MPI - Message Passing Interface is a standardized and portable message-passing standard designed to function on parallel computing architectures</p> <p>Multiprocessing - the ability to operate more than one task simultaneously on the same program across two or more processors in a computer</p> <p>Node - a server in the cluster</p> <p>Option - a single letter or full word that modifies the behavior of a command in a predetermined way (also known as a flag or switch)</p> <p>Package - a collection of hardware and software needed to create a working system</p> <p>Parallel - (ex. Computing/Programming) Architecture in which several processes are carried out simultaneously across smaller, independent parts</p> <p>Partition - a section of a storage device that is given a designated purpose</p> <p>Partition (Slurm) - a collection of compute nodes available via the scheduler</p> <p>Path - A string of characters used to identify locations throughout a directory structure</p> <p>Pane - (Associate with window) A subdivision within a window where an independent terminal can run simultaneously alongside another terminal</p> <p>Processor - (see CPU)</p> <p>Queue - a sequence of objects arranged according to priority waiting to be processed</p> <p>RAM - Random Access Memory, also known as \"Memory\" can be read and changed in any order and is typically used to to store working data</p> <p>Reproducibility - the ability to execute the same results across multiple systems by different individuals using the same data</p> <p>Scheduler - the software used to assign resources to a job for tasks</p> <p>Scheduling - the act of assigning resources to a task through a software product</p> <p>Session - a temporary information exchange between two or more devices</p> <p>SSH - secure shell is a cryptographic network protocol for operating network services securely over an unsecured network</p> <p>Software - a collection of data and instructions that tell a computer how to operate</p> <p>Switch - (see Options)</p> <p>System - a set of integrated hardware and software that input, output, process, and store data and information</p> <p>Task ID - a unique sequential number used to refer to a task</p> <p>Terminal - Referring to a terminal program, a text-based interface for typing commands</p> <p>Toolchain - a set of tools performing individual actions used in delivering an operation</p> <p>Unload - remove a program or data from memory or out of the CPU</p> <p>User - a person interacting and utilizing a computing service</p> <p>Variable - assigned and referenced data values that can be called within a program and changed depending on how the program runs</p> <p>Window - (Associate with pane) the whole screen being displayed, containing subdivisions, or panes, that can run independent terminals alongside each other</p>"},{"location":"news/","title":"News","text":"<p>{{ blog_content }}</p>"},{"location":"user-group/","title":"YCRC User Group","text":"<p>The YCRC User Group is a community of researchers at Yale who utilize computing resources and technology to enable their research.</p> <p>You can join the User Group mailing list and forum where you can post questions or tips to other YCRC users at https://groups.io/g/ycrcusergroup.</p>"},{"location":"applications/","title":"Overview","text":""},{"location":"applications/#software-modules","title":"Software Modules","text":"<p>The YCRC will install and manage commonly used software. These software are available as modules, which allow you to add or remove different combinations and versions of software to your environment as needed. See our module guide for more info. You can run <code>module avail</code> to page through all available software once you log in.</p>"},{"location":"applications/#conda-python-r","title":"Conda, Python &amp; R","text":"<p>You should also feel free to install things for yourself. See our Conda, Python, R guides for guidance on running these on the clusters.</p>"},{"location":"applications/#compile-your-own-software","title":"Compile Your Own Software","text":"<p>For all other software, we encourage users to attempt to install their own software into their directories. Here are instructions for common software procedures.</p> <ul> <li>Make</li> <li>Cmake</li> <li>Apptainer: create containers and port Docker containers to the clusters (formerly know as \"Singularity\")</li> </ul> <p>If you run into issues with your software installations, contact us.</p>"},{"location":"applications/#software-guides","title":"Software Guides","text":"<p>We provide additional guides for running specific software on the clusters as well.</p>"},{"location":"applications/compile/","title":"Build Software","text":"<p>How to get software you need up and running on the clusters.</p>"},{"location":"applications/compile/#caveat-emptor","title":"caveat emptor","text":"<p>We recommend either use existing software modules, Conda, Apptainer, or pre-compiled software where available. However, there are cases where compiling applications is necessary or desired. This can be because the pre-compiled version isn't readily available/compatible or because compiling applications on the cluster will make an appreciable difference in performance. It is also the case that many R packages are compiled at install time.</p> <p>When compiling applications on the clusters, it is important to consider the ways in which you expect to run the application you are endeavoring to get working. If you want to be able to run jobs calling your application any node on the cluster, you will need to target the oldest hardware available so that newer optimizations are not used that will fail on some nodes. If your application is already quite specialized (e.g. needs GPUs or brand-new CPU instructions), you will want to compile it natively for the subset of compute nodes on which your jobs will run. This decision is often a trade-off between faster individual jobs or jobs that can run on more nodes at once.</p> <p>Each of the cluster pages (see the HPC Resources page for a list) has a \"Compute Node Configurations\" section where nodes are roughly listed from oldest to newest.</p>"},{"location":"applications/compile/#illegal-instruction-instructions","title":"Illegal Instruction Instructions","text":"<p>You may find that software compiled on newer compute nodes will fail with the error <code>Illegal instruction (core dumped)</code>. This includes R/Python libraries that include code that compiles from source. To remedy this issue make sure to always either:</p> <ul> <li>Build or install software on the oldest available nodes. You can ensure you are on the oldest hardware by specifying the <code>oldest</code> feature (<code>--constraint oldest</code>) in your job submission.</li> <li>Require that your jobs running the software in question request similar hardware to their build environment. If your software needs newer instructions using <code>avx512</code> as a constraint will probably work, but limit the pool of nodes your job can run on.</li> </ul> <p>Either way, you will want to control where your jobs run with job constraints.</p> <p>Warning</p> <p>Grace's login nodes have newer architecture than the oldest nodes on the cluster. Always compile in an interactive job submitted with the <code>--constraint oldest</code> Slurm flag if you want to ensure your program will run on all generations of the compute nodes.</p>"},{"location":"applications/compile/#conventions","title":"Conventions","text":""},{"location":"applications/compile/#local-install","title":"Local Install","text":"<p>Because you don't have admin/root/sudo privileges on the clusters, you won't be able to use <code>sudo</code> and a package manager like <code>apt</code>, <code>yum</code>, etc.; You will need to adapt install instructions to allow for what is called a local or user install. If you prefer or require this method, you should create a container image (see our Apptainer guide), then run it on the cluster.</p> <p>For things to work smoothly you will need to choose and stick with a prefix, or path to your installed applications and libraries. We recommend this be either in your home or project directory, something like <code>~/software</code> or <code>/path/to/project/software</code>. Make sure you have created it before continuing.</p> <p>Tip</p> <p>If you choose a project directory prefix, it will be easier to share your applications with lab mates or other cluster users. Just make sure to use the true path (the one returned by <code>mydirectories</code>).</p> <p>Once you've chosen a prefix you will want to add any directory with executables you want to run to your <code>PATH</code> environment variable, and any directores with libraries that your application(s) link to your <code>LD_LIBRARY_PATH</code> environment variable. Each of these tell your shell where to look when you call your application without specifying an absolute path to it. To set these variables permanently, add the following to the end of your <code>~/.bashrc</code> file:</p> <pre><code># local installs\nexport MY_PREFIX=~/software\nexport PATH=$MY_PREFIX/bin:$PATH\nexport LD_LIBRARY_PATH=$MY_PREFIX/lib:$LD_LIBRARY_PATH\n</code></pre> <p>For the remainder of the guide we'll use the <code>$MY_PREFIX</code> variable to refer to the prefix. See below or your application's install instructions for exactly how to specify your prefix at build/install time.</p>"},{"location":"applications/compile/#dependencies","title":"Dependencies","text":"<p>You will need to develop a build strategy that works for you and stay consistent. If you're happy using libraries and toolchains that are already available on the cluster as dependencies (recommended), feel free to create module collections that serve as your environments. If you prefer to completely build your own software tree, that is ok too. Whichever route you choose, try to stick with the same version of dependencies (e.g. MPI, zlib, numpy) and compiler you're using (e.g. GCC, intel). We find that unless absolutely necessary, the newest version of a compiler or library might not be the most compatible with a wide array of scientific software so you may want to step back a few versions or try using what was available at the time your application was being developed.</p>"},{"location":"applications/compile/#autotools-configuremake","title":"Autotools (<code>configure</code>/<code>make</code>)","text":"<p>If your application includes instructions to run <code>./bootstrap</code>, <code>./autogen.sh</code>, <code>./configure</code> or <code>make</code>, it is using the GNU Build System.</p> <p>Warning</p> <p>If you are using GCC 10+, you will need to load a separate Autotools module for your version of GCC; e.g., <pre><code>module load Autotools/20200321-GCCcore-10.2.0\n</code></pre></p>"},{"location":"applications/compile/#configure","title":"<code>configure</code>","text":"<p>If you are instructed to run <code>./configure</code> to generate a Makefile, specify your prefix with the <code>--prefix</code> option. This creates a file, usually named <code>Makefile</code> that is a recipe for <code>make</code> to use to build your application.</p> <pre><code>export MY_PREFIX=~/software\n./configure --prefix=$MY_PREFIX\n</code></pre>"},{"location":"applications/compile/#make-install","title":"<code>make install</code>","text":"<p>If your configure ran properly, <code>make install</code> should properly place your application in your prefix directory. If there is no install target specified for your application, you can either run <code>make</code> and copy the application to your <code>$MY_PREFIX/bin</code> directory or build it somewhere in <code>$MY_PREFIX</code> and add its relevant paths to your <code>PATH</code> and/or <code>LD_LIBRARY_PATH</code> environment variables in your <code>~/.bashrc</code> file as shown in the local install section.</p>"},{"location":"applications/compile/#cmake","title":"CMake","text":"<p>CMake is a popular cross-platform build system. On a linux system, CMake will create a <code>Makefile</code> in a step analogous to <code>./configure</code>. It is common to create a build directory then run the <code>cmake</code> and <code>make</code> commands from there. Below is what installing to your <code>$MY_DIRECTORY</code> prefix might look like with CMake. CMake instructions also tend to link together the build process onto on line with <code>&amp;&amp;</code>, which tells your shell to only continue to the next command if the previous one exited without error.</p> <pre><code>export MY_PREFIX=~/software\nmkdir build &amp;&amp; cd build &amp;&amp; cmake -DCMAKE_INSTALL_PREFIX=$MY_PREFIX .. &amp;&amp; make &amp;&amp; make install\n</code></pre>"},{"location":"applications/lifecycle/","title":"Software Module Lifecycle","text":"<p>To keep the YCRC cluster software modules catalogs tidy, relevant, and up to date, we periodically deprecate and introduce modules.</p>"},{"location":"applications/lifecycle/#deprecated-modules","title":"Deprecated Modules","text":"<p>The two major criteria we use to decide which modules to deprecate are:</p> <ul> <li>A software module has not been used much in the past year</li> <li>We are ending support for the toolchain with which a module was built</li> </ul> <p>As we deprecate modules, every time you load a module that has been marked for removal a warning message will appear. The message state when the module will no appear in the module list. If you see such a message, we recommend you update your project to use a supported module as soon as possible or contacting us for help. </p>"},{"location":"applications/lifecycle/#toolchain-support","title":"Toolchain Support","text":"<p>The YCRC maintains a rolling two toolchain version support model. At any given time on a cluster, we aim to support two versions of each of the major toolchains, <code>foss</code> and <code>intel</code>. The two versions are separated by two years and new software is typically installed with the later version. When we introduce a new toolchain version, we phase out support for the oldest by marking software in that toolchain for deprecation. A few months later, software in the oldest toolchain version will be removed from the module list and no longer supported by the YCRC.</p>"},{"location":"applications/modules/","title":"Load Software with Modules","text":"<p>To facilitate the diverse work that happens on the YCRC clusters we compile, install, and manage software packages separately from those installed in standard system directories. We use EasyBuild to build, install, and manage packages. You can access these packages as Lmod modules. The modules involving compiled software are arranged into hierarchical toolchains that make dependencies more consistent when you load multiple modules.</p> <p>Warning</p> <p>Avoid loading Python or R modules simultaneously with conda environments. This will almost always break something.</p>"},{"location":"applications/modules/#find-modules","title":"Find Modules","text":""},{"location":"applications/modules/#all-available-modules","title":"All Available Modules","text":"<p>To list all available modules, run:</p> <pre><code>module avail\n</code></pre>"},{"location":"applications/modules/#search-for-modules","title":"Search For Modules","text":"<p>You can search for modules or extensions with <code>spider</code> and <code>avail</code>. For example, to find and list all Python version 3 modules, run:</p> <pre><code>module avail python/3\n</code></pre> <p>To find any module or extension that mentions python in its name or description, use the command:</p> <pre><code>module spider python\n</code></pre>"},{"location":"applications/modules/#get-module-help","title":"Get Module Help","text":"<p>You can get a brief description of a module and the url to the software's homepage by running:</p> <pre><code>module help modulename/version\n</code></pre> <p>If you don't find a commonly used software package you require, contact us with a software installation request. Otherwise, check out our installation guides to install it for yourself.</p>"},{"location":"applications/modules/#load-and-unload-modules","title":"Load and Unload Modules","text":""},{"location":"applications/modules/#load","title":"Load","text":"<p>The <code>module load</code> command modifies your environment so you can use the specified software package(s).</p> <ul> <li>This command is case-sensitive to module names.</li> <li>The <code>module load</code> command will load dependencies as needed, you don't need to load them separately.</li> <li>For batch jobs, add <code>module load</code> command(s) to your submission script.</li> </ul> <p>For example, to load <code>Python</code> version <code>3.8.6</code> and <code>BLAST+</code> version <code>2.11.0</code>, find modules with matching toolchain suffixes and run the command:</p> <pre><code>module load Python/3.8.6-GCCcore-10.2.0 BLAST+/2.11.0-GCCcore-10.2.0\n</code></pre> <p>Lmod will add <code>python</code> and the BLAST commands to your environment.  Since both of these modules were built with the <code>GCCcore/10.2.0</code> toolchain module, they will not load conflicting libraries. Recall you can see the other modules that were loaded by running <code>module list</code>.</p> <p>Module Defaults</p> <p>As new versions of software get installed and others are deprecated, the default module version can change over time. It is best practice to note the specific module versions you are using for a project and load those explicitly, e.g. <code>module load Python/3.8.6-GCCcore-10.2.0</code> not <code>module load Python</code>. This makes your work more reproducible and less likely to change unexpectedly in the future.</p>"},{"location":"applications/modules/#unload","title":"Unload","text":"<p>You can also unload a specific module that you've previously loaded:</p> <pre><code>module unload R\n</code></pre> <p>Or unload all modules at once with:</p> <pre><code>module purge\n</code></pre> <p>Purge Lightly</p> <p><code>module purge</code> will alert you to a sticky module that is always loaded called <code>StdEnv</code>. Avoid unloading <code>StdEnv</code> unless explicitly told to do so, othewise you will lose some important setup for the cluster you are on.</p>"},{"location":"applications/modules/#module-collections","title":"Module Collections","text":""},{"location":"applications/modules/#save-collections","title":"Save Collections","text":"<p>It can be a pain to enter a long list of modules every time you return to a project. Module collections allow you to create sets of modules to load together. This method is particularly useful if you have two or more module sets that may conflict with one another.</p> <p>Save a collection of modules by first loading all the modules you want to save together then run:</p> <pre><code>module save environment_name\n</code></pre> <p>(replace <code>environment_name</code> with something more meaningful to you)</p>"},{"location":"applications/modules/#restore-collections","title":"Restore Collections","text":"<p>Load a collection with <code>module restore</code>:</p> <pre><code>module restore environment_name\n</code></pre> <p>To modify a collection: <code>restore</code> it, make the desired changes by <code>load</code>ing and/or <code>unload</code>ing modules, then <code>save</code> it to the same name. </p>"},{"location":"applications/modules/#list-collections","title":"List Collections","text":"<p>To get a list of your collections, run:</p> <pre><code>module savelist\n</code></pre>"},{"location":"applications/modules/#ml-a-convinient-tool","title":"<code>ml</code>: A Convinient Tool","text":"<p>Lmod provides a convinient tool called <code>ml</code> to simplify all of the module commands. </p>"},{"location":"applications/modules/#list-module-loaded","title":"List Module Loaded","text":"<pre><code>ml\n</code></pre>"},{"location":"applications/modules/#load-modules","title":"Load Modules","text":"<pre><code>ml Python/3.8.6-GCCcore-10.2.0\n</code></pre>"},{"location":"applications/modules/#unload-modules","title":"Unload Modules","text":"<pre><code>ml -Python\n</code></pre>"},{"location":"applications/modules/#with-moudle-sub-commands","title":"With <code>moudle</code> Sub-commands","text":"<p><code>ml</code> can be used to replace the <code>module</code> command. It can take all the sub-commands from <code>module</code> and works the same way as <code>module</code> does.</p> <pre><code>ml load Python R   \nml unload Python\nml spider Python\nml avail\nml whatis Python\nml key Python\nml purge\nml save test\nml restore test\n</code></pre>"},{"location":"applications/modules/#environment-variables","title":"Environment Variables","text":"<p>To refer to the directory where the software from a module is stored, you can use the environment variable <code>$EBROOTMODULENAME</code> where MODULENAME is the name of the module in all caps with no spaces. This can be useful for finding the executables, libraries, or readme files that are included with the software:</p> <pre><code>[netid@node ~ ]$ module load SAMtools\n[netid@node ~ ]$ echo $EBVERSIONSAMTOOLS\n1.11\n[netid@node ~ ]$ ls $EBROOTSAMTOOLS\nbin  easybuild  include  lib  lib64  share\n[netid@node ~ ]$ ls $EBROOTSAMTOOLS/bin\nace2sam             maq2sam-short       psl2sam.pl             soap2sam.pl\nblast2sam.pl        md5fa               r2plot.lua             vcfutils.lua\nbowtie2sam.pl       md5sum-lite         sam2vcf.pl             wgsim\nexport2sam.pl       novo2sam.pl         samtools               wgsim_eval.pl\ninterpolate_sam.pl  plot-ampliconstats  samtools.pl            zoom2sam.pl\nmaq2sam-long        plot-bamstats       seq_cache_populate.pl\n</code></pre>"},{"location":"applications/modules/#further-reading","title":"Further Reading","text":"<p>You can view documentation while on the cluster using the command:</p> <pre><code>man module\n</code></pre> <p>There is even more information at the offical Lmod website and related documentation.</p>"},{"location":"applications/toolchains/","title":"Software Module Toolchains","text":"<p>The YCRC uses a framework called EasyBuild to build and install the software you access via the module system.</p>"},{"location":"applications/toolchains/#toolchains","title":"Toolchains","text":"<p>When we install software, we use pre-defined build environment modules called toolchains. These are modules that include dependencies like compilers and libraries such as GCC, OpenMPI, CUDA, etc. We do this to keep our build process simpler, and to ensure that sets of software modules loaded together function properly. The two groups of toolchains we use on the YCRC clusters are <code>foss</code> and <code>intel</code>, which hierarchically include some shared sub-toolchains. Toolchains will have versions associated with the version of the compiler and/or when the toolchain was composed. Toolchain names and versions are appended as suffixes in module names. This tells you that a module was built with that toolchain and which other modules are compatible with it. The YCRC maintains a rolling two toolchain version support model. The toolchain versions supported on each cluster are listed in the Module Lifecycle documentation.</p>"},{"location":"applications/toolchains/#free-open-source-software-foss","title":"Free Open Source Software (<code>foss</code>)","text":"<p>The <code>foss</code> toolchains are versioned with a yearletter scheme, e.g. <code>foss/2020b</code> is the second <code>foss</code> toolchain composed in 2020. Software modules that were built with a sub-toolchain, e.g. <code>GCCcore</code>, are still safe to load with their parents as long as their versions match. The major difference between <code>foss</code> and <code>fosscuda</code> is that <code>fosscuda</code> includes CUDA and builds applications for GPUs by default. You shoould only use <code>fosscuda</code> modules on nodes with GPUs. Below is a tree depicting which toolchains inherit each other.</p> <pre><code>foss: gompi + FFTW, OpenBLAS, ScaLAPACK\n\u2514\u2500\u2500 gompi: GCC + OpenMPI\n    \u2514\u2500\u2500 GCC: GCCcore + zlib, binutils\n        \u2514\u2500\u2500 GCCcore: GNU Compiler Collection\n\nfosscuda: gompic + FFTW, OpenBLAS, ScaLAPACK\n\u2514\u2500\u2500 gompic: gcccuda + CUDA-enabled OpenMPI\n    \u2514\u2500\u2500 gcccuda: GCC + CUDA\n        \u2514\u2500\u2500 GCC: GCCcore + zlib, binutils\n            \u2514\u2500\u2500 GCCcore: GNU Compiler Collection\n</code></pre>"},{"location":"applications/toolchains/#intel","title":"Intel","text":"<p>The YCRC licenses Intel Parallel Studio XE (Intel oneAPI Base &amp; HPC Toolkit coming soon). The <code>intel</code> and <code>iomkl</code> toolchains are versioned with a yearletter scheme, e.g. <code>intel/2020b</code> is the second <code>intel</code> toolchain composed in 2020. The major difference between <code>iomkl</code> and <code>intel</code> is MPI - <code>intel</code> uses Intel's MPI implementation and <code>iomkl</code> uses OpenMPI. Below is a tree depicting which toolchains inherit each other.</p> <pre><code>iomkl: iompi + Intel Math Kernel Library\n\u2514\u2500\u2500 iompi: iccifort + OpenMPI\n    \u2514\u2500\u2500 iccifort: Intel compilers\n        \u2514\u2500\u2500 GCCcore: GNU Compiler Collection\n\nintel: iimpi + Intel Math Kernel Library\n\u2514\u2500\u2500 iimpi: iccifort + Intel MPI\n    \u2514\u2500\u2500 iccifort: Intel C/C++/Fortran compilers\n        \u2514\u2500\u2500 GCCcore: GNU Compiler Collection\n</code></pre>"},{"location":"applications/toolchains/#what-versions-match","title":"What Versions Match?","text":"<p>To see what versions of sub-toolchains are compatible with their parents, load a <code>foss</code> or <code>intel</code> module of interest and run <code>module list</code>.</p> <pre><code>[netid@node ~]$ module load foss/2020b\n[netid@node ~]$ module list\n\nCurrently Loaded Modules:\n  1) StdEnv                        (S)   7) XZ/5.2.5-GCCcore-10.2.0           13) OpenMPI/4.0.5-GCC-10.2.0\n  2) GCCcore/10.2.0                      8) libxml2/2.9.10-GCCcore-10.2.0     14) OpenBLAS/0.3.12-GCC-10.2.0\n  3) zlib/1.2.11-GCCcore-10.2.0          9) libpciaccess/0.16-GCCcore-10.2.0  15) gompi/2020b\n  4) binutils/2.35-GCCcore-10.2.0       10) hwloc/2.2.0-GCCcore-10.2.0        16) FFTW/3.3.8-gompi-2020b\n  5) GCC/10.2.0                         11) UCX/1.9.0-GCCcore-10.2.0          17) ScaLAPACK/2.1.0-gompi-2020b\n  6) numactl/2.0.13-GCCcore-10.2.0      12) libfabric/1.11.0-GCCcore-10.2.0   18) foss/2020b\n\n  Where:\n   S:  Module is Sticky, requires --force to unload or purge\n</code></pre> <p>Here you see that <code>foss/2020b</code> includes <code>GCCcore/10.2.0</code>, so modules with either the <code>foss-2020b</code> or <code>GCCcore-10.2.0</code> should be compatible.</p>"},{"location":"clusters/","title":"HPC Resources","text":"<p>The YCRC maintains and supports a number of high performance computing systems for the Yale research community. Our high performance computing systems are named after notable members of the Yale community.</p> <p>Each YCRC cluster undergoes regular scheduled maintenance twice a year, see our maintenance schedule for more details.</p> <p>For proposals, we provide a description of our facilities, equipment, and other resources for HPC and research computing.</p>"},{"location":"clusters/#compute","title":"Compute","text":"<p>We maintain and support three Red Hat Linux compute clusters, listed below. Please click on cluster names for more information. </p> <p>Info</p> <p>The Farnam and Ruddle clusters were both retired in 2023 and their users are now supported on the McCleary cluster.</p> Cluster Name Approx. Core Count Approx. Node Count Login Address Purpose Bouchet 4,000 60 - Coming Soon (beta in Fall 2024) Grace 26,000 740 <code>grace.ycrc.yale.edu</code> general and highly parallel, tightly coupled (InfiniBand) McCleary 13,000 340 <code>mccleary.ycrc.yale.edu</code> medical and life science, YCGA Milgram 2,000 50 <code>milgram.ycrc.yale.edu</code> HIPAA and other sensitive data Misha 2,000 40 <code>misha.ycrc.yale.edu</code> Wu Tsai Institute"},{"location":"clusters/#storage","title":"Storage","text":"<p>We maintain several high performance storage systems. Listed below are these shared filesystems and the clusters where they are available. We distinguish where clusters store their home directories with an asterisk. The directory <code>/home</code> will always point to your home directory on the cluster you logged into. For more information about storage quotas and purchasing storage see the Cluster Storage page.</p> Name Path Size Mounting Clusters File System Software Purpose Roberts /nfs/roberts 3.6 PiB Bouchet* VAST Bouchet primary storage Palmer /vast/palmer 700 TiB Grace*, McCleary* VAST home, scratch storage, purchased project-style storage Gibbs /gpfs/gibbs 14.0 PiB Grace, McCleary IBM Spectrum Scale (GPFS) project, purchased project-style storage YCGA /gpfs/ycga 3.0 PiB McCleary IBM Spectrum Scale (GPFS) YCGA storage Milgram /gpfs/milgram 3.0 PiB Milgram* IBM Spectrum Scale (GPFS) Milgram primary storage Radev /gpfs/radev 2.0 PiB Misha* IBM Spectrum Scale (GPFS) Misha primary storage"},{"location":"clusters/bouchet/","title":"Bouchet","text":"<p>Yale recently joined the Massachusetts Green High Performance Computing Center (MGHPCC), a not-for-profit, state-of-the-art data center dedicated to computationally-intensive research. We are pleased to announce our first installation at MGHPCC will be a new HPC cluster called Bouchet. Bouchet is named for Edward Bouchet (1852-1918), the first self-identified African American to earn a doctorate from an American university, a PhD in physics at Yale University in 1876.</p>"},{"location":"clusters/bouchet/#announcing-the-bouchet-hpc-cluster","title":"Announcing the Bouchet HPC Cluster","text":"<p>Bouchet Beta</p> <p>The Bouchet HPC cluster is now available in beta for tightly coupled, parallel workloads. Please see the Bouchet Beta Testing documentation for more information.</p> <p>The first installation of nodes, approximately 4,000 direct-liquid-cooled cores, will be dedicated to tightly coupled parallel workflows, such as those run in the <code>mpi</code> partition on the Grace cluster.  Later on this year we will be acquiring and installing a large number of general purpose compute nodes as well as GPU-enabled compute nodes.  At that point Bouchet will be available to all Yale researchers for computational work involving low-risk data.</p> <p>Ultimately, Bouchet is the planned successor to both Grace and McCleary, with the majority of HPC infrastructure refreshes and growth deployed at MGHPCC going forward.  However, we are still in the early stages of planning that transition and will continue to operate both Grace and McCleary in their current form for a number of years.  More details will be provided as we consult with faculty and researchers about the transition and how we can minimize disruptions to critical work.  To this effect, we will be convening a faculty advisory committee this fall to ensure a smooth migration.</p> <p>If you have any questions about Yale\u2019s partnership at MGHPCC or the Bouchet cluster, please reach out to us.</p>"},{"location":"clusters/bouchet/#access-the-cluster","title":"Access the Cluster","text":"<p>Once you have an account, the cluster can be accessed via ssh.</p>"},{"location":"clusters/bouchet/#system-status-and-monitoring","title":"System Status and Monitoring","text":"<p>For system status messages and the schedule for upcoming maintenance, please see the system status page.  For a current node-level view of job activity, see the cluster monitor page (VPN only).</p>"},{"location":"clusters/bouchet/#partitions-and-hardware","title":"Partitions and Hardware","text":""},{"location":"clusters/bouchet/#public-partitions","title":"Public Partitions","text":"<p>See each tab below for more information about the available common use partitions.</p> devel <p>Use the devel partition to jobs with which you need ongoing interaction. For example, exploratory analyses or debugging compilation.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the devel partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>06:00:00</code> Maximum CPUs per user <code>10</code> Maximum memory per user <code>70G</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 1 6426Y 32 251 cpugen:sapphirerapids,cpumodel:6426Y,common:yes mpi <p>Use the mpi partition for tightly-coupled parallel programs that make efficient use of multiple nodes. See our MPI documentation if your workload fits this description.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --exclusive --mem=498688\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the mpi partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>1-00:00:00</code> Maximum nodes per group <code>58</code> Maximum nodes per user <code>58</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 60 8562Y+ 64 479 cpugen:emeraldrerapids, cpumodel:8562Y+, common:yes"},{"location":"clusters/bouchet/#storage","title":"Storage","text":"<p>Bouchet has access to one filesystem called Roberts.  Roberts is an all-flash, NFS filesystem similar to the Palmer filesystem on Grace and McCleary. For more details on the different storage spaces, see our Cluster Storage documentation.</p> <p>Your <code>~/project</code> and <code>~/scratch</code> directories are shortcuts.  Get a list of the absolute paths to your directories with the <code>mydirectories</code> command.  If you want to share data in your Project or Scratch directory, see the permissions page.</p> <p>For information on data recovery, see the Backups and Snapshots documentation.</p> <p>Warning</p> <p>Files stored in <code>scratch</code> are purged if they are older than 60 days. You will receive an email alert one week before they are deleted. Artificial extension of scratch file expiration is forbidden without explicit approval from the YCRC. Please purchase storage if you need additional longer term storage.</p> Partition Root Directory Storage File Count Backups Snapshots Notes home <code>/home</code> 125GiB/user 500,000 Not yet &gt;=2 days project <code>/nfs/roberts/project</code> 1TiB/group, increase to 4TiB on request 5,000,000 No &gt;=2 days scratch <code>/nfs/roberts/scratch</code> 10TiB/group 15,000,000 No No"},{"location":"clusters/bouchet_beta/","title":"Bouchet Beta Testing","text":"<p>The Bouchet HPC cluster is YCRC's first installation at Massachusetts High Performance Computing Center (MGHPCC).  Bouchet is the planned successor to both Grace and McCleary, with the majority of HPC infrastructure refreshes and growth deployed at MGHPCC going forward. The first installations of nodes, approximately 4,000 direct-liquid-cooled cores, will be dedicated to tightly coupled parallel workflows using <code>mpi</code>.  A second set of nodes will be installed in early 2025 aimed at general purpose workflows, including GPU-accelerated work. We encourage researchers to participate in the beta as it will help ensure that the production cluster will run your work without issue and provide optimal performance. </p> <p>Compared to the existing <code>mpi</code> partition Bouchet provides several key improvements:</p> <ol> <li>Bouchet MPI nodes are 64-core Emerald Rapids Platinum 8562Y+ nodes, which are several generations newer compared to current Skylake 6136 nodes </li> <li>Each compute node in Bouchet has 487GiB of usable RAM, significantly more than the 88GiB/node currently available</li> <li>The installed software packages in Bouchet are compiled specifically for the Emerald Rapids architecture to provide optimal performance</li> </ol> <p>Warning</p> <p>While in beta, Bouchet may become unavailable with little or no notice as the YCRC configures and troubleshoots the system for production deployment. No data on Bouchet is currently backed up. Bouchet is expected to stay in beta until at least mid-January.</p>"},{"location":"clusters/bouchet_beta/#access-the-cluster","title":"Access the Cluster","text":"<p>Access to the Bouchet Beta can be requested via the Beta Access Request From. For the beta period,  we are explicitly and exclusively seeking tightly coupled, parallel workloads.</p> <p>Once you have an account, the cluster can be accessed via ssh.  You can log in with the command:</p> <pre><code>ssh NETID@bouchet.ycrc.yale.edu\n</code></pre> <p>Open OnDemand for Bouchet will be available in the future. Additionally, the implementation of certain utility commands that are available on other clusters is still in progress.  </p>"},{"location":"clusters/bouchet_beta/#key-differences-from-other-clusters","title":"Key Differences from Other Clusters","text":""},{"location":"clusters/bouchet_beta/#primary-group","title":"Primary Group","text":"<p>On non-Bouchet clusters, your PI name is the primary group of your account.  On Bouchet, your primary group is your NetID, and secondary groups are assigned for any PI group you belong to. This setup makes the process of group change easier and can also accomodate \"project\"-based secondary groups rather than PI-based secondary groups. PI groups on Bouchet take the form <code>pi_&lt;netid of the pi&gt;</code>, instead of <code>&lt;lastname of pi&gt;</code> to avoid collisions and confusion between PIs who share a lastname.   Files created in PI-owned project and scratch directories will inherit the correct PI group-ownership. However, be careful about copying existing files from <code>$HOME</code> to project spaces, as those files may need to have their group ownership updated. </p>"},{"location":"clusters/bouchet_beta/#partition","title":"Partition","text":"<p>Currently, <code>devel</code> and <code>mpi</code> partitions are available.  For detailed information about job limits and avalable compute nodes in each partition, please refer to our Bouchet partition documentation.  Please use <code>devel</code> partition for code development, debugging, and compilation.  Jobs submitted to <code>mpi</code> partitions need to request at least two nodes and are allocated full nodes.   </p>"},{"location":"clusters/bouchet_beta/#storage","title":"Storage","text":"<p>Bouchet's filesystem, Roberts, is an all-flash storage system from VAST data and does not have a GPFS filesystem.  <code>/nfs/roberts/</code> hosts Bouchet's home, project, and scratch directories.  Your project and scratch storage usage and quota are shared with the members of the associated secondary group. </p> Storage Root Directory Quota File Count home <code>/nfs/roberts/home</code> 125GiB/user 500,000 project <code>/nfs/roberts/project</code> 1TiB/group, increase to 4TiB on request 5,000,000 scratch <code>/nfs/roberts/scratch</code> 10TiB/group 15,000,000"},{"location":"clusters/bouchet_beta/#transfer-data-from-other-clusters","title":"Transfer data from other clusters","text":"<p>To transfer data from other clusters to Bouchet, we encourage using <code>rsync</code>.  <code>rsync</code> is a commonly used command-line tool for remote transfers between two systems.  Globus is not yet available on Bouchet.</p> <p>Before getting started with the transfer with <code>rsync</code>, we first need to enable access to Bouchet from other clusters.  Please run the following command on Bouchet:</p> <pre><code>cat ~/.ssh/id_rsa.pub\n</code></pre> <p>and copy and paste the output to our SSH key uploader.  The propagation of this public key to other clusters can take a few minutes. </p> <p>To initiate the transfer from Bouchet cluster, log into the <code>transfer</code> node via ssh:</p> <p><pre><code>[an492@login1.bouchet ~]$ ssh transfer1\n[an492@transfer1.bouchet ~]$\n</code></pre> and run the <code>rsync</code> commands on the <code>transfer</code> node. </p> <p>We recommend using the following flags with the <code>rsync</code> command: <pre><code>rsync -avP NETID@transfer-CLUSTER.ycrc.yale.edu:/path/to/existing/data /path/to/new/home/for/data\n</code></pre> Here the <code>-a</code> will run transfer in <code>archive</code> mode, which preserves ownership, permissions, and creation/modification times. Additionally, the <code>-v</code> will run in <code>verbose</code> mode where the name of every file is printed out, and <code>-P</code> displays a progress bar. </p> <p>As an example, to transfer a directory (named <code>mydata</code>) from Grace project directory to Bouchet project directory: <pre><code>rsync -avP NETID@transfer-grace.ycrc.yale.edu:/gpfs/gibbs/project/GROUP/NETID/mydata /nfs/roberts/project/GROUP/NETID \n</code></pre></p> <p>For <code>rsync</code> transfers that may take a while, it is best to run the transfer inside a tmux sesseion. </p>"},{"location":"clusters/bouchet_beta/#applications-and-software","title":"Applications and software","text":"<p>Commonly used software is available as modules, similar to other clusters.  Currently, all software is compiled and installed with the 2022b version of the toolchain on Bouchet, even if the same software version is installed with an older toolchain (e.g. 2020b) on other clusters. If you would like to compile your own code specifically to the Bouchet MPI compute node architecture, you can request an interactive compute session in the <code>devel</code> partition of Bouchet.  Because Bouchet does not have a GPFS filesystem, be sure to turn off any GPFS related optimization configuration. </p>"},{"location":"clusters/bouchet_beta/#report-issues","title":"Report Issues","text":"<p>If you discover issues when running your workflow or experience performance issues, feel free to contact us for assistance.  Please include the working directory, the commands that were run, the software modules used, and any more information needed to reproduce the issue.</p>"},{"location":"clusters/grace/","title":"Grace","text":"<p>Grace is a shared-use resource for the Faculty of Arts and Sciences (FAS). It consists of a variety of compute nodes networked over low-latency InfiniBand and mounts several shared filesystems.</p> <p>The Grace cluster is is named for the computer scientist and United States Navy Rear Admiral Grace Murray Hopper, who received her Ph.D. in Mathematics from Yale in 1934.</p> <p>Operating System Upgrade</p> <p>During the August 2023 maintenance, the operating system on Grace was upgraded from Red Hat 7 to Red Hat 8. For more information, see our Grace Operating System Upgrade page. </p>"},{"location":"clusters/grace/#access-the-cluster","title":"Access the Cluster","text":"<p>Once you have an account, the cluster can be accessed via ssh or through the Open OnDemand web portal.</p>"},{"location":"clusters/grace/#system-status-and-monitoring","title":"System Status and Monitoring","text":"<p>For system status messages and the schedule for upcoming maintenance, please see the system status page. For a current node-level view of job activity, see the cluster monitor page (VPN only).</p>"},{"location":"clusters/grace/#partitions-and-hardware","title":"Partitions and Hardware","text":"<p>Grace is made up of several kinds of compute nodes. We group them into (sometimes overlapping) Slurm partitions meant to serve different purposes. By combining the <code>--partition</code> and <code>--constraint</code> Slurm options you can more finely control what nodes your jobs can run on.</p> <p>Job Submission Limits</p> <ul> <li> <p>You are limited to 4 interactive app instances (of any type) at one time.  Additional instances will be rejected until you delete older open instances.  For OnDemand jobs, closing the window does not terminate the interactive app job. To terminate the job, click the \"Delete\" button in your \"My Interactive Apps\" page in the web portal.</p> </li> <li> <p>Job submissions are limited to 200 jobs per hour. See the Rate Limits section in the Common Job Failures page for more info.</p> </li> </ul>"},{"location":"clusters/grace/#public-partitions","title":"Public Partitions","text":"<p>See each tab below for more information about the available common use partitions.</p> day <p>Use the day partition for most batch jobs. This is the default if you don't specify one with <code>--partition</code>.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the day partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>1-00:00:00</code> Maximum CPUs per group <code>2500</code> Maximum CPUs per user <code>1000</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 66 6342 48 487 icelake, avx512, 6342, nogpu, standard, common, bigtmp 72 8268 48 355 cascadelake, avx512, 8268, nogpu, standard, common, bigtmp, oldest 130 6240 36 181 cascadelake, avx512, 6240, nogpu, standard, common, bigtmp, oldest devel <p>Use the devel partition to jobs with which you need ongoing interaction. For example, exploratory analyses or debugging compilation.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the devel partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>06:00:00</code> Maximum CPUs per user <code>4</code> Maximum memory per user <code>32G</code> Maximum submitted jobs per user <code>1</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 5 6240 36 181 cascadelake, avx512, 6240, nogpu, standard, common, bigtmp, oldest 1 6126 24 174 skylake, avx512, 6126, nogpu, standard, common week <p>Use the week partition for jobs that need a longer runtime than day allows.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the week partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> Maximum CPUs per group <code>252</code> Maximum CPUs per user <code>108</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 18 6342 48 487 icelake, avx512, 6342, nogpu, standard, common, bigtmp transfer <p>Use the transfer partition to stage data for your jobs to and from cluster storage.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the transfer partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>1-00:00:00</code> Maximum running jobs per user <code>2</code> Maximum CPUs per job <code>1</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 2 7642 8 237 epyc, 7642, nogpu, standard, common gpu <p>Use the gpu partition for jobs that make use of GPUs. You must request GPUs explicitly with the <code>--gpus</code> option in order to use them. For example, <code>--gpus=gtx1080ti:2</code> would request 2 GeForce GTX 1080Ti GPUs per node.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the gpu partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>2-00:00:00</code> Maximum GPUs per user <code>24</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 2 6342 48 984 a100 4 80 icelake, avx512, 6342, doubleprecision, bigtmp, common, gpu, a100, a100-80g 11 6326 32 206 a5000 4 24 icelake, avx512, 6326, doubleprecision, a5000, common 1 6326 32 984 a100 4 80 icelake, avx512, 6326, doubleprecision, bigtmp, common, gpu, a100, a100-80g 3 6240 36 181 rtx2080ti 4 11 cascadelake, avx512, 6240, singleprecision, common, bigtmp, rtx2080ti, oldest 2 6240 36 361 a100 4 40 cascadelake, avx512, 6240, doubleprecision, bigtmp, common, a100, a100-40g, oldest 2 5222 8 181 rtx5000 4 16 cascadelake, avx512, 5222, doubleprecision, common, bigtmp, rtx5000, oldest 4 6240 36 370 v100 4 16 cascadelake, avx512, 6240, doubleprecision, common, v100, oldest 2 6136 24 90 v100 2 16 skylake, avx512, 6136, doubleprecision, common, bigtmp, v100 gpu_devel <p>Use the gpu_devel partition to debug jobs that make use of GPUs, or to develop GPU-enabled code.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the gpu_devel partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>06:00:00</code> Maximum CPUs per user <code>10</code> Maximum GPUs per user <code>3</code> Maximum submitted jobs per user <code>2</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 1 6326 32 984 a100 4 80 icelake, avx512, 6326, doubleprecision, bigtmp, common, gpu, a100, a100-80g 1 6326 32 206 a5000 4 24 icelake, avx512, 6326, doubleprecision, a5000, common 1 6240 36 370 v100 4 16 cascadelake, avx512, 6240, doubleprecision, common, v100, oldest 2 6240 36 181 rtx2080ti 4 11 cascadelake, avx512, 6240, singleprecision, common, bigtmp, rtx2080ti, oldest 2 6240 36 166 rtx3090 4 24 cascadelake, avx512, 6240, doubleprecision, bigtmp, common, rtx3090, oldest 4 5222 8 181 rtx5000 4 16 cascadelake, avx512, 5222, doubleprecision, common, bigtmp, rtx5000, oldest bigmem <p>Use the bigmem partition for jobs that have memory requirements other partitions can't handle.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the bigmem partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>1-00:00:00</code> Maximum CPUs per user <code>40</code> Maximum memory per user <code>4000G</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 3 6240 36 1505 cascadelake, avx512, 6240, nogpu, common, bigtmp, oldest 2 6234 16 1505 cascadelake, avx512, nogpu, 6234, common, bigtmp, oldest 4 6346 32 3935 cascadelake, avx512, 6346, common, nogpu, bigtmp, oldest mpi <p>Use the mpi partition for tightly-coupled parallel programs that make efficient use of multiple nodes. See our MPI documentation if your workload fits this description.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --exclusive --mem=90112\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the mpi partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>1-00:00:00</code> Maximum nodes per group <code>64</code> Maximum nodes per user <code>64</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 122 6136 24 88 hdr, skylake, avx512, 6136, nogpu, standard, common, bigtmp scavenge <p>Use the scavenge partition to run preemptable jobs on more resources than normally allowed. For more information about scavenge, see the Scavenge documentation.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the scavenge partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>1-00:00:00</code> Maximum CPUs per user <code>10000</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 50 6342 48 487 icelake, avx512, 6342, nogpu, standard, pi, bigtmp 1 6326 32 983 l40s 4 48 icelake, avx512, 6326, pi, standard, bigtmp, l40s 6 6342 48 1999 icelake, avx512, 6342, nogpu, standard, pi, bigtmp 84 6342 48 487 icelake, avx512, 6342, nogpu, standard, common, bigtmp 2 6342 48 984 a100 4 80 icelake, avx512, 6342, doubleprecision, bigtmp, common, gpu, a100, a100-80g 11 6326 32 206 a5000 4 24 icelake, avx512, 6326, doubleprecision, a5000, common 2 6326 32 984 a100 4 80 icelake, avx512, 6326, doubleprecision, bigtmp, common, gpu, a100, a100-80g 2 6326 32 206 a5000 4 24 icelake, avx512, 6326, doubleprecision, a5000, pi 2 6326 32 468 a5000 4 24 icelake, avx512, 6326, doubleprecision, a5000, pi 72 8268 48 355 cascadelake, avx512, 8268, nogpu, standard, pi, bigtmp, oldest 72 8268 48 355 cascadelake, avx512, 8268, nogpu, standard, common, bigtmp, oldest 130 6240 36 181 cascadelake, avx512, 6240, nogpu, standard, common, bigtmp, oldest 87 6240 36 181 cascadelake, avx512, 6240, nogpu, standard, pi, bigtmp, oldest 4 6240 36 181 rtx2080ti 4 11 cascadelake, avx512, 6240, singleprecision, pi, bigtmp, rtx2080ti, oldest 3 6240 36 1505 cascadelake, avx512, 6240, nogpu, common, bigtmp, oldest 3 6240 36 181 rtx2080ti 4 11 cascadelake, avx512, 6240, singleprecision, common, bigtmp, rtx2080ti, oldest 1 6254 36 370 rtx2080ti 8 11 cascadelake, avx512, 6254, singleprecision, pi, bigtmp, rtx2080ti, oldest 2 6240 36 370 v100 4 16 cascadelake, avx512, 6240, doubleprecision, pi, v100, oldest 2 6240 36 179 rtx3090 4 24 cascadelake, avx512, 6240, doubleprecision, bigtmp, pi, rtx3090, oldest 2 6240 36 361 a100 4 40 cascadelake, avx512, 6240, doubleprecision, bigtmp, common, a100, a100-40g, oldest 2 6234 16 1505 cascadelake, avx512, nogpu, 6234, common, bigtmp, oldest 2 5222 8 181 rtx5000 4 16 cascadelake, avx512, 5222, doubleprecision, common, bigtmp, rtx5000, oldest 20 8260 96 181 cascadelake, avx512, 8260, nogpu, pi, oldest 4 6240 36 370 v100 4 16 cascadelake, avx512, 6240, doubleprecision, common, v100, oldest 1 6326 32 1001 a100 4 80 cascadelake, avx512, 6326, doubleprecision, bigtmp, pi, a100, a100-80g, oldest 4 6346 32 3935 cascadelake, avx512, 6346, common, nogpu, bigtmp, oldest 3 6234 16 1505 cascadelake, avx512, nogpu, 6234, pi, bigtmp, oldest 8 6240 36 370 cascadelake, avx512, 6240, nogpu, pi, bigtmp, oldest 16 6136 24 90 edr, skylake, avx512, 6136, nogpu, standard, pi, bigtmp 4 6136 24 88 hdr, skylake, avx512, 6136, nogpu, standard, pi, bigtmp 12 6136 24 90 hdr, skylake, avx512, 6136, nogpu, standard, pi, bigtmp 3 6142 32 181 skylake, avx512, 6142, nogpu, standard, pi, bigtmp 9 6136 24 181 p100 4 16 skylake, avx512, 6136, doubleprecision, pi, p100 4 6136 24 90 hdr, skylake, avx512, 6136, nogpu, pi, common, bigtmp 1 6136 24 749 skylake, avx512, 6136, nogpu, pi, bigtmp 2 6136 24 90 v100 2 16 skylake, avx512, 6136, doubleprecision, common, bigtmp, v100 2 5122 8 181 rtx2080 4 8 skylake, avx512, 5122, singleprecision, pi, rtx2080 scavenge_gpu <p>Use the scavenge_gpu partition to run preemptable jobs on more GPU resources than normally allowed. For more information about scavenge, see the Scavenge documentation.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the scavenge_gpu partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>1-00:00:00</code> Maximum GPUs per user <code>30</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 1 6326 32 983 l40s 4 48 icelake, avx512, 6326, pi, standard, bigtmp, l40s 2 6342 48 984 a100 4 80 icelake, avx512, 6342, doubleprecision, bigtmp, common, gpu, a100, a100-80g 11 6326 32 206 a5000 4 24 icelake, avx512, 6326, doubleprecision, a5000, common 2 6326 32 984 a100 4 80 icelake, avx512, 6326, doubleprecision, bigtmp, common, gpu, a100, a100-80g 2 6326 32 206 a5000 4 24 icelake, avx512, 6326, doubleprecision, a5000, pi 2 6326 32 468 a5000 4 24 icelake, avx512, 6326, doubleprecision, a5000, pi 4 6240 36 181 rtx2080ti 4 11 cascadelake, avx512, 6240, singleprecision, pi, bigtmp, rtx2080ti, oldest 3 6240 36 181 rtx2080ti 4 11 cascadelake, avx512, 6240, singleprecision, common, bigtmp, rtx2080ti, oldest 1 6254 36 370 rtx2080ti 8 11 cascadelake, avx512, 6254, singleprecision, pi, bigtmp, rtx2080ti, oldest 2 6240 36 370 v100 4 16 cascadelake, avx512, 6240, doubleprecision, pi, v100, oldest 2 6240 36 179 rtx3090 4 24 cascadelake, avx512, 6240, doubleprecision, bigtmp, pi, rtx3090, oldest 2 6240 36 361 a100 4 40 cascadelake, avx512, 6240, doubleprecision, bigtmp, common, a100, a100-40g, oldest 2 5222 8 181 rtx5000 4 16 cascadelake, avx512, 5222, doubleprecision, common, bigtmp, rtx5000, oldest 4 6240 36 370 v100 4 16 cascadelake, avx512, 6240, doubleprecision, common, v100, oldest 1 6326 32 1001 a100 4 80 cascadelake, avx512, 6326, doubleprecision, bigtmp, pi, a100, a100-80g, oldest 9 6136 24 181 p100 4 16 skylake, avx512, 6136, doubleprecision, pi, p100 2 6136 24 90 v100 2 16 skylake, avx512, 6136, doubleprecision, common, bigtmp, v100 2 5122 8 181 rtx2080 4 8 skylake, avx512, 5122, singleprecision, pi, rtx2080 scavenge_mpi <p>Use the scavenge_mpi partition to run preemptable jobs on more MPI resources than normally allowed. For more information about scavenge, see the Scavenge documentation.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --exclusive --mem=90112\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the scavenge_mpi partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>1-00:00:00</code> Maximum nodes per group <code>64</code> Maximum nodes per user <code>64</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 128 6136 24 88 hdr, skylake, avx512, 6136, nogpu, standard, common, bigtmp"},{"location":"clusters/grace/#private-partitions","title":"Private Partitions","text":"<p>With few exceptions, jobs submitted to private partitions are not considered when calculating your group's Fairshare. Your group can purchase additional hardware for private use, which we will make available as a <code>pi_groupname</code> partition. These nodes are purchased by you, but supported and administered by us. After vendor support expires, we retire compute nodes. Compute nodes can range from $10K to upwards of $50K depending on your requirements. If you are interested in purchasing nodes for your group, please contact us.</p> PI Partitions (click to expand) pi_anticevic <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the pi_anticevic partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>100-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 2 6342 48 1999 icelake, avx512, 6342, nogpu, standard, pi, bigtmp 15 6342 48 487 icelake, avx512, 6342, nogpu, standard, pi, bigtmp 2 6326 32 206 a5000 4 24 icelake, avx512, 6326, doubleprecision, a5000, pi pi_balou <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_balou partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 9 6240 36 181 cascadelake, avx512, 6240, nogpu, standard, pi, bigtmp, oldest 14 8268 48 355 cascadelake, avx512, 8268, nogpu, standard, pi, bigtmp, oldest pi_berry <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_berry partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 1 8268 48 355 cascadelake, avx512, 8268, nogpu, standard, pi, bigtmp, oldest 1 6240 36 181 cascadelake, avx512, 6240, nogpu, standard, pi, bigtmp, oldest pi_chem_chase <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=3840\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the pi_chem_chase partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 8 6240 36 181 cascadelake, avx512, 6240, nogpu, standard, pi, bigtmp, oldest 1 6240 36 181 rtx2080ti 4 11 cascadelake, avx512, 6240, singleprecision, pi, bigtmp, rtx2080ti, oldest pi_cowles <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_cowles partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> Maximum CPUs per user <code>120</code> Maximum nodes per user <code>5</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 9 8268 48 355 cascadelake, avx512, 8268, nogpu, standard, pi, bigtmp, oldest pi_econ_io <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_econ_io partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 6 6240 36 181 cascadelake, avx512, 6240, nogpu, standard, pi, bigtmp, oldest pi_econ_lp <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_econ_lp partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 6 6342 48 487 icelake, avx512, 6342, nogpu, standard, pi, bigtmp 5 6240 36 181 cascadelake, avx512, 6240, nogpu, standard, pi, bigtmp, oldest 7 8268 48 355 cascadelake, avx512, 8268, nogpu, standard, pi, bigtmp, oldest 1 6234 16 1505 cascadelake, avx512, nogpu, 6234, pi, bigtmp, oldest pi_esi <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_esi partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> Maximum CPUs per user <code>648</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 36 6240 36 181 cascadelake, avx512, 6240, nogpu, standard, pi, bigtmp, oldest pi_fedorov <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=3840\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_fedorov partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 6 6342 48 487 icelake, avx512, 6342, nogpu, standard, pi, bigtmp 12 6136 24 90 hdr, skylake, avx512, 6136, nogpu, standard, pi, bigtmp 4 6136 24 90 hdr, skylake, avx512, 6136, nogpu, pi, common, bigtmp pi_gelernter <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_gelernter partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 1 6240 36 181 cascadelake, avx512, 6240, nogpu, standard, pi, bigtmp, oldest pi_hammes_schiffer <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=3840\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the pi_hammes_schiffer partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 6 8268 48 355 cascadelake, avx512, 8268, nogpu, standard, pi, bigtmp, oldest 8 6240 36 181 cascadelake, avx512, 6240, nogpu, standard, pi, bigtmp, oldest 1 6240 36 181 rtx2080ti 4 11 cascadelake, avx512, 6240, singleprecision, pi, bigtmp, rtx2080ti, oldest 16 6136 24 90 edr, skylake, avx512, 6136, nogpu, standard, pi, bigtmp 1 6136 24 749 skylake, avx512, 6136, nogpu, pi, bigtmp 2 5122 8 181 rtx2080 4 8 skylake, avx512, 5122, singleprecision, pi, rtx2080 pi_hodgson <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_hodgson partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 1 6240 36 181 cascadelake, avx512, 6240, nogpu, standard, pi, bigtmp, oldest pi_holland <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_holland partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 2 6342 48 487 icelake, avx512, 6342, nogpu, standard, pi, bigtmp 8 6240 36 181 cascadelake, avx512, 6240, nogpu, standard, pi, bigtmp, oldest pi_howard <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_howard partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 1 6240 36 181 cascadelake, avx512, 6240, nogpu, standard, pi, bigtmp, oldest pi_jorgensen <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_jorgensen partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 3 6240 36 181 cascadelake, avx512, 6240, nogpu, standard, pi, bigtmp, oldest pi_kim_theodore <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_kim_theodore partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 1 8268 48 355 cascadelake, avx512, 8268, nogpu, standard, pi, bigtmp, oldest 1 6234 16 1505 cascadelake, avx512, nogpu, 6234, pi, bigtmp, oldest pi_korenaga <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_korenaga partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 3 8268 48 355 cascadelake, avx512, 8268, nogpu, standard, pi, bigtmp, oldest pi_lederman <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the pi_lederman partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 1 6254 36 1505 rtx4000,rtx8000,v100 4,2,2 8,48,16 cascadelake, avx512, 6254, pi, bigtmp, rtx8000, oldest pi_levine <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=1952\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_levine partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 20 8260 96 181 cascadelake, avx512, 8260, nogpu, pi, oldest pi_lora <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=3840\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_lora partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 5 8268 48 355 cascadelake, avx512, 8268, nogpu, standard, pi, bigtmp, oldest 4 6136 24 88 hdr, skylake, avx512, 6136, nogpu, standard, pi, bigtmp pi_manohar <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_manohar partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>180-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 6 6342 48 487 icelake, avx512, 6342, nogpu, standard, pi, bigtmp 2 8268 48 355 cascadelake, avx512, 8268, nogpu, standard, pi, bigtmp, oldest 4 6240 36 181 cascadelake, avx512, 6240, nogpu, standard, pi, bigtmp, oldest pi_mingarelli <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_mingarelli partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 4 6342 48 1999 icelake, avx512, 6342, nogpu, standard, pi, bigtmp 6 6342 48 487 icelake, avx512, 6342, nogpu, standard, pi, bigtmp pi_ohern <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the pi_ohern partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 2 6342 48 487 icelake, avx512, 6342, nogpu, standard, pi, bigtmp 2 6240 36 181 cascadelake, avx512, 6240, nogpu, standard, pi, bigtmp, oldest 8 8268 48 355 cascadelake, avx512, 8268, nogpu, standard, pi, bigtmp, oldest 9 6136 24 181 p100 4 16 skylake, avx512, 6136, doubleprecision, pi, p100 pi_owen_miller <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_owen_miller partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 2 8268 48 355 cascadelake, avx512, 8268, nogpu, standard, pi, bigtmp, oldest 1 6234 16 1505 cascadelake, avx512, nogpu, 6234, pi, bigtmp, oldest pi_padmanabhan <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_padmanabhan partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 3 6240 36 181 cascadelake, avx512, 6240, nogpu, standard, pi, bigtmp, oldest pi_panda <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the pi_panda partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 2 6342 48 487 icelake, avx512, 6342, nogpu, standard, pi, bigtmp 2 6326 32 468 a5000 4 24 icelake, avx512, 6326, doubleprecision, a5000, pi 1 6254 36 370 rtx2080ti 8 11 cascadelake, avx512, 6254, singleprecision, pi, bigtmp, rtx2080ti, oldest 2 6240 36 370 v100 4 16 cascadelake, avx512, 6240, doubleprecision, pi, v100, oldest 3 6240 36 181 rtx2080ti 4 11 cascadelake, avx512, 6240, singleprecision, pi, bigtmp, rtx2080ti, oldest 1 6326 32 1001 a100 4 80 cascadelake, avx512, 6326, doubleprecision, bigtmp, pi, a100, a100-80g, oldest pi_poland <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_poland partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 8 6240 36 370 cascadelake, avx512, 6240, nogpu, pi, bigtmp, oldest pi_polimanti <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_polimanti partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 2 6342 48 487 icelake, avx512, 6342, nogpu, standard, pi, bigtmp 2 6240 36 181 cascadelake, avx512, 6240, nogpu, standard, pi, bigtmp, oldest pi_seto <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_seto partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 3 6142 32 181 skylake, avx512, 6142, nogpu, standard, pi, bigtmp pi_spielman <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_spielman partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 1 8268 48 355 cascadelake, avx512, 8268, nogpu, standard, pi, bigtmp, oldest pi_sweeney <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the pi_sweeney partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 2 6240 36 179 rtx3090 4 24 cascadelake, avx512, 6240, doubleprecision, bigtmp, pi, rtx3090, oldest pi_tsmith <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_tsmith partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 2 6342 48 487 icelake, avx512, 6342, nogpu, standard, pi, bigtmp 1 8268 48 355 cascadelake, avx512, 8268, nogpu, standard, pi, bigtmp, oldest pi_vaccaro <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_vaccaro partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 1 6342 48 487 icelake, avx512, 6342, nogpu, standard, pi, bigtmp pi_ying_rex <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the pi_ying_rex partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 1 6326 32 983 l40s 4 48 icelake, avx512, 6326, pi, standard, bigtmp, l40s pi_zhu <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_zhu partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 12 8268 48 355 cascadelake, avx512, 8268, nogpu, standard, pi, bigtmp, oldest 6 6136 24 88 hdr, skylake, avx512, 6136, nogpu, standard, common, bigtmp"},{"location":"clusters/grace/#storage","title":"Storage","text":"<p>Grace has access to a number of filesystems. <code>/vast/palmer</code> hosts Grace's home and scratch directories and <code>/gpfs/gibbs</code> hosts project directories and most additional purchased storage allocations. For more details on the different storage spaces, see our Cluster Storage documentation.</p> <p>You can check your current storage usage &amp; limits by running the <code>getquota</code> command. Your <code>~/project</code> and <code>~/palmer_scratch</code> directories are shortcuts. Get a list of the absolute paths to your directories with the <code>mydirectories</code> command. If you want to share data in your Project or Scratch directory, see the permissions page.</p> <p>For information on data recovery, see the Backups and Snapshots documentation.</p> <p>Warning</p> <p>Files stored in <code>palmer_scratch</code> are purged if they are older than 60 days. You will receive an email alert one week before they are deleted. Artificial extension of scratch file expiration is forbidden without explicit approval from the YCRC. Please purchase storage if you need additional longer term storage.</p> Partition Root Directory Storage File Count Backups Snapshots Notes home <code>/vast/palmer/home.grace</code> 125GiB/user 500,000 Yes &gt;=2 days project <code>/gpfs/gibbs/project</code> 1TiB/group, increase to 4TiB on request 5,000,000 No &gt;=2 days scratch <code>/vast/palmer/scratch</code> 10TiB/group 15,000,000 No No"},{"location":"clusters/grace_rhel8/","title":"Grace Operating System Upgrade","text":"<p>Grace's previous operating system, Red Hat (RHEL) 7, will be offically end-of-life in 2024 and will no longer be supported with security patches by the developer. Therefore Grace has been upgraded to RHEL 8 during the August maintenance window, August 15-17, 2023. This provides a number of key benefits to Grace:</p> <ol> <li>consistency with the McCleary cluster</li> <li>continued security patches and support beyond 2023</li> <li>updated system libraries to better support modern software</li> <li>improved node management system to facilitate the growing number of nodes on Grace</li> <li>shared application tree between McCleary and Grace, which brings software parity between the clusters*</li> </ol> <p>* some software and workflows will only be supported by YCRC staff on one of the cluster, e.g. tightly couple MPI codes (Grace) or RELION (McCleary).</p>"},{"location":"clusters/grace_rhel8/#new-host-key","title":"New Host Key","text":"<p>The ssh host key for Grace's login nodes were changed during the August maintenance, which will result in an error similar to the following when you attempt to login for the first time after the maintenance.</p> <pre><code>@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nIT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!\n</code></pre> <p>To access the cluster again, first remove the old host keys with the following command (if accessing the cluster via command line):</p> <pre><code>ssh-keygen -R grace.hpc.yale.edu\n</code></pre> <p>If you are using a GUI, such as MobaXterm, you will need to manually edit your known host file and remove the lines related to Grace. For MobaXterm, this file is located (by default) in <code>Documents\\MobaXterm\\home\\.ssh</code>.</p> <p>Then attempt a new login and accept the new host key. The valid host keys for the login nodes are as follows:</p> <pre><code>3072 SHA256:8jJ/dKJVntzBJQWW8pU901PHbWcIe2r8ACvq30zQxKU login1 (RSA)\n256 SHA256:vhmGumY/XI/PAaheWQCadspl22/mqMiUiNXk+ov/zRc login1 (ECDSA)\n256 SHA256:NWNrMNoLwcqMm+E2NpsKKmirSbku9iXgbfk8ucn5aZE login1 (ED25519)\n</code></pre>"},{"location":"clusters/grace_rhel8/#new-software-tree","title":"New Software Tree","text":"<p>Grace now shares a software module tree with the McCleary cluster, providing a more consistent experience for all our users. Existing applications will continue to be available during this transition period. We plan to deprecate and remove the old application tree during the December 2023 maintenance window.</p> <p>If you experience any issues with software, please let us know at hpc@yale.edu and we can look into reinstalling.</p>"},{"location":"clusters/grace_rhel8/#common-errors","title":"Common Errors","text":""},{"location":"clusters/grace_rhel8/#python-not-found","title":"Python not found","text":"<p>Under RHEL8, we have only installed Python 3, which must be executed using <code>python3</code> (not <code>python</code>).  As always, if you need additional packages, we strongly recommend setting up your own conda environment.</p> <p>In addition, Python 2.7 is no longer support and therefore not installed by default.  To use Python 2.7, we request you setup a conda environment.</p>"},{"location":"clusters/grace_rhel8/#missing-system-libraries","title":"Missing System Libraries","text":"<p>Some of the existing applications may depend on libraries that are no longer installed in the operating system. If you run into these errors please email hpc@yale.edu and include which application/version you are using along with the full error message. We will investigate these on a case-by-case basis and work to get the issue resolved.</p> <p>There will be a small number of compute nodes reserved with RHEL7 (in a partition named <code>legacy</code>) to enable work to continue while we resolve these issues. This partition will remain available until the December maintenance window.</p> <p>Warning</p> <p>Some of the applications in the new shared apps tree may not work perfectly on the legacy RHEL7 nodes.  When running jobs in the legacy partition, you should therefore run <code>module purge</code> at the begining of interactive sessions and add it to the start of your batch scripts.  This will ensure that you only load modules built for RHEL7.</p>"},{"location":"clusters/grace_rhel8/#report-issues","title":"Report Issues","text":"<p>If you continue to have or discover new issues with your workflow, feel free to contact us for assistance. Please include the working directory, the commands that were run, the software modules used, and any more information needed to reproduce the issue.</p>"},{"location":"clusters/maintenance/","title":"Cluster Maintenance","text":"<p>Each YCRC cluster undergoes regular scheduled maintenance twice a year. During the maintenance, the cluster is unavailable, logins are deactivated and all pending jobs are held.  Unless otherwise stated, the storage for that cluster will also be inaccessible during the maintenance. We use this opportunity when jobs are not running and there are no users on the machine to make upgrades and changes that would be disruptive.  These activities include updating and patching the compute resources including the compute nodes, networking, service nodes and storage as well as making changes to critical infrastructure.</p> <p>Each maintenance is scheduled for three days, from Tuesday morning through end of day Thursday of the respective week.  In many cases, the cluster may return to service early and, under extenuating circumstances, we may choose to extend maintenance if necessary to make sure the system is stable before restoring access and jobs.</p> <p>Communication will be sent to all users of the respective cluster both 4 weeks and 1 week prior to the maintenance period.</p>"},{"location":"clusters/maintenance/#schedule","title":"Schedule","text":"<p>The schedule for the regular cluster maintenance is posted below.  Please be mindful of these dates and schedule your work accordingly to avoid disruptions.</p> Date Cluster Jun 4-6 2024 Grace Aug 20-22 2024 Milgram Oct 15-17 2024 McCleary Dec 3-5 2024 Grace <p>Occasionally we will schedule additional maintenance periods beyond those listed above, and potentially with shorter notices, if urgent work arises, such as power work on the data center or critical upgrades for stability or security.  We will give as much notice as possible in advance of these maintenance outages.</p>"},{"location":"clusters/mccleary-farnam-ruddle/","title":"McCleary for Farnam and Ruddle Users","text":"<p>McCleary is the successor to both the Farnam and Ruddle clusters, which were retired in summer 2023.</p>"},{"location":"clusters/mccleary-farnam-ruddle/#key-dates","title":"Key Dates","text":""},{"location":"clusters/mccleary-farnam-ruddle/#farnam","title":"Farnam","text":"<ul> <li>April: Migration of purchased nodes and storage from Farnam to McCleary</li> <li>June 1st: Access to Farnam login and OnDemand nodes disabled<ul> <li>Compute service charges on McCleary commons partitions begin</li> </ul> </li> <li>July 13: <code>/gpfs/ysm</code> no longer be available</li> </ul>"},{"location":"clusters/mccleary-farnam-ruddle/#ruddle","title":"Ruddle","text":"<ul> <li>April: Migration of purchased nodes from Ruddle to McCleary</li> <li>June 1st: Official Farnam retirement date, and beginning of compute service charges on McCleary commons partitions. Jobs in the ycga partitions will always be exempt from compute service charge.</li> <li>July 24th: Access to Ruddle login and OnDemand nodes disabled. Old <code>/gpfs/ycga</code> replaced with new system.</li> </ul>"},{"location":"clusters/mccleary-farnam-ruddle/#accounts","title":"Accounts","text":"<p>Most Farnam and Ruddle users who have been active in the last year have accounts automatically created on McCleary for them and have received an email to that effect. All other users who conduct life sciences research can request an account using our Account Request form.</p> <p>Group Membership</p> <p>Check which group your new McCleary account is associated with and make sure that matches your expection. This is the group that will be charged (if/when applicable) for your compute usage as well as dictate which private partitions you may have access to. Any cluster specific changes previously made on Farnam or Ruddle will not be automatically reflected on McCleary. To check, run the following command (replacing <code>&lt;netid&gt;</code> with your netid):</p> <pre><code>sacctmgr show user &lt;netid&gt;\n</code></pre> <p>If you need your group association changed, please let us know at hpc@yale.edu.</p>"},{"location":"clusters/mccleary-farnam-ruddle/#access","title":"Access","text":""},{"location":"clusters/mccleary-farnam-ruddle/#hostname","title":"Hostname","text":"<p>McCleary can be accessed via SSH (or MobaXterm) at the hostname <code>mccleary.ycrc.yale.edu</code>. </p> <p>Transfers and transfer applications should be connected via <code>transfer-mccleary.ycrc.yale.edu</code>. </p> <p>Note</p> <p>The hostname does not use the domain hpc.yale.edu, but uses ycrc.yale.edu instead.</p> <p>Multifactor authentication via Duo is required for all users on McCleary, similar to how Ruddle is currently configured. This will be new to Farnam users. For most usage this additional step is minimally invasive and makes our clusters much more secure.  However, for users who use graphical transfer tools such as Cyberduck, please see our MFA transfer documentation.</p>"},{"location":"clusters/mccleary-farnam-ruddle/#web-portal-open-ondemand","title":"Web Portal (Open OnDemand)","text":"<p>McCleary web portal url is available at ood-mccleary.ycrc.yale.edu.</p> <p>On McCleary, you are limited to 4 interactive app instances (of any type) through the web portal at one time.  Additional instances will remain pending in the queue until you terminate older open instances.  Closing the window does not terminate the interactive app job. To terminate the job, click the \"Delete\" button in your \"My Interactive Apps\" page in the web portal.</p> <p>Note</p> <p>Again, the url does not use the domain hpc.yale.edu, but uses ycrc.yale.edu instead.</p>"},{"location":"clusters/mccleary-farnam-ruddle/#software","title":"Software","text":"<p>We have installed most commonly used software modules from Farnam and Ruddle onto McCleary.  Usage of modules on McCleary is similar to the other clusters (e.g. <code>module avail</code>, <code>module load</code>).  Some software may only be initially available in a newer version than was installed on Farnam or Ruddle. </p> <p>If you cannot find a software package on McCleary that you need, please let us know at hpc@yale.edu and we can look into installing it for you.</p>"},{"location":"clusters/mccleary-farnam-ruddle/#partition-and-job-scheduler","title":"Partition and Job Scheduler","text":"<p>The most significant changes on transitioning from Farnam or Ruddle to McCleary is in respect to the partition scheme.  McCleary uses the partition scheme used on the Grace and Milgram clusters, so should be familiar to users of those clusters. A full list of McCleary partitions can be found on the cluster page.</p>"},{"location":"clusters/mccleary-farnam-ruddle/#default-time-request","title":"Default Time Request","text":"<p>The default walltime on McCleary is 1 hour on all partitions, down from 24 hours on Farnam and Ruddle.  Use the <code>-t</code> flag to request a longer time limit.</p>"},{"location":"clusters/mccleary-farnam-ruddle/#changes-to-partitions","title":"Changes to Partitions","text":"<p>Below are notable changes to the partitions relative to Farnam and Ruddle. Many of these changes are reductions to maximum time request.  If you job cannot run in the available partition time limits, please contact us at hpc@yale.edu so we can discuss your situation.</p>"},{"location":"clusters/mccleary-farnam-ruddle/#general","title":"<code>general</code>","text":"<p>McCleary does not have a <code>general</code> partition, but instead has <code>day</code> and <code>week</code> partitions with maximum time limits of 24 hours and 7 days, respectively.  The <code>week</code> partition contains significantly fewer nodes than <code>day</code> and will reject any job that request less than 24 hours of walltime, so please think carefully about how long your job needs to run for when selecting a partition. We strongly encourage checkpointing if it is an option or dividing up your workload into less than 24 hour chunks. This scheme promotes high turnover of compute resources and reduces the number of idle jobs, resulting in lower overall wait time.</p> <p>Interactive jobs are blocked from running in the <code>day</code> or <code>week</code> partitions. See the <code>interactive</code> partition below instead.</p> <p><code>day</code> is the default partition for batch jobs (where your job goes if you do not specify a partition with <code>-p</code> or <code>--partition</code>).</p>"},{"location":"clusters/mccleary-farnam-ruddle/#interactive","title":"<code>interactive</code>","text":"<p>The <code>interactive</code> partition is called <code>devel</code> and contains a set of dedicated nodes specifically for development or interactive uses (<code>salloc</code> jobs). To ensure high availability of resources, users are limited to one job at time. That job cannot request more than 6 hours, 4 cpus and 32G of memory.</p> <p><code>devel</code> is the default partition for jobs started using <code>salloc</code> (where your job goes if you do not specify a partition with <code>-p</code> or <code>--partition</code>).</p>"},{"location":"clusters/mccleary-farnam-ruddle/#bigmem","title":"<code>bigmem</code>","text":"<p>McCleary has a <code>bigmem</code> partition, but the maximum time request is now 24 hours.  Jobs requesting less than 120G of RAM will be rejected from the partition and we ask you to submit those jobs to <code>day</code>.</p>"},{"location":"clusters/mccleary-farnam-ruddle/#scavenge","title":"<code>scavenge</code>","text":"<p>McCleary has a <code>scavenge</code> partition that operates in the same preemptable mode as before, but the maximum time request is now 24 hours. </p>"},{"location":"clusters/mccleary-farnam-ruddle/#gpu_devel","title":"<code>gpu_devel</code>","text":"<p>There is no <code>gpu_devel</code> on McCleary. We are evaluating the needs and potential solutions for interactive GPU-enabled jobs. For now, interactive GPU-enabled jobs should be submitted to the <code>gpu</code> partition.</p>"},{"location":"clusters/mccleary-farnam-ruddle/#ycga-compute","title":"YCGA Compute","text":"<p>YCGA researchers have access to a dedicated set of nodes totally over 3000 cores on McCleary that are prefixed with <code>ycga</code>.</p> <ul> <li><code>ycga</code>: general purpose partition for batch jobs</li> <li><code>ycga_interactive</code>: partition for interactive jobs (limit of 1 job at a time in this partition)</li> <li><code>ycga_bigmem</code>: for jobs requiring large amount of RAM (&gt;120G)</li> </ul>"},{"location":"clusters/mccleary-farnam-ruddle/#dedicated-nodes","title":"Dedicated Nodes","text":"<p>If you have purchased nodes on Farnam or Ruddle that are not in the <code>haswell</code> generation, we have coordinated with your group to migrate those nodes to McCleary in April into a partition of the same name.</p>"},{"location":"clusters/mccleary-farnam-ruddle/#storage-and-data","title":"Storage and Data","text":"<p>If you have data on the Gibbs filesystem, there was no action required as they are already available on McCleary.</p>"},{"location":"clusters/mccleary-farnam-ruddle/#farnam-data","title":"Farnam Data","text":"<p>Farnam\u2019s primary filesystem, YSM (/gpfs/ysm), was retired on July 13th. If you previously had a Farnam account, you have been give new, empty home and scratch directories for McCleary on our Palmer filesystem and a 1 TiB project space on our Gibbs filesystem. Project quotas can be increased to 4 TiB at no cost by sending a request to hpc@yale.edu. </p>"},{"location":"clusters/mccleary-farnam-ruddle/#ruddle-data","title":"Ruddle Data","text":"<p>The YCGA storage system (<code>/gpfs/ycga</code>) has been replaced with a new, larger storage system at the same namespace. All data in the <code>project</code> (now at <code>work</code>), <code>sequencers</code>, <code>special</code>, and <code>pi</code> directories under <code>/gpfs/ycga</code> were migrated by YCRC staff to the new storage system. All other data on <code>/gpfs/ycga</code> (Ruddle home and scratch60) was retired with Ruddle on July 24th.</p> <p>As a McCleary user, you have also been given new, empty home and scratch directories for McCleary on our Palmer filesystem and a 1 TiB project space on our Gibbs filesystem. Project quotas can be increased to 4 TiB at no cost by sending a request to hpc@yale.edu.</p> <p>Ruddle Project Data</p> <p>Data previously in <code>/gpfs/ycga/project/&lt;groupname&gt;/&lt;netid&gt;</code> can now be found at <code>/gpfs/ycga/work/&lt;groupname&gt;/&lt;netid&gt;</code>. The <code>project</code> symlink in your home directory links to your Gibbs project space, not your YCGA storage. </p>"},{"location":"clusters/mccleary-farnam-ruddle/#researchers-with-purchased-storage","title":"Researchers with Purchased Storage","text":"<p>If you have purchased space on <code>/gpfs/ycga</code>or <code>/gpfs/ysm</code> that has not expired, we have migrated your allocation. This is the only data that the YCRC automatically migrated from Farnam to McCleary. </p> <p>If you have purchased storage on <code>/gpfs/ysm</code> that has expired as of December 31st 2022, you should have received a separate communication from us with information on purchasing replacement storage on Gibbs (which is available on McCleary).</p> <p>If you have any questions or concerns about what has been moved to McCleary and when, please reach out to us.</p>"},{"location":"clusters/mccleary-farnam-ruddle/#storageyale-say-shares","title":"Storage@Yale (SAY) Shares","text":"<p>Storage@Yale shares are available on McCleary, but only on the <code>transfer</code> node.  To access your SAY data, make sure to login to the <code>transfer</code> node and then copy your data to either <code>project</code> or <code>scratch</code>.  Note, this is different than how Ruddle was set up, where SAY shares were available on all nodes. </p>"},{"location":"clusters/mccleary/","title":"McCleary","text":"<p>McCleary is a shared-use resource for the Yale School of Medicine (YSM), life science researchers elsewhere on campus and projects related to the Yale Center for Genome Analysis. It consists of a variety of compute nodes networked over ethernet and mounts several shared filesystems.</p> <p>McCleary is named for Beatrix McCleary Hamburg, who received her medical degree in 1948 and was the first female African American graduate of Yale School of Medicine. The McCleary HPC cluster is Yale's first direct-to-chip liquid cooled cluster, moving the YCRC and the Yale research computing community into a more environmentally friendly future.</p> <p>Info</p> <p>Farnam or Ruddle user? Farnam and Ruddle were both retired in summer 2023. See our explainer for what you need to know about using McCleary and how it differs from Farnam and Ruddle.</p>"},{"location":"clusters/mccleary/#access-the-cluster","title":"Access the Cluster","text":"<p>Once you have an account, the cluster can be accessed via ssh or through the Open OnDemand web portal.</p>"},{"location":"clusters/mccleary/#system-status-and-monitoring","title":"System Status and Monitoring","text":"<p>For system status messages and the schedule for upcoming maintenance, please see the system status page. For a current node-level view of job activity, see the cluster monitor page (VPN only).</p>"},{"location":"clusters/mccleary/#partitions-and-hardware","title":"Partitions and Hardware","text":"<p>McCleary is made up of several kinds of compute nodes. We group them into (sometimes overlapping) Slurm partitions meant to serve different purposes. By combining the <code>--partition</code> and <code>--constraint</code> Slurm options you can more finely control what nodes your jobs can run on.</p> <p>Info</p> <p>YCGA sequence data user?  To avoid being charged for your cpu usage for YCGA-related work, make sure to submit jobs to the ycga partition with -p ycga.</p> <p>Job Submission Limits</p> <ul> <li> <p>You are limited to 4 interactive app instances (of any type) at one time.  Additional instances will be rejected until you delete older open instances.  For OnDemand jobs, closing the window does not terminate the interactive app job. To terminate the job, click the \"Delete\" button in your \"My Interactive Apps\" page in the web portal.</p> </li> <li> <p>Job submissions are limited to 200 jobs per hour. See the Rate Limits section in the Common Job Failures page for more info.</p> </li> </ul>"},{"location":"clusters/mccleary/#public-partitions","title":"Public Partitions","text":"<p>See each tab below for more information about the available common use partitions.</p> day <p>Use the day partition for most batch jobs. This is the default if you don't specify one with <code>--partition</code>.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the day partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>1-00:00:00</code> Maximum CPUs per group <code>512</code> Maximum memory per group <code>6000G</code> Maximum CPUs per user <code>256</code> Maximum memory per user <code>3000G</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 26 8358 64 983 icelake, avx512, 8358, nogpu, bigtmp, common 5 6240 36 180 cascadelake, avx512, 6240, nogpu, bigtmp, standard, oldest, common devel <p>Use the devel partition to jobs with which you need ongoing interaction. For example, exploratory analyses or debugging compilation.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the devel partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>06:00:00</code> Maximum CPUs per user <code>4</code> Maximum memory per user <code>32G</code> Maximum submitted jobs per user <code>1</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 7 6240 36 180 cascadelake, avx512, 6240, nogpu, bigtmp, standard, oldest, common week <p>Use the week partition for jobs that need a longer runtime than day allows.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the week partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> Maximum CPUs per group <code>192</code> Maximum memory per group <code>2949G</code> Maximum CPUs per user <code>192</code> Maximum memory per user <code>2949G</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 14 8358 64 983 icelake, avx512, 8358, nogpu, bigtmp, common 2 6240 36 180 cascadelake, avx512, 6240, nogpu, bigtmp, standard, oldest, common long <p>Use the long partition for jobs that need a longer runtime than week allows.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=7-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the long partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> Maximum CPUs per group <code>36</code> Maximum CPUs per user <code>36</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 3 6240 36 180 cascadelake, avx512, 6240, nogpu, bigtmp, standard, oldest, common transfer <p>Use the transfer partition to stage data for your jobs to and from cluster storage.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the transfer partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>1-00:00:00</code> Maximum CPUs per user <code>4</code> Maximum running jobs per user <code>4</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 2 72 8 227 milan, 72F3, nogpu, standard, common gpu <p>Use the gpu partition for jobs that make use of GPUs. You must request GPUs explicitly with the <code>--gpus</code> option in order to use them. For example, <code>--gpus=gtx1080ti:2</code> would request 2 GeForce GTX 1080Ti GPUs per node.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the gpu partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>2-00:00:00</code> Maximum GPUs per group <code>24</code> Maximum GPUs per user <code>12</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 9 6326 32 206 a5000 4 24 icelake, avx512, 6326, doubleprecision, a5000, common 1 8358 64 983 a100 4 80 icelake, avx512, 8358, doubleprecision, bigtmp, common, a100, a100-80g 1 8358 64 983 a100 4 80 icelake, avx512, 8358, gpu, bigtmp, common, doubleprecision, a100, a100-80g 1 8358 64 984 a100 4 80 icelake, avx512, 8358, doubleprecision, bigtmp, common, a100, a100-80g 3 5222 8 163 rtx3090 4 24 cascadelake, avx512, 5222, doubleprecision, common, rtx3090 4 5222 8 163 rtx5000 4 16 cascadelake, avx512, 5222, doubleprecision, common, bigtmp, rtx5000 gpu_devel <p>Use the gpu_devel partition to debug jobs that make use of GPUs, or to develop GPU-enabled code.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the gpu_devel partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>06:00:00</code> Maximum CPUs per user <code>10</code> Maximum GPUs per user <code>2</code> Maximum submitted jobs per user <code>2</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 3 6326 32 206 a5000 4 24 icelake, avx512, 6326, doubleprecision, a5000, common 2 5222 8 163 rtx5000 4 16 cascadelake, avx512, 5222, doubleprecision, common, bigtmp, rtx5000 1 5222 8 163 rtx3090 4 24 cascadelake, avx512, 5222, doubleprecision, common, rtx3090 1 6240 36 352 a100 4 40 cascadelake, avx512, 6240, doubleprecision, common, bigtmp, oldest, a100, a100-40g bigmem <p>Use the bigmem partition for jobs that have memory requirements other partitions can't handle.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the bigmem partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>1-00:00:00</code> Maximum CPUs per user <code>32</code> Maximum memory per user <code>3960G</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 4 6346 32 3960 icelake, avx512, 6346, nogpu, bigtmp, common 3 6240 36 1486 cascadelake, avx512, 6240, nogpu, pi, bigtmp, oldest 2 6234 16 1486 cascadelake, avx512, 6234, nogpu, common, bigtmp scavenge <p>Use the scavenge partition to run preemptable jobs on more resources than normally allowed. For more information about scavenge, see the Scavenge documentation.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the scavenge partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>1-00:00:00</code> Maximum CPUs per user <code>1000</code> Maximum memory per user <code>20000G</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 48 8362 64 479 icelake, avx512, 8362, nogpu, standard, pi 1 8358 64 1007 a5000 8 24 icelake, avx512, 8358, doubleprecision, bigtmp, pi, a5000 17 6326 32 206 a5000 4 24 icelake, avx512, 6326, doubleprecision, a5000, common 2 6326 32 984 a100 4 80 icelake, avx512, 6326, doubleprecision, pi, a100, a100-80g 40 8358 64 983 icelake, avx512, 8358, nogpu, bigtmp, common 1 8358 64 983 a100 4 80 icelake, avx512, 8358, doubleprecision, bigtmp, common, a100, a100-80g 1 8358 64 983 a100 4 80 icelake, avx512, 8358, doubleprecision, bigtmp, pi, a100, a100-80g 1 8358 64 983 a100 4 80 icelake, avx512, 8358, gpu, bigtmp, common, doubleprecision, a100, a100-80g 1 8358 64 984 a100 4 80 icelake, avx512, 8358, doubleprecision, bigtmp, common, a100, a100-80g 4 6346 32 1991 icelake, avx512, 6346, nogpu, pi 4 6326 32 479 a5000 4 24 icelake, avx512, 6326, doubleprecision, a5000, pi 1 8358 64 1007 l40s 8 48 icelake, avx512, 8358, doubleprecision, pi, bigtmp, l40s, 4 6346 32 3960 icelake, avx512, 6346, nogpu, bigtmp, common 41 6240 36 180 cascadelake, avx512, 6240, nogpu, bigtmp, standard, pi 4 6240 36 730 cascadelake, avx512, 6240, nogpu, bigtmp, standard, pi 4 6240 36 352 cascadelake, avx512, 6240, nogpu, bigtmp, standard, pi 12 6240 36 180 cascadelake, avx512, 6240, nogpu, bigtmp, standard, oldest, common 9 6240 36 163 cascadelake, avx512, 6240, nogpu, bigtmp, standard, pi 2 6240 36 166 cascadelake, avx512, 6240, nogpu, bigtmp, standard, pi 3 5222 8 163 rtx3090 4 24 cascadelake, avx512, 5222, doubleprecision, common, rtx3090 6 6240 36 1486 cascadelake, avx512, 6240, nogpu, pi, bigtmp, oldest 10 8268 48 352 cascadelake, avx512, 8268, nogpu, bigtmp, pi 1 6248r 48 352 cascadelake, avx512, 6248r, nogpu, pi, bigtmp 2 6234 16 1486 cascadelake, avx512, 6234, nogpu, common, bigtmp 1 6240 36 352 v100 4 16 cascadelake, avx512, 6240, pi, oldest, v100 4 5222 8 163 rtx5000 4 16 cascadelake, avx512, 5222, doubleprecision, common, bigtmp, rtx5000 8 5222 8 163 rtx5000 4 16 cascadelake, avx512, 5222, doubleprecision, pi, bigtmp, rtx5000 2 6240 36 352 a100 4 40 cascadelake, avx512, 6240, doubleprecision, pi, bigtmp, oldest, a100, a100-40g 1 6226r 32 163 rtx3090 4 24 cascadelake, avx512, 6226r, doubleprecision, pi, rtx3090 2 6240 36 163 rtx2080ti 4 11 cascadelake, avx512, 6240, singleprecision, pi, bigtmp, oldest, rtx2080ti 1 6242 32 981 rtx8000 2 48 cascadelake, avx512, 6242, doubleprecision, pi, bigtmp, oldest, rtx8000 1 6240 36 352 rtx3090 8 24 cascadelake, avx512, 6240, doubleprecision, pi, bigtmp, oldest, rtx3090 1 6240 36 730 a100 4 40 cascadelake, avx512, 6240, doubleprecision, pi, bigtmp, oldest, a100, a100-40g 1 6240 36 163 rtx3090 4 24 cascadelake, avx512, 6240, doubleprecision, pi, bigtmp, oldest, rtx3090 1 6240 36 163 rtx3090 8 24 cascadelake, avx512, 6240, doubleprecision, pi, bigtmp, oldest, rtx3090 1 6132 28 730 skylake, avx512, 6132, nogpu, standard, bigtmp, pi 2 6132 28 163 skylake, avx512, 6132, nogpu, standard, bigtmp, pi 2 5122 8 163 rtx2080 4 8 skylake, avx512, 5122, singleprecision, pi, rtx2080 scavenge_gpu <p>Use the scavenge_gpu partition to run preemptable jobs on more GPU resources than normally allowed. For more information about scavenge, see the Scavenge documentation.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the scavenge_gpu partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>1-00:00:00</code> Maximum GPUs per group <code>100</code> Maximum GPUs per user <code>64</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 1 8358 64 1007 a5000 8 24 icelake, avx512, 8358, doubleprecision, bigtmp, pi, a5000 17 6326 32 206 a5000 4 24 icelake, avx512, 6326, doubleprecision, a5000, common 2 6326 32 984 a100 4 80 icelake, avx512, 6326, doubleprecision, pi, a100, a100-80g 1 8358 64 983 a100 4 80 icelake, avx512, 8358, doubleprecision, bigtmp, common, a100, a100-80g 1 8358 64 983 a100 4 80 icelake, avx512, 8358, doubleprecision, bigtmp, pi, a100, a100-80g 1 8358 64 983 a100 4 80 icelake, avx512, 8358, gpu, bigtmp, common, doubleprecision, a100, a100-80g 1 8358 64 984 a100 4 80 icelake, avx512, 8358, doubleprecision, bigtmp, common, a100, a100-80g 4 6326 32 479 a5000 4 24 icelake, avx512, 6326, doubleprecision, a5000, pi 1 8358 64 1007 l40s 8 48 icelake, avx512, 8358, doubleprecision, pi, bigtmp, l40s, 3 5222 8 163 rtx3090 4 24 cascadelake, avx512, 5222, doubleprecision, common, rtx3090 1 6240 36 352 v100 4 16 cascadelake, avx512, 6240, pi, oldest, v100 4 5222 8 163 rtx5000 4 16 cascadelake, avx512, 5222, doubleprecision, common, bigtmp, rtx5000 8 5222 8 163 rtx5000 4 16 cascadelake, avx512, 5222, doubleprecision, pi, bigtmp, rtx5000 2 6240 36 352 a100 4 40 cascadelake, avx512, 6240, doubleprecision, pi, bigtmp, oldest, a100, a100-40g 1 6226r 32 163 rtx3090 4 24 cascadelake, avx512, 6226r, doubleprecision, pi, rtx3090 2 6240 36 163 rtx2080ti 4 11 cascadelake, avx512, 6240, singleprecision, pi, bigtmp, oldest, rtx2080ti 1 6242 32 981 rtx8000 2 48 cascadelake, avx512, 6242, doubleprecision, pi, bigtmp, oldest, rtx8000 1 6240 36 352 rtx3090 8 24 cascadelake, avx512, 6240, doubleprecision, pi, bigtmp, oldest, rtx3090 1 6240 36 730 a100 4 40 cascadelake, avx512, 6240, doubleprecision, pi, bigtmp, oldest, a100, a100-40g 1 6240 36 163 rtx3090 4 24 cascadelake, avx512, 6240, doubleprecision, pi, bigtmp, oldest, rtx3090 1 6240 36 163 rtx3090 8 24 cascadelake, avx512, 6240, doubleprecision, pi, bigtmp, oldest, rtx3090 2 5122 8 163 rtx2080 4 8 skylake, avx512, 5122, singleprecision, pi, rtx2080"},{"location":"clusters/mccleary/#private-partitions","title":"Private Partitions","text":"<p>With few exceptions, jobs submitted to private partitions are not considered when calculating your group's Fairshare. Your group can purchase additional hardware for private use, which we will make available as a <code>pi_groupname</code> partition. These nodes are purchased by you, but supported and administered by us. After vendor support expires, we retire compute nodes. Compute nodes can range from $10K to upwards of $50K depending on your requirements. If you are interested in purchasing nodes for your group, please contact us.</p> PI Partitions (click to expand) pi_bunick <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the pi_bunick partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 1 6240 36 352 a100 4 40 cascadelake, avx512, 6240, doubleprecision, pi, bigtmp, oldest, a100, a100-40g pi_butterwick <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the pi_butterwick partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 1 6240 36 352 a100 4 40 cascadelake, avx512, 6240, doubleprecision, pi, bigtmp, oldest, a100, a100-40g pi_chenlab <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_chenlab partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>14-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 1 8268 48 352 cascadelake, avx512, 8268, nogpu, bigtmp, pi pi_cryo_realtime <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the pi_cryo_realtime partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>14-00:00:00</code> Maximum GPUs per user <code>12</code> Maximum running jobs per user <code>2</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 1 6326 32 206 a5000 4 24 icelake, avx512, 6326, doubleprecision, a5000, common pi_cryoem <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the pi_cryoem partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>4-00:00:00</code> Maximum CPUs per user <code>32</code> Maximum GPUs per user <code>12</code> Maximum running jobs per user <code>2</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 6 6326 32 206 a5000 4 24 icelake, avx512, 6326, doubleprecision, a5000, common pi_dewan <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_dewan partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 2 6240 36 163 cascadelake, avx512, 6240, nogpu, bigtmp, standard, pi pi_dijk <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the pi_dijk partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 1 6240 36 352 rtx3090 8 24 cascadelake, avx512, 6240, doubleprecision, pi, bigtmp, oldest, rtx3090 pi_dunn <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_dunn partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 1 6240 36 163 cascadelake, avx512, 6240, nogpu, bigtmp, standard, pi pi_edwards <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_edwards partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 1 6240 36 163 cascadelake, avx512, 6240, nogpu, bigtmp, standard, pi pi_falcone <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the pi_falcone partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 1 6240 36 163 cascadelake, avx512, 6240, nogpu, bigtmp, standard, pi 1 6240 36 1486 cascadelake, avx512, 6240, nogpu, pi, bigtmp, oldest 1 6240 36 352 v100 4 16 cascadelake, avx512, 6240, pi, oldest, v100 pi_galvani <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_galvani partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 7 8268 48 352 cascadelake, avx512, 8268, nogpu, bigtmp, pi pi_gerstein <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_gerstein partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 1 6132 28 730 skylake, avx512, 6132, nogpu, standard, bigtmp, pi 2 6132 28 163 skylake, avx512, 6132, nogpu, standard, bigtmp, pi pi_gerstein_gpu <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the pi_gerstein_gpu partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 1 8358 64 983 a100 4 80 icelake, avx512, 8358, doubleprecision, bigtmp, pi, a100, a100-80g 1 6240 36 163 rtx3090 4 24 cascadelake, avx512, 6240, doubleprecision, pi, bigtmp, oldest, rtx3090 1 6240 36 163 rtx3090 8 24 cascadelake, avx512, 6240, doubleprecision, pi, bigtmp, oldest, rtx3090 pi_hall <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the pi_hall partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 2 6326 32 984 a100 4 80 icelake, avx512, 6326, doubleprecision, pi, a100, a100-80g 39 6240 36 180 cascadelake, avx512, 6240, nogpu, bigtmp, standard, pi pi_hall_bigmem <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_hall_bigmem partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>28-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 2 6240 36 1486 cascadelake, avx512, 6240, nogpu, pi, bigtmp, oldest pi_jetz <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_jetz partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 2 8358 64 1991 icelake, avx512, 8358, nogpu, bigtmp, pi 4 6240 36 730 cascadelake, avx512, 6240, nogpu, bigtmp, standard, pi 4 6240 36 352 cascadelake, avx512, 6240, nogpu, bigtmp, standard, pi pi_kleinstein <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_kleinstein partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 2 6240 36 163 cascadelake, avx512, 6240, nogpu, bigtmp, standard, pi pi_krishnaswamy <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the pi_krishnaswamy partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 1 6240 36 730 a100 4 40 cascadelake, avx512, 6240, doubleprecision, pi, bigtmp, oldest, a100, a100-40g pi_ma <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_ma partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 1 8268 48 352 cascadelake, avx512, 8268, nogpu, bigtmp, pi pi_medzhitov <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_medzhitov partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 2 6240 36 166 cascadelake, avx512, 6240, nogpu, bigtmp, standard, pi pi_miranker <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_miranker partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 1 6248r 48 352 cascadelake, avx512, 6248r, nogpu, pi, bigtmp pi_ohern <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_ohern partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 4 8358 64 984 icelake, avx512, 8358, nogpu, bigtmp, pi pi_reinisch <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the pi_reinisch partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 2 5122 8 163 rtx2080 4 8 skylake, avx512, 5122, singleprecision, pi, rtx2080 pi_sestan <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_sestan partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 2 8358 64 1991 icelake, avx512, 8358, nogpu, bigtmp, pi pi_sigworth <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the pi_sigworth partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 1 6240 36 163 rtx2080ti 4 11 cascadelake, avx512, 6240, singleprecision, pi, bigtmp, oldest, rtx2080ti pi_sindelar <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the pi_sindelar partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 1 6240 36 163 rtx2080ti 4 11 cascadelake, avx512, 6240, singleprecision, pi, bigtmp, oldest, rtx2080ti pi_tomography <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the pi_tomography partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>4-00:00:00</code> Maximum CPUs per user <code>32</code> Maximum GPUs per user <code>24</code> Maximum running jobs per user <code>2</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 8 5222 8 163 rtx5000 4 16 cascadelake, avx512, 5222, doubleprecision, pi, bigtmp, rtx5000 1 6242 32 981 rtx8000 2 48 cascadelake, avx512, 6242, doubleprecision, pi, bigtmp, oldest, rtx8000 pi_townsend <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_townsend partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 2 6240 36 180 cascadelake, avx512, 6240, nogpu, bigtmp, standard, pi pi_tsang <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_tsang partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 4 8358 64 983 icelake, avx512, 8358, nogpu, bigtmp, pi pi_ya-chi_ho <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_ya-chi_ho partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 1 8268 48 352 cascadelake, avx512, 8268, nogpu, bigtmp, pi pi_yong_xiong <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the pi_yong_xiong partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 1 8358 64 1007 a5000 8 24 icelake, avx512, 8358, doubleprecision, bigtmp, pi, a5000 4 6326 32 479 a5000 4 24 icelake, avx512, 6326, doubleprecision, a5000, pi 1 8358 64 1007 l40s 8 48 icelake, avx512, 8358, doubleprecision, pi, bigtmp, l40s, 1 6226r 32 163 rtx3090 4 24 cascadelake, avx512, 6226r, doubleprecision, pi, rtx3090 pi_zhao <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the pi_zhao partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 2 6240 36 163 cascadelake, avx512, 6240, nogpu, bigtmp, standard, pi"},{"location":"clusters/mccleary/#ycga-partitions","title":"YCGA Partitions","text":"<p>The following partitions are intended for projects related to the Yale Center for Genome Analysis. Please do not use these partitions for other proejcts. Access is granted on a group basis. If you need access to these partitions, please contact us to get approved and added.</p> YCGA Partitions (click to expand) ycga <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the ycga partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>2-00:00:00</code> Maximum CPUs per group <code>1024</code> Maximum memory per group <code>3934G</code> Maximum CPUs per user <code>256</code> Maximum memory per user <code>1916G</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 40 8362 64 479 icelake, avx512, 8362, nogpu, standard, pi ycga_admins <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 2 8362 64 479 icelake, avx512, 8362, nogpu, standard, pi ycga_bigmem <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the ycga_bigmem partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>4-00:00:00</code> Maximum CPUs per user <code>64</code> Maximum memory per user <code>1991G</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 4 6346 32 1991 icelake, avx512, 6346, nogpu, pi ycga_long <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the ycga_long partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>14-00:00:00</code> Maximum CPUs per group <code>64</code> Maximum memory per group <code>479G</code> Maximum CPUs per user <code>32</code> Maximum memory per user <code>239G</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 6 8362 64 479 icelake, avx512, 8362, nogpu, standard, pi"},{"location":"clusters/mccleary/#public-datasets","title":"Public Datasets","text":"<p>We host datasets of general interest in a loosely organized directory tree in <code>/gpfs/gibbs/data</code>:</p> <pre><code>\u251c\u2500\u2500 alphafold-2.3\n\u251c\u2500\u2500 alphafold-2.2 (deprecated)\n\u251c\u2500\u2500 alphafold-2.0 (deprecated)\n\u251c\u2500\u2500 annovar\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 humandb\n\u251c\u2500\u2500 cryoem\n\u251c\u2500\u2500 db\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 annovar\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 blast\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 busco\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Pfam\n\u2514\u2500\u2500 genomes\n    \u251c\u2500\u2500 1000Genomes\n    \u251c\u2500\u2500 10xgenomics\n    \u251c\u2500\u2500 Aedes_aegypti\n    \u251c\u2500\u2500 Bos_taurus\n    \u251c\u2500\u2500 Chelonoidis_nigra\n    \u251c\u2500\u2500 Danio_rerio\n    \u251c\u2500\u2500 Drosophila_melanogaster\n    \u251c\u2500\u2500 Gallus_gallus\n    \u251c\u2500\u2500 hisat2\n    \u251c\u2500\u2500 Homo_sapiens\n    \u251c\u2500\u2500 Macaca_mulatta\n    \u251c\u2500\u2500 Mus_musculus\n    \u251c\u2500\u2500 Monodelphis_domestica\n    \u251c\u2500\u2500 PhiX\n    \u2514\u2500\u2500 Saccharomyces_cerevisiae\n    \u2514\u2500\u2500 tmp\n\u2514\u2500\u2500 hisat2\n    \u2514\u2500\u2500 mouse\n</code></pre> <p>If you would like us to host a dataset or questions about what is currently available, please contact us.</p>"},{"location":"clusters/mccleary/#ycga-data","title":"YCGA Data","text":"<p>Data associated with YCGA projects and sequenceers are located on the YCGA storage system, accessible at <code>/gpfs/ycga</code>.</p> <p>For more information on accessing this data as well as sequencing data retention polices, see the YCGA Data documentation.</p>"},{"location":"clusters/mccleary/#storage","title":"Storage","text":"<p>McCleary has access to a number of GPFS filesystems. <code>/vast/palmer</code> is McCleary's primary filesystem where Home and Scratch60 directories are located. Every group on McCleary also has access to a Project allocation on the Gibbs filesytem on <code>/gpfs/gibbs</code>. For more details on the different storage spaces, see our Cluster Storage documentation.</p> <p>You can check your current storage usage &amp; limits by running the <code>getquota</code> command. Your <code>~/project</code> and <code>~/palmer_scratch</code> directories are shortcuts. Get a list of the absolute paths to your directories with the <code>mydirectories</code> command. If you want to share data in your Project or Scratch directory, see the permissions page.</p> <p>For information on data recovery, see the Backups and Snapshots documentation.</p> <p>Warning</p> <p>Files stored in <code>palmer_scratch</code> are purged if they are older than 60 days. You will receive an email alert one week before they are deleted. Artificial extension of scratch file expiration is forbidden without explicit approval from the YCRC. Please purchase storage if you need additional longer term storage.</p> Partition Root Directory Storage File Count Backups Snapshots home <code>/vast/palmer/home.mccleary</code> 125GiB/user 500,000 Yes &gt;=2 days project <code>/gpfs/gibbs/project</code> 1TiB/group, increase to 4TiB on request 5,000,000 No &gt;=2 days scratch <code>/vast/palmer/scratch</code> 10TiB/group 15,000,000 No No"},{"location":"clusters/milgram/","title":"Milgram","text":"<p>Milgram is a HIPAA aligned cluster intended for use on projects that may involve sensitive data. This applies to both storage and computation. If you have any questions about this policy, please contact us.</p> <p>Milgram is named for Dr. Stanley Milgram, a psychologist who researched the behavioral motivations behind social awareness in individuals and obedience to authority figures. He conducted several famous experiments during his professorship at Yale University including the lost-letter experiment, the small-world experiment, and the Milgram experiment.</p>"},{"location":"clusters/milgram/#milgram-usage-policies","title":"Milgram Usage Policies","text":"<p>Users wishing to use Milgram must agree to the following:</p> <ul> <li>All Milgram users must have fulfilled and be current with Yale's HIPAA training requirement.</li> <li>Since Milgram's resources are limited, we ask that you only use Milgram for work on and storage of sensitive data, and that you do your other high performance computing on our other clusters.  </li> </ul> <p>Operating System Upgrade\"</p> <p>Milgram has been upgraded to RHEL 8 during the February maintenance window, February 6-8, 2024. For more information see our Milgram Operating System Upgrade documentation.</p>"},{"location":"clusters/milgram/#access-the-cluster","title":"Access the Cluster","text":"<p>Once you have an account, the cluster can be accessed via ssh or through the Open OnDemand web portal.</p> <p>Info</p> <p>Connections to Milgram can only be made from the Yale VPN (<code>access.yale.edu</code>)--even if you are already on campus (YaleSecure or ethernet). See our VPN page for setup instructions.</p>"},{"location":"clusters/milgram/#system-status-and-monitoring","title":"System Status and Monitoring","text":"<p>For system status messages and the schedule for upcoming maintenance, please see the system status page. For a current node-level view of job activity, see the cluster monitor page (VPN only).</p>"},{"location":"clusters/milgram/#partitions-and-hardware","title":"Partitions and Hardware","text":"<p>Milgram is made up of several kinds of compute nodes. We group them into  (sometimes overlapping) Slurm partitions meant to serve different purposes. By combining the <code>--partition</code> and <code>--constraint</code> Slurm options you can more finely control what nodes your jobs can run on.</p> <p>Job Submission Limits</p> <ul> <li> <p>You are limited to 4 interactive app instances (of any type) at one time.  Additional instances will be rejected until you delete older open instances.  For OnDemand jobs, closing the window does not terminate the interactive app job. To terminate the job, click the \"Delete\" button in your \"My Interactive Apps\" page in the web portal.</p> </li> <li> <p>Job submissions are limited to 200 jobs per hour. See the Rate Limits section in the Common Job Failures page for more info.</p> </li> </ul> <p>Interactive Partition Name Change</p> <p>The 'interactive' and 'psych_interactive partitions have been renamed to 'devel' and 'psych_devel', respectively.  Please adjust your job submissions accordingly.</p>"},{"location":"clusters/milgram/#public-partitions","title":"Public Partitions","text":"<p>See each tab below for more information about the available common use partitions.</p> day <p>Use the day partition for most batch jobs. This is the default if you don't specify one with <code>--partition</code>.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the day partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>1-00:00:00</code> Maximum CPUs per user <code>324</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 9 6240 36 181 cascadelake, avx512, 6240, nogpu, standard, common, bigtmp, oldest devel <p>Use the devel partition to jobs with which you need ongoing interaction. For example, exploratory analyses or debugging compilation.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the devel partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>06:00:00</code> Maximum CPUs per user <code>4</code> Maximum memory per user <code>32G</code> Maximum running jobs per user <code>1</code> Maximum submitted jobs per user <code>1</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 2 6240 36 181 cascadelake, avx512, 6240, nogpu, standard, common, bigtmp, oldest week <p>Use the week partition for jobs that need a longer runtime than day allows.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the week partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> Maximum CPUs per user <code>72</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 4 6240 36 181 cascadelake, avx512, 6240, nogpu, standard, common, bigtmp, oldest gpu <p>Use the gpu partition for jobs that make use of GPUs. You must request GPUs explicitly with the <code>--gpus</code> option in order to use them. For example, <code>--gpus=gtx1080ti:2</code> would request 2 GeForce GTX 1080Ti GPUs per node.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the gpu partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>2-00:00:00</code> Maximum GPUs per user <code>4</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 2 5222 8 181 rtx5000 4 16 cascadelake, avx512, 5222, doubleprecision, common, bigtmp, rtx5000 scavenge <p>Use the scavenge partition to run preemptable jobs on more resources than normally allowed. For more information about scavenge, see the Scavenge documentation.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the scavenge partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>1-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 20 6342 48 478 icelake, avx512, 6342, bigtmp, nogpu, standard, pi 1 6326 32 497 a40 4 48 icelake, avx512, pi, 6326, singleprecision, bigtmp, a40 17 6240 36 181 cascadelake, avx512, 6240, nogpu, standard, common, bigtmp, oldest 10 6240 36 372 rtx2080ti 4 11 cascadelake, avx512, 6240, singleprecision, pi, bigtmp, rtx2080ti, oldest 2 5222 8 181 rtx5000 4 16 cascadelake, avx512, 5222, doubleprecision, common, bigtmp, rtx5000"},{"location":"clusters/milgram/#private-partitions","title":"Private Partitions","text":"<p>With few exceptions, jobs submitted to private partitions are not considered when calculating your group's Fairshare. Your group can purchase additional hardware for private use, which we will make available as a <code>pi_groupname</code> partition. These nodes are purchased by you, but supported and administered by us. After vendor support expires, we retire compute nodes. Compute nodes can range from $10K to upwards of $50K depending on your requirements. If you are interested in purchasing nodes for your group, please contact us.</p> PI Partitions (click to expand) pi_shung <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 1 6326 32 497 a40 4 48 icelake, avx512, pi, 6326, singleprecision, bigtmp, a40 psych_day <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the psych_day partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>1-00:00:00</code> Maximum CPUs per group <code>500</code> Maximum memory per group <code>2500G</code> Maximum CPUs per user <code>350</code> Maximum memory per user <code>1750G</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 19 6342 48 478 icelake, avx512, 6342, bigtmp, nogpu, standard, pi psych_devel <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the psych_devel partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>06:00:00</code> Maximum CPUs per user <code>4</code> Maximum memory per user <code>32G</code> Maximum running jobs per user <code>1</code> Maximum submitted jobs per user <code>1</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 1 6342 48 478 icelake, avx512, 6342, bigtmp, nogpu, standard, pi psych_gpu <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the psych_gpu partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> Maximum GPUs per user <code>20</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 10 6240 36 372 rtx2080ti 4 11 cascadelake, avx512, 6240, singleprecision, pi, bigtmp, rtx2080ti, oldest psych_scavenge <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the psych_scavenge partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 20 6342 48 478 icelake, avx512, 6342, bigtmp, nogpu, standard, pi 10 6240 36 372 rtx2080ti 4 11 cascadelake, avx512, 6240, singleprecision, pi, bigtmp, rtx2080ti, oldest psych_week <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the psych_week partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> Maximum CPUs per group <code>350</code> Maximum memory per group <code>2000G</code> Maximum CPUs per user <code>250</code> Maximum memory per user <code>1500G</code> Maximum CPUs in use <code>448</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 20 6342 48 478 icelake, avx512, 6342, bigtmp, nogpu, standard, pi"},{"location":"clusters/milgram/#storage","title":"Storage","text":"<p><code>/gpfs/milgram</code> is Milgram's primary filesystem where home, project, and scratch60 directories are located. For more details on the different storage spaces, see our Cluster Storage documentation.</p> <p>You can check your current storage usage &amp; limits by running the <code>getquota</code> command. Note that the per-user usage breakdown only update once daily.</p> <p>For information on data recovery, see the Backups and Snapshots documentation.</p> <p>Warning</p> <p>Files stored in <code>scratch60</code> are purged if they are older than 60 days. You will receive an email alert one week before they are deleted. Artificial extension of scratch file expiration is forbidden without explicit approval from the YCRC. Please purchase storage if you need additional longer term storage.</p> Partition Root Directory Storage File Count Backups Snapshots home <code>/gpfs/milgram/home</code> 125GiB/user 500,000 Yes &gt;=2 days project <code>/gpfs/milgram/project</code> 1TiB/group, increase to 4TiB on request 5,000,000 Yes &gt;=2 days scratch60 <code>/gpfs/milgram/scratch60</code> 20TiB/group 15,000,000 No No"},{"location":"clusters/milgram_rhel8/","title":"Milgram Operating System Upgrade","text":"<p>Milgram's previous operating system, Red Hat (RHEL) 7, will be officially end-of-life in 2024 and will no longer be supported with security patches by the developer. Therefore Milgram has been upgraded to RHEL 8 during the February maintenance window, February 6-8, 2024. This provides a number of key benefits to Milgram:</p> <ol> <li>continued security patches and support beyond 2024</li> <li>updated system libraries to better support modern software</li> <li>improved node management system to facilitate the growing number of nodes on Milgram</li> </ol>"},{"location":"clusters/milgram_rhel8/#common-errors","title":"Common Errors","text":"<p>Below are common errors encountered when moving workflows to </p>"},{"location":"clusters/milgram_rhel8/#python-not-found","title":"Python not found","text":"<p>Under RHEL8, we have only installed Python 3, which must be executed using <code>python3</code> (not <code>python</code>).  As always, if you need additional packages, we strongly recommend setting up your own conda environment.</p> <p>In addition, Python 2.7 is no longer support and therefore not installed by default.  To use Python 2.7, we request you setup a conda environment.</p>"},{"location":"clusters/milgram_rhel8/#missing-system-libraries","title":"Missing System Libraries","text":"<p>Some of the existing applications may depend on libraries that are no longer installed in the operating system. If you run into these errors please email hpc@yale.edu and include which application/version you are using along with the full error message. We will investigate these on a case-by-case basis and work to get the issue resolved.</p>"},{"location":"clusters/milgram_rhel8/#report-issues","title":"Report Issues","text":"<p>If you continue to have or discover new issues with your workflow, feel free to contact us for assistance. Please include the working directory, the commands that were run, the software modules used, and any more information needed to reproduce the issue.</p>"},{"location":"clusters/misha/","title":"Misha","text":"<p>Misha is a cluster intended for use on projects associated with the Wu Tsai Institute, an interdisciplinary research endeavor at Yale University connecting neuroscience and data science to accelerate breakthroughs in understanding cognition. </p> <p>Misha is named for Dr. Misha Mahowald, an American computational neuroscientist in the neuromorphic engineering, known for her work on the silicon retina. </p> <p>Beta</p> <p>Misha is currently in closed beta. For access, please contact Ping Luo (ping.luo@yale.edu). </p>"},{"location":"clusters/misha/#access-the-cluster","title":"Access the Cluster","text":"<p>Once you have an account, the cluster can be accessed via ssh or through the Open OnDemand web portal.</p>"},{"location":"clusters/misha/#system-status-and-monitoring","title":"System Status and Monitoring","text":"<p>For system status messages and the schedule for upcoming maintenance, please see the system status page.</p>"},{"location":"clusters/misha/#partitions-and-hardware","title":"Partitions and Hardware","text":"<p>Misha is made up of several kinds of compute nodes. We group them into  (sometimes overlapping) Slurm partitions meant to serve different purposes. By combining the <code>--partition</code> and <code>--constraint</code> Slurm options you can more finely control what nodes your jobs can run on.</p> <p>Job Submission Limits</p> <ul> <li> <p>You are limited to 4 interactive app instances (of any type) at one time.  Additional instances will be rejected until you delete older open instances.  For OnDemand jobs, closing the window does not terminate the interactive app job. To terminate the job, click the \"Delete\" button in your \"My Interactive Apps\" page in the web portal.</p> </li> <li> <p>Job submissions are limited to 200 jobs per hour. See the Rate Limits section in the Common Job Failures page for more info.</p> </li> </ul>"},{"location":"clusters/misha/#public-partitions","title":"Public Partitions","text":"<p>See each tab below for more information about the available common use partitions.</p> day <p>Use the day partition for most batch jobs. This is the default if you don't specify one with <code>--partition</code>.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the day partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>1-00:00:00</code> Maximum CPUs per group <code>512</code> Maximum memory per group <code>3840G</code> Maximum CPUs per user <code>512</code> Maximum memory per user <code>1280G</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 18 6458 64 479 sapphirerapids, avx512, 6458q, common devel <p>Use the devel partition to jobs with which you need ongoing interaction. For example, exploratory analyses or debugging compilation.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the devel partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>06:00:00</code> Maximum CPUs per user <code>10</code> Maximum GPUs per user <code>4</code> Maximum memory per user <code>70G</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 2 6458 64 479 sapphirerapids, avx512, 6458q, common week <p>Use the week partition for jobs that need a longer runtime than day allows.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the week partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>7-00:00:00</code> Maximum CPUs per group <code>192</code> Maximum memory per group <code>1920G</code> Maximum CPUs per user <code>128</code> Maximum memory per user <code>1280G</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 6 6458 64 479 sapphirerapids, avx512, 6458q, common gpu <p>Use the gpu partition for jobs that make use of GPUs. You must request GPUs explicitly with the <code>--gpus</code> option in order to use them. For example, <code>--gpus=gtx1080ti:2</code> would request 2 GeForce GTX 1080Ti GPUs per node.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the gpu partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>2-00:00:00</code> Maximum CPUs per group <code>192</code> Maximum GPUs per group <code>24</code> Maximum CPUs per user <code>192</code> Maximum GPUs per user <code>18</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 2 6442 48 975 h100 4 80 sapphirerapids, avx512, 6442, doubleprecision, common, gpu, h100 2 6442 48 975 h100 4 80 sapphirerapids, avx512, 6442, doubleprecision, common, gpu, h100 6 6326 32 975 a40 4 48 icelake, avx512, 6326, doubleprecision, a40, common 5 6326 32 1000 a100 4 80 icelake, avx512, 6326, doubleprecision, a100, a100-80g, common gpu_devel <p>Use the gpu_devel partition to debug jobs that make use of GPUs, or to develop GPU-enabled code.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>GPU jobs need GPUs!</p> <p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <code>--gpus</code> option.</p> <p>Job Limits</p> <p>Jobs submitted to the gpu_devel partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>12:00:00</code> Maximum CPUs per user <code>10</code> Maximum GPUs per user <code>4</code> Maximum memory per user <code>100G</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) GPU Type GPUs/Node vRAM/GPU (GB) Node Features 1 6326 32 1000 icelake, avx512, 6326, doubleprecision, a100, a100-80g-MIG, common bigmem <p>Use the bigmem partition for jobs that have memory requirements other partitions can't handle.</p> <p>Request Defaults</p> <p>Unless specified, your jobs will run with the following options to <code>salloc</code> and <code>sbatch</code> options for this partition.</p> <pre><code>--time=01:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120\n</code></pre> <p>Job Limits</p> <p>Jobs submitted to the bigmem partition are subject to the following limits:</p> Limit Value Maximum job time limit <code>1-00:00:00</code> Maximum CPUs per user <code>64</code> Maximum memory per user <code>2T</code> <p>Available Compute Nodes</p> <p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p> Count CPU Type CPUs/Node Memory/Node (GiB) Node Features 2 6458 64 1991 sapphirerapids, avx512, 6458q, common"},{"location":"clusters/misha/#storage","title":"Storage","text":"<p><code>/gpfs/radev</code> is Misha's filesystem where home, project, and scratch directories are located. For more details on the different storage spaces, see our Cluster Storage documentation.</p> <p>You can check your current storage usage &amp; limits by running the <code>getquota</code> command. Note that the per-user usage breakdown only update once daily.</p> <p>For information on data recovery, see the Backups and Snapshots documentation.</p> <p>Warning</p> <p>Files stored in <code>scratch</code> are purged if they are older than 60 days. You will receive an email alert one week before they are deleted. Artificial extension of scratch file expiration is forbidden without explicit approval from the YCRC. Please purchase storage if you need additional longer term storage.</p> Partition Root Directory Storage File Count Backups Snapshots home <code>/gpfs/radev/home</code> 125GiB/user 500,000 Not yet &gt;=2 days project <code>/gpfs/radev/project</code> 1TiB/group, increase to 4TiB on request 5,000,000 No &gt;=2 days scratch <code>/gpfs/radev/scratch</code> 10TiB/group 15,000,000 No No"},{"location":"clusters-at-yale/","title":"Getting Started","text":"<p>New to HPC?</p> <p>Are you new to HPC clusters? Or used one before but new to Yale's systems? We encourage you to watch our Introduction to HPC workshop.</p>"},{"location":"clusters-at-yale/#hpc-clusters","title":"HPC Clusters","text":"<p>A high performance computing (HPC) cluster is a collection of networked computers and data storage. We refer to individual servers in this network as nodes. Our clusters are only accessible to researchers remotely; your gateways to the cluster are the login nodes. From these nodes, you view files and dispatch jobs to other nodes across the cluster configured for computation, called  compute nodes. The tool we use to manage these jobs is called a job scheduler. All compute nodes on a cluster mount a shared filesystem; a file server or set of servers store files on a large array of disks. This allows your jobs to access and edit your data from any compute node. </p> <ul> <li>Compute and storage systems</li> </ul>"},{"location":"clusters-at-yale/#request-an-account","title":"Request an Account","text":"<p>The first step in gaining access to one of our clusters is to request an account. All users must adhere to the YCRC HPC Policies.</p> <ul> <li>Account Request Form</li> </ul>"},{"location":"clusters-at-yale/#log-in","title":"Log in","text":"<p>Once you have an account, you can connect to the cluster either via our Web Portal or more traditional SSH access. If you want to access the clusters from outside Yale's network, you must use the Yale VPN.</p> <ul> <li>Log in to the Clusters </li> </ul>"},{"location":"clusters-at-yale/#submit-a-job","title":"Submit a Job","text":"<p>You control your computations using a job scheduling system called Slurm that allocates and manages compute resources for you. For testing and small jobs you may want to run a job interactively, which lets you interact with the compute node(s) in real time. Batch submission, the preferred way for multiple jobs or long-running jobs, involves writing your job commands in a script and submitting that to the job scheduler. </p> <ul> <li>Slurm Job Submissions</li> </ul>"},{"location":"clusters-at-yale/#use-software","title":"Use Software","text":"<p>We use software modules to make multiple versions of popular software available. Modules allow you to swap between different applications and versions of those applications with relative ease. We also provide assistance for installing less commonly used packages.</p> <ul> <li>Applications &amp; Software</li> </ul>"},{"location":"clusters-at-yale/#transfer-your-files","title":"Transfer Your Files","text":"<p>There are a number of methods for transferring file between your computer and the cluster, and the best for each situation usually depends on the size and number of files you would like to transfer. For most situations, uploading files through Open OnDemand's upload interface is the best option. This can be done directly through the file viewer interface by clicking the Upload button and dragging and dropping your files into the upload window. </p> <ul> <li>Transfer data using the Web Portal</li> <li>Other ways to transfer data (for larger transfers)</li> </ul>"},{"location":"clusters-at-yale/#get-help","title":"Get Help","text":"<p>Additional Training</p> <p>We offer several courses that range from orientation for beginners to advanced topics on application-specific optimization. Please peruse our catalog of training to see what is available.</p> <p>Our clusters run the Linux operating system, where we support the use of the Bash shell. If you are new to linux, check out our Intro to Linux Workshop.</p> <p>If you have additional questions/comments, please contact us.</p>"},{"location":"clusters-at-yale/glossary/","title":"Glossary","text":"<p>To help clarify the way we refer to certain terms in our user documentation, here is a brief list of some of the words that regularly come up in our documents. Please reach out to us at hpc@yale.edu if there are any other words that need to be added.</p> <p>Account - used to authenticate and grant permission to access resources</p> <p>Account (Slurm) - an accounting mechanism to keep track of a group's computing usage</p> <p>Activate - making something operational</p> <p>Array - a data structure across a series of memory locations consisting of elements organized in an index</p> <p>Array (job) - a series of jobs that all request the same resources and run the same batch script</p> <p>Array Task ID - a unique sequential number with an appended number that refers to an individual task within the set of submitted jobs</p> <p>Channel - Community-led collections of packages created by a group or lab installed with conda to allow for a homogenous environment across systems</p> <p>CLI - Command Line Interface processes commands to a computer program in the form of lines of text in a window</p> <p>Cluster - a set of computers, called nodes) networked together so nodes can perform the tasks facilitated by a scheduling software</p> <p>Command - a specific order from a computer to execute a service with either an application or the operating system</p> <p>Compute Node - the nodes that work runs on to perform computational work</p> <p>Container - A stack of software, libraries and operating system that is independent of the host computer and can be accessed on other computers</p> <p>Container Image - Self-contained read-only files used to run applications</p> <p>CPU - Central Processing Units are the components of a system that perform basic operations and exchange data with the system\u2019s memory (also known as a processor)</p> <p>Data - items of information collected together for reference or analysis</p> <p>Database - a collection of structured data held within a computer</p> <p>Deactivate - making something de-operational</p> <p>Environment - a collection of hardware, software, data storage and networks that work together in facilitating the processing and exchange of information</p> <p>Extension - Suffix at the end of a filename to indicate the file type</p> <p>Fileset - a section of a storage device that is given a designated purpose</p> <p>Filesystem - a process that manages how and where data is stored</p> <p>Flag - (see Options)</p> <p>GPU - Graphics Processing Units are specialized circuits designed to rapidly manipulate memory and create images in a frame buffer for a displayed output</p> <p>GridFTP - an extension of the Fire Transfer Protocol for grid computing that allows users to transfer and save data on a different account such as Google Drive or other off network memory</p> <p>Group - a collection of users who can all be given the same permissions on a system</p> <p>GUI - Graphical User Interface allows users to interact with devices and applications through a visual window that can commonly display icons and predetermined fields</p> <p>Hardware - the physical parts of a computer</p> <p>Host - (ie. Host Computer) A device connected to a computer network that offers resources, services and applications to users on the network</p> <p>Image - (See Container Image)</p> <p>Index - a method of sorting data by creating keywords or a listing of data</p> <p>Interface - a boundary across which two or more computer system components can exchange information</p> <p>Job - a unit of work given to an operating system by a scheduler</p> <p>Job Array - a way to submit multiple similar jobs by associating each subjob with an index value based on an array task id</p> <p>Key - a variable value applied using an algorithm to a block of unencrypted text to produce encrypted text</p> <p>Load - transfer a program or data into memory or into the CPU</p> <p>Login Node - a node that users log in on to access the cluster</p> <p>Memory - (see RAM)</p> <p>Metadata - A set of data that describes and gives basic information about other data</p> <p>Module - a number of distinct but interrelated units that build up or into a program</p> <p>MPI - Message Passing Interface is a standardized and portable message-passing standard designed to function on parallel computing architectures</p> <p>Multiprocessing - the ability to operate more than one task simultaneously on the same program across two or more processors in a computer</p> <p>Node - a server in the cluster</p> <p>Option - a single letter or full word that modifies the behavior of a command in a predetermined way (also known as a flag or switch)</p> <p>Package - a collection of hardware and software needed to create a working system</p> <p>Parallel - (ex. Computing/Programming) Architecture in which several processes are carried out simultaneously across smaller, independent parts</p> <p>Partition - a section of a storage device that is given a designated purpose</p> <p>Partition (Slurm) - a collection of compute nodes available via the scheduler</p> <p>Path - A string of characters used to identify locations throughout a directory structure</p> <p>Pane - (Associate with window) A subdivision within a window where an independent terminal can run simultaneously alongside another terminal</p> <p>Processor - (see CPU)</p> <p>Queue - a sequence of objects arranged according to priority waiting to be processed</p> <p>RAM - Random Access Memory, also known as \"Memory\" can be read and changed in any order and is typically used to to store working data</p> <p>Reproducibility - the ability to execute the same results across multiple systems by different individuals using the same data</p> <p>Scheduler - the software used to assign resources to a job for tasks</p> <p>Scheduling - the act of assigning resources to a task through a software product</p> <p>Session - a temporary information exchange between two or more devices</p> <p>SSH - secure shell is a cryptographic network protocol for operating network services securely over an unsecured network</p> <p>Software - a collection of data and instructions that tell a computer how to operate</p> <p>Switch - (see Options)</p> <p>System - a set of integrated hardware and software that input, output, process, and store data and information</p> <p>Task ID - a unique sequential number used to refer to a task</p> <p>Terminal - Referring to a terminal program, a text-based interface for typing commands</p> <p>Toolchain - a set of tools performing individual actions used in delivering an operation</p> <p>Unload - remove a program or data from memory or out of the CPU</p> <p>User - a person interacting and utilizing a computing service</p> <p>Variable - assigned and referenced data values that can be called within a program and changed depending on how the program runs</p> <p>Window - (Associate with pane) the whole screen being displayed, containing subdivisions, or panes, that can run independent terminals alongside each other</p>"},{"location":"clusters-at-yale/help-requests/","title":"Help Requests","text":"<p>See our Get Help section for ways to get assistance, from email support to setting up 1-on-1 appointments with our staff.  When requesting assistance provide the information described below (where applicable), so we can most effectively assist you. Before requesting assistance, we encourage you to take a look at the relevant documentation on this site.</p> <p>If you are new to the cluster, please watch our Intro to HPC tutorial available on the YCRC YouTube Channel as it covers many common usages of the systems.</p>"},{"location":"clusters-at-yale/help-requests/#troubleshoot-login","title":"Troubleshoot Login","text":"<p>If you are having trouble logging in to the cluster, please see our Troubleshoot Login guide.</p>"},{"location":"clusters-at-yale/help-requests/#information-to-provide-with-help-requests","title":"Information to Provide with Help Requests","text":"<p>Whenever requesting assistance with HPC related issues, please provide the YCRC staff with the following information (where applicable) so we can investigate the problem you are encountering. To assist with providing this information, we have included instructions below on retreiving the information if you are working in the command line interface.</p> <ul> <li>Your NetID</li> <li>name of the cluster you are working on (e.g. Grace, Milgram or McCleary)</li> <li> <p>instructions on how to repeat your issue. Please include the following:</p> <ul> <li>which directory are you working in or where you submitted your job<ul> <li>Run the command <code>pwd</code> when you are in the directory where you encountered the issue</li> </ul> </li> <li>the software modules you have loaded<ul> <li>Run <code>module list</code> when you encounter the issue</li> </ul> </li> <li>the commands you ran that resulted in the error or issue</li> <li>the name of the submission script your submitted to the scheduler with <code>sbatch</code> (if reporting an issue with a batch job)</li> </ul> </li> <li> <p>the error message you received, and, if applicable, the path to the output file containing the error message</p> <ul> <li>if you are using the default Slurm output options, this will look <code>slurm-&lt;your job id&gt;.out</code></li> <li>certain software may output additional information to other log files and, if applicable, include the paths to those files as well</li> </ul> </li> <li> <p>job ids for your Slurm jobs</p> <ul> <li>you can get the job ids for recently run jobs by running the command <code>sacct</code></li> <li>identify the job(s) that contained the error and provide the job id(s)</li> </ul> </li> </ul> <p>If possible, please paste the output into the email or include in a text file as an attachment. Screenshots or pictures are very hard for us to work with.</p> <p>We look forwarding to assisting you!</p>"},{"location":"clusters-at-yale/troubleshoot/","title":"Troubleshoot Login","text":""},{"location":"clusters-at-yale/troubleshoot/#checklist","title":"Checklist","text":"<p>If you are having trouble logging into a cluster, please use the checklist below to check for common issues:</p> <ul> <li> Make sure you have submitted an account request and have gotten word that we created your account for the cluster.</li> <li> Make sure that the cluster is online in the System Status page.</li> <li> Check the hostname for the cluster. See the clusters page for a list.</li> <li> Verify that your ssh keys are setup correctly<ul> <li> Check for your public key in the ssh key uploader. If you recently uploaded one, it will take a few minutes appear on the cluster. </li> <li> If you are using macOS or Linux, make sure your private key is in <code>~/.ssh</code>.</li> <li> If you are using Windows, make sure you have pointed MobaXterm to your private ssh key (ends in .pem)</li> <li> If you are asked for a passphrase when logging in, this is the ssh key passphrase you set when first creating your key pair. If you have forgotten this passphrase, you need to create a new key pair and upload a new public key.</li> </ul> </li> <li> Make sure your computer is either on Yale's campus network (ethernet or YaleSecure wireless) or Yale's VPN.<ul> <li> If you get an error like <code>could not resolve hostname</code> you may have lost connection to the Yale network. If you are sure you have not, make sure that you are also using the Yale DNS servers (172.18.190.12,20,28).</li> </ul> </li> <li> Your home directory should only be writable by you. If you recently modified the permissions to your home directory and can't log in, contact us and we can fix the permissions for you.</li> <li> If you are using McCleary or Milgram, we require Duo MFA for every login. If following our MFA Troubleshooting steps doesn't  work, contact the ITS Help Desk.</li> </ul> <p>If none of the above solve your issue, please contact us with your netid and the cluster you are attempting to connect to.</p>"},{"location":"clusters-at-yale/troubleshoot/#common-ssh-errors","title":"Common SSH Errors","text":""},{"location":"clusters-at-yale/troubleshoot/#permission-denied-publickey","title":"Permission denied (publickey)","text":"<p>This message means that the clusters don't (yet) have they key you are using to authenticate. Make sure you have an account on the cluster you're connecting, that you have created an ssh key pair, and uploaded the public key. If you recently uploaded one, it will take a few minutes appear on the cluster.</p>"},{"location":"clusters-at-yale/troubleshoot/#remote-host-identification-has-changed","title":"REMOTE HOST IDENTIFICATION HAS CHANGED!","text":"<p>If you are seeing the following error:</p> <pre><code>@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nIT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!\n....\nOffending key in /home/user/.ssh/known_hosts:34\n...\n</code></pre> <p>This usually means that the keys that identify the cluster login nodes have changed. This can be the result of system upgrades on the cluster (see Grace August 2023 Maintenance). It could also mean someone is trying to intercept your ssh session. Please contact us if you receive this error outside of known system upgrades.</p> <p>If the host keys have indeed changed on the server you are connecting to, you can edit <code>~/.ssh/known_hosts</code> and remove the offending line. In the example above, you would need to delete line 34 in <code>~/.ssh/known_hosts</code> before you re-connect.</p>"},{"location":"clusters-at-yale/access/","title":"Log on to the Clusters","text":"<p>To log on the cluster, you must first request an account (if you do not already have one). When using the clusters, please review and abide by our HPC usage policies and best practices.</p> <p>Off Campus Access</p> <p>You must be on the campus network to access the clusters. For off-campus access you need to use the Yale VPN.</p>"},{"location":"clusters-at-yale/access/#web-portal-open-ondemand","title":"Web Portal - Open OnDemand","text":"<p>For most users, we recommend using the web portal, Open OnDemand, to access the clusters. For hostnames and more instructions see our Open OnDemand documentation.</p>"},{"location":"clusters-at-yale/access/#ssh-connection","title":"SSH Connection","text":"<p>For more advanced use cases that are not well supported by the Web Portal (Open OnDemand), you can connect to the clusters over the more traditional SSH connection.</p>"},{"location":"clusters-at-yale/access/accounts/","title":"Accounts &amp; Best Practices","text":"<p>The YCRC HPC Policies can found here. All users are required to abide by the described policies.</p>"},{"location":"clusters-at-yale/access/accounts/#user-responsibilities","title":"User Responsibilities","text":"<ul> <li>User accounts are personal to individual users and may not be shared. Never give your password or ssh key to anyone else. Never allow another individual to use your account. </li> <li>Do not run jobs, transfers or computation on a login node, instead submit jobs.</li> <li>Similarly, transfer nodes are only for data transfers. Do not run jobs or computation on the transfer nodes.</li> <li>Do not store any high risk data on the clusters, except Milgram.</li> <li>Jobs must be submitted to partitions in alignment with the stated purposes and limits of those partitions.</li> <li>Do not run large numbers of very short (less than a minute) jobs.</li> <li>Terminate interactive or Open OnDemand session when no longer in use. Idle sessions may be canceled without warning.</li> <li>Avoid workflows that generate numerous (thousands) of files as these put great stress on the shared filesystem.</li> <li>Use of scratch for long term storage, (through artificial extension of file expiration or other means) is forbidden without explicit approval from the YCRC. Please purchase storage if you need additional longer term storage.</li> <li>Any data covered by a DUA must be explicitly approved by the YCRC before it is stored on a cluster.</li> <li>Each YCRC cluster undergoes regular scheduled maintenance twice a year, see our maintenance schedule for more details. Please plan accordingly.</li> </ul>"},{"location":"clusters-at-yale/access/accounts/#group-allocations","title":"Group Allocations","text":"<p>A research group may request an allocation on one of Yale's HPC clusters. Each group is granted access to the common compute resources and a limited cluster storage allocation. </p>"},{"location":"clusters-at-yale/access/accounts/#external-collaborators","title":"External Collaborators","text":"<p>The YCRC can provide access to the clusters and Yale\u2019s network for collaborators at other institutions through Yale\u2019s \u201cSponsored NetID\u201d service. To request access for a new collaborator or extend access for a departing student or post doc, have your business office fill out the Sponsored NetID Request Form and VPN Access Request Form. After receiving the sponsored NetID, you may submit the HPC Account Request Form.</p>"},{"location":"clusters-at-yale/access/accounts/#request-an-account","title":"Request an Account","text":"<p>You may request an account on a cluster using the account request form.  User accounts are personal to individual users and may not be shared. Under no circumstances may any user make use of another user\u2019s account.</p>"},{"location":"clusters-at-yale/access/accounts/#inactive-accounts-and-account-deletion","title":"Inactive Accounts and Account Deletion","text":"<p>For security and communication purposes, you must have a valid email address associated with your account. Login privileges will be disable on a regular basis for any accounts without a valid email address. Therefore, if you are leaving Yale, but will continue to use the cluster on a \"Sponsored netid\", please contact us to update the email address associated with your account as soon as possible. If you find your login has been disabled, please contact us to provide a valid email address to have your login reinstated.</p> <p>Additionally, an annual account audit is performed on November 1st and any accounts associated with an inactive netids (regular and Sponsored netids) will be deactivated at that time. Note that Sponsored netids need to be renewed annually through the appropriate channels.</p> <p>When an account is deactivated, logins and scheduler access are disabled, the home directory is archived for 5 years and all project data owned by the account is reassigned to the group's PI. The group's PI will receive a report once a year in November with a list of deactivated group members. </p> <p>Every group must have a PI with a valid affiliation with Yale. If your PI has left Yale, you may be asked to identify a new faculty sponsor for your account in order to continue accessing the cluster.</p>"},{"location":"clusters-at-yale/access/advanced-config/","title":"Advanced SSH Configuration","text":""},{"location":"clusters-at-yale/access/advanced-config/#example-ssh-config","title":"Example SSH <code>config</code>","text":"<p>The following configuration is an example ssh client configuration file specific to our clusters. You can use it on Linux, Windows Subsystem for Linux (WSL), and macOS. It allows you to use tab completion of the clusters, without the <code>.ycrc.yale.edu</code> suffixes (i.e. <code>ssh grace</code> or <code>scp ~/my_file grace:my_file</code> should work). It will also allow you to re-use and multiplex authenticated sessions. This means although the clusters require Duo MFA, it will not force you to re-authenticate, as you use the same ssh connection to host multiple sessions. If you attempt to close your first connection with others running, it will wait until all others are closed.</p> <p>Save the text below to <code>~/.ssh/config</code> and replace <code>NETID</code> with your Yale netid. Lines that begin with <code>#</code> will be ignored.</p> <pre><code># If you use a ssh key that is named something other than id_rsa,\n# you can specify your private key like this:\n# IdentityFile ~/.ssh/other_key_rsa\n\n# Uncomment the ForwardX11 options line to enable X11 Forwarding by default (no -Y necessary)\n# On a Mac you still need xquartz installed\n\nHost *.ycrc.yale.edu bouchet grace mccleary milgram misha\n    User NETID\n    #ForwardX11 yes\n    # To re-use your connections with multi-factor authentication\n    # Uncomment the two lines below\n    #ControlMaster auto\n    #ControlPath /tmp/%h_%p_%r\n    #ControlPersist 2h\n\nHost mccleary grace milgram misha\n    HostName %h.ycrc.yale.edu\n</code></pre> <p>For more info on ssh configuration, run:</p> <pre><code>man ssh_config\n</code></pre>"},{"location":"clusters-at-yale/access/advanced-config/#store-passphrase-and-use-ssh-agent-on-macos","title":"Store Passphrase and Use SSH Agent on macOS","text":"<p>By default, macOS won't always remember your ssh key passphrase and keep your ssh key in the agent for SSH agent forwarding. In order to not repeatedly enter your passphrase and instead store it in your keychain, enter the following command on your Mac (just once):</p> <pre><code># In MacOS version 12.0 Monterey or newer\nssh-add --apple-use-keychain ~/.ssh/id_rsa\n\n# Older MacOS version\nssh-add -K ~/.ssh/id_rsa\n</code></pre> <p>Or whatever your private key file is named.</p> <p>Note</p> <p>If you use homebrew your default OpenSSH may have changed. To add your key(s) to the system ssh agent, use the absolute path: <code>/usr/bin/ssh-add</code></p> <p>Then and add the following to your <code>~/.ssh/config</code> file (create this file if it doesn't exist, or add these settings to the <code>Host *.ycrc.yale.edu ...</code> rule if it does).</p> <pre><code>Host *.ycrc.yale.edu mccleary grace milgram misha\n    UseKeychain yes\n    AddKeystoAgent yes\n</code></pre> <p>You can view a list of the keys currently in your agent with:</p> <pre><code>ssh-add -L\n</code></pre>"},{"location":"clusters-at-yale/access/courses/","title":"Courses","text":"<p>The YCRC Grace and McCleary clusters can be made available for Yale courses with a suitable computational component. The YCRC hosts over a dozen courses on the clusters every semester.</p> <p>Warning</p> <p>All course allocations are temporary. All associated accounts and data will be removed one month after the last day of exams for that semester.</p> <p>For Instructors</p> <p>If you are interested in using a YCRC cluster in your Yale course, fill out this form. If at all possible, please let us know of your interest in using a cluster at least two weeks prior to start of classes so we can plan accordingly, even if you have used the cluster in a previous semester.</p>"},{"location":"clusters-at-yale/access/courses/#course-id","title":"Course ID","text":"<p>Your course will be give a specific <code>courseid</code> based on the Yale course catalog number. This <code>courseid</code> will be used in the course account names and web portal.</p>"},{"location":"clusters-at-yale/access/courses/#course-accounts","title":"Course Accounts","text":"<p>All members of a course, including the instructor and TFs will be given temporary course accounts. These accounts take the form of <code>courseid_netid</code>. Course accounts are distinct from any research accounts a course member may already have. As with all cluster access, you must be on the VPN to access the web portal if you are off campus.</p> <p>All course-related accounts are subject to the same policies and expectation as standard accounts.</p>"},{"location":"clusters-at-yale/access/courses/#course-specific-web-portal","title":"Course-specific Web Portal","text":"<p>Your course also has a course-specific web portal, based on Open OnDemand, accessible via the URL (replacing <code>courseid</code> with the id given to your course):</p> <pre><code>courseid.ycrc.yale.edu\n</code></pre> <p>Course members must use the course URL to log in to course accounts on Open OnDemand--the normal cluster portals are not accessible to course accounts. You will then authenticate using your standard NetID (without the courseid prefix) and password. </p>"},{"location":"clusters-at-yale/access/courses/#ssh-access","title":"SSH Access","text":"<p>To access your course account via terminal and <code>ssh</code> authentication, connect to the cluster using your course account name. For example:</p> <pre><code>ssh courseid_netid@grace.ycrc.yale.edu\n\n# or\n\nssh courseid_netid@mccleary.ycrc.yale.edu\n</code></pre> <p>If you already have a permanent researcher account (one that is just your netid) on one of the clusters, the course account will already be setup with any ssh keys previously uploaded to your researcher account. To add a new key, upload your new key and it will be delivered to all of your accounts within a few minutes.</p>"},{"location":"clusters-at-yale/access/courses/#course-storage","title":"Course Storage","text":"<p>Courses on the YCRC clusters are typically granted a standard 1TiB project storage quota, as well as 125GiB home directory for each course member. If the course needs additional storage beyond the default 1TiB, please contact us at research.computing@yale.edu.</p> <p>See our cluster storage documentation for details about the different classifications of storage.</p>"},{"location":"clusters-at-yale/access/courses/#node-reservations","title":"Node Reservations","text":"<p>If the instructor has coordinated with the YCRC for dedicated nodes for the course, they are available in an <code>education</code> or <code>education_gpu</code> partition. The nodes can be requested using the <code>-p partition_name</code> flag. See our Slurm documentation for more information on submitting jobs. Note you will be sharing the partitions with any other courses that also requested nodes. If your jobs need to exceed the restrictions of the partitions, please have your instructor or TF contact us.</p> <p>Course members are welcome to use the public partitions of the cluster. However, we request that students be respectful in their usage as not to disrupt ongoing research work.</p>"},{"location":"clusters-at-yale/access/courses/#interactive-jobs","title":"Interactive Jobs","text":"<pre><code>salloc -p education \n</code></pre> <p>or if your instructor has reserved GPU nodes</p> <pre><code>salloc -p education_gpu --gpus=1\n</code></pre>"},{"location":"clusters-at-yale/access/courses/#batch-jobs","title":"Batch Jobs","text":"<p>Add the following to your submission script:</p> <pre><code>#SBATCH -p education\n</code></pre> <p>or if your instructor has reserved GPU nodes</p> <pre><code>#SBATCH -p education_gpu --gpus=1\n</code></pre>"},{"location":"clusters-at-yale/access/courses/#web-portal","title":"Web Portal","text":"<p>In any of the app submission forms, type the correct paritition name into the \"Partition\" field.</p>"},{"location":"clusters-at-yale/access/courses/#cluster-maintenance","title":"Cluster Maintenance","text":"<p>Each cluster is inaccessible twice a year for a three day regularly scheduled maintenance. The maintenance schedule is published here.</p> <p>Please account for the cluster unavailability when developing course schedules and (for students) completing your assignments.</p>"},{"location":"clusters-at-yale/access/courses/#end-of-semester-course-deletion","title":"End of Semester Course Deletion","text":"<p>As mentioned above, all course allocations are temporary. All associated accounts and data will be removed one month after the last day of exams for that semester. If you would like to retain any data in your course account, please download it prior to the deletion date or, if applicable, submit a request to hpc@yale.edu to transfer the data into your research account. </p> <p>A reminder of the removal will be sent to the instructor to see if it needs to be delayed for any incompletes (for example). Students will not receive a reminder. Instructors, if you would like to retain course materials for future semesters, please copy them off the cluster or to a research account.</p>"},{"location":"clusters-at-yale/access/courses/#transfer-data-to-research-account","title":"Transfer Data to Research Account","text":"<p>If you have a research account on the cluster, you can transfer any data you want to save from your course account to your research account.</p> <p>Warning</p> <p>Make sure there is sufficient free space in your research account storage to accomodate any data you are transferring from your course account using <code>getquota</code>.</p> <ol> <li> <p>Login to the cluster using your course account either via Terminal or the Shell app in the OOD web portal.</p> </li> <li> <p>Grant your research account access to your course accounts directories (substitute in your courseid and netid in the example).</p> <pre><code># home directory\nsetfacl -m u:netid:rX /home/courseid_netid\n\n# project directory on Grace and McCleary\nsetfacl -m u:netid:rX /gpfs/gibbs/project/courseid/courseid_netid\n</code></pre> </li> <li> <p>Log in as your research account. Check that you can access the above paths.</p> </li> <li> <p>Move to the <code>transfer</code> node with <code>ssh transfer</code>. If you are transferring a lot of data, open a tmux session so the transfer can continue if you disconnect from the cluster.</p> </li> <li> <p>Initiate a copy of the desired data using <code>rsync</code>. For example:</p> <pre><code>mkdir /gpfs/gibbs/project/group/netid/my_course_data\nrsync -av /gpfs/gibbs/project/courseid/courseid_netid/mydata /gpfs/gibbs/project/group/netid/my_course_data\n</code></pre> </li> </ol>"},{"location":"clusters-at-yale/access/mfa/","title":"Multi-factor Authentication","text":"<p>To improve security, access to all cluster requires both a public key and multi-factor authentication (MFA). We use the same MFA (Duo) as is used elsewhere at Yale. To get set up with Duo, see these instructions.</p> <p>You will need upload your ssh public key to our site. For more info on how to use ssh, please see the SSH instructions.</p> <p>Once you've set up Duo and your key is registered, you can log in to the cluster. Use ssh to connect to your cluster of choice, and you will be prompted for a passcode or to select a notification option. We recommend choosing Duo Push (option 1). If you chose this option you should receive a notification on your phone. Once approved, you should be allowed to continue to log in.</p> <p>Note</p> <p>You can set up more than one phone for Duo. For example, you can set up your smartphone plus your office landline. That way, if you forget or lose your phone, you can still authenticate. For instructions on how to add additional phones go here.</p>"},{"location":"clusters-at-yale/access/mfa/#connection-multiplexing-and-file-transfers-with-duo-mfa","title":"Connection Multiplexing and File Transfers with DUO MFA","text":"<p>Some file transfer clients attempt new and sometimes multiple concurrent connections to transfer files for you. When this happens, you will be asked to Duo authenticate for each connection.</p>"},{"location":"clusters-at-yale/access/mfa/#ssh-config-file","title":"SSH Config File","text":"<p>On macOS and Linux-based systems setting up a config file lets you re-uses your authenticated sessions for command-line tools and tools that respect your ssh configuration.  An example config file is shown below which enables SSH multiplexing (<code>ControlMaster</code>) by caching connections in a directory (<code>ControlPath</code>) for a period of time (2h, <code>ControlPersist</code>). </p> <pre><code># If you use a ssh key that is named something other than id_rsa,\n# you can specify your private key like this:\n# IdentityFile ~/.ssh/other_key_rsa\n\n# Uncomment the ForwardX11 options line to enable X11 Forwarding by default (no -Y necessary)\n# On a Mac you still need xquartz installed\n\nHost *.ycrc.yale.edu bouchet grace mccleary milgram misha\n    User NETID\n    #ForwardX11 yes\n    # To re-use your connections with multi-factor authentication\n    # Uncomment the two lines below\n    #ControlMaster auto\n    #ControlPath /tmp/%h_%p_%r\n    #ControlPersist 2h\n\nHost mccleary grace milgram misha\n    HostName %h.ycrc.yale.edu\n</code></pre> <p>Tip</p> <p>You can change the <code>ControlPath</code> directory to <code>/tmp</code> or any other directory, so long as it exists. </p>"},{"location":"clusters-at-yale/access/mfa/#cyberduck","title":"CyberDuck","text":"<p>CyberDuck's interface with MFA can be stream-lined with a few additional configuration steps. Under <code>Cyberduck &gt; Preferences &gt; Transfers &gt; General</code> change the setting to \"Use browser connection\" instead of \"Open multiple connections\".</p> <p>When you connect type one of the following when prompted with a \"Partial authentication success\" window.</p> <ul> <li>\"push\" to receive a push notification to your smart phone (requires the Duo mobile app)</li> <li>\"sms\" to receive a verification passcode via text message</li> <li>\"phone\" to receive a phone call</li> </ul>"},{"location":"clusters-at-yale/access/mfa/#mobaxterm","title":"MobaXTerm","text":"<p>MobaXTerm is able to cache MFA connections to reduce the frequency of push notifications. Under <code>Settings &gt; SSH &gt; Advanced SSH settings</code> set the ssh browser type to <code>scp (enhanced speed)</code> as seen here:</p> <p>MobaXTerm SSH Settings</p>"},{"location":"clusters-at-yale/access/mfa/#winscp","title":"WinSCP","text":"<p>Similarly, WinSCP can reuse existing SSH connections to reduce the frequency of push notifications.  Under <code>Options &gt; Preferences &gt; Background (under Transfer)</code> and:</p> <ul> <li>Set <code>Maximal number of transfers at the same time:</code> to 1</li> <li>Check the <code>Use multiple connections for single transfer</code> box</li> <li>Click <code>OK</code> to save settings</li> </ul>"},{"location":"clusters-at-yale/access/mfa/#troubleshoot-mfa","title":"Troubleshoot MFA","text":"<p>If you are having problems initially registering Duo, please contact the Yale ITS Helpdesk.</p> <p>If you have successfully used MFA connect to a cluster before, but cannot now, first please check the following:</p> <ul> <li>Test MFA using http://access.yale.edu</li> <li>Verify that your ssh client is using the correct login node</li> <li>Verify you are attempting to connect from a Yale machine or via the proper VPN</li> </ul> <p>If all of this is true, please contact us. Include the following information (and anything else you think is helpful):</p> <ul> <li>Your netid</li> <li>Have you ever successfully used ssh and Duo to connect to a cluster?</li> <li>How long have you been having problems?</li> <li>Where are you trying to connect from? (fully qualified hostname/IP, if possible)</li> <li>Are you using a VPN?</li> <li>What is the error message you see?</li> </ul>"},{"location":"clusters-at-yale/access/ood/","title":"Web Portal (Open OnDemand)","text":"<p>Open OnDemand (OOD) is platform for accessing the clusters that only requires a web browser. This web-portal provides a shell, file browser, and graphical interface for certain apps (like Jupyter or MATLAB).</p>"},{"location":"clusters-at-yale/access/ood/#access","title":"Access","text":"<p>If you access Open OnDemand installed on YCRC clusters from off campus, you will need to first connect to the Yale VPN. </p> <p>Open OnDemand is available on each cluster using your NetID credentials (CAS login). The Yale CAS login is configured with the DUO authentication. We recommend that you click \"Remember me for 90 days\" when you are prompted to choose an authentication menthod for DUO. This will simplified the login process.</p> Cluster OOD site Grace ood-grace.ycrc.yale.edu McCleary ood-mccleary.ycrc.yale.edu Milgram ood-milgram.ycrc.yale.edu <p>The above four URLs are also called cluster OOD URLs. They are available to any user with a research account (also called a lab account) on the clusters. Your research account is the same as your NetID. </p>"},{"location":"clusters-at-yale/access/ood/#ood-for-courses","title":"OOD for Courses","text":"<p>Each course on the YCRC clusters has its own URL to access OOD on the cluster. The URL is unique to each course and is also called course OOD.  Course OODs all follow the same naming convention: coursename.ycrc.yale.edu. 'courename' is an abbreviated name given to the course by YCRC.  Students must use the course URL to log in to OOD. They will with their NetID to log in but work under their student account on the cluster while they are in OOD. </p> <p>Course OOD and cluster OOD have different URLs, even if they use the same physical machine.  Student accounts can only log in to OOD through a course OOD URL, and a regular account (same as your NetID) can only log in through the cluster OOD URL.</p> <p>Warning</p> <p>If you only have a student account, but try to log in through the cluster OOD URL, you will get an error in the browser: <pre><code>Error -- can't find user for cpsc424_test\nRun 'nginx_stage --help' to see a full list of available command line options.\n</code></pre> Use the URL for your course OOD will resolve the problem.</p> <p>Additional information about course OOD can be found at academic support.</p>"},{"location":"clusters-at-yale/access/ood/#the-dashboard","title":"The Dashboard","text":"<p>On login you will see the OOD dashboard.</p> <p></p> <p>Along the top are pull-down menus for various Apps, including File Managers, Job Composer, a Shell, a list of Interactive Apps, etc.</p>"},{"location":"clusters-at-yale/access/ood/#file-browser","title":"File Browser","text":"<p>The file browser is a graphical interface to manage, upload, and download files from the clusters. You can use the built-in file editor to view and edit files from your browser without having to download and upload scripts.</p> <p>You can also drag-and-drop to download and upload files and directories, and move files between directories using this interface.</p>"},{"location":"clusters-at-yale/access/ood/#customize-favorite-paths","title":"Customize Favorite Paths","text":"<p>Users are allowed to customize favorite paths in the file manager. Using the scripts below to add, remove, and list customized paths:</p> <p><pre><code>  ood_add_path\n  ood_remove_path\n  ood_list_path\n</code></pre> When you run <code>ood_add_path</code> from a shell command line, it will prompt you to add one path at a time, until you type 'n' to discontinue. All the paths added by you will be shown in the OOD pull-down menu for the file manager, as well as the left pane when the file manager is opened. </p> <p><code>ood_remove_path</code> allows you to remove any of the paths added by you and <code>ood_list_path</code> will list all the paths added by you. </p> <p>After you have customized the path configuration from a shell, go to the OOD dashbaord and click <code>Develop</code>-&gt;<code>Restart Web Server</code> on the top menu bar to make the change effective immediately.  </p>"},{"location":"clusters-at-yale/access/ood/#shell","title":"Shell","text":"<p>You can launch a traditional command-line interface to the cluster using the Shell pull-down menu. This opens a terminal in a web-browser that you can use in the exact same way as when logging into the cluster via SSH.</p> <p>This is a convenient way to access the clusters when you don't have access to an ssh client or do not have your ssh keys.</p>"},{"location":"clusters-at-yale/access/ood/#interactive-apps","title":"Interactive Apps","text":"<p>We have deployed a selection of common graphical programs as Interactive Apps on Open OneDemand. Currently, we have apps for Remote Desktop, MATLAB, Mathematica, RStudio Desktop, RStudio Server, and Jupyter Notebook, etc.</p> <p>Warning</p> <p>You are limited to 4 interactive app instances (of any type) at one time.  Additional instances will be rejected until you delete older open instances.  Closing the window does not terminate the interactive app job. To terminate the job, click the \"Delete\" button in your \"My Interactive Apps\" page in the web portal.</p>"},{"location":"clusters-at-yale/access/ood/#remote-desktop","title":"Remote Desktop","text":"<p>Occasionally, it is helpful to use a graphical interface to explore data or run certain programs. In the past your options were to use VNC or X11 forwarding. These tools can be complex to setup or suffer from reduced performance. The Remote Desktop app from OOD simplifies the configuration of a VNC desktop session on a compute node. The MATLAB, Mathematica, and RStudio Desktop Apps are special versions of this app. To get started choose Remote Desktop (or another desktop app) from the Interactive Apps menu on the dashboard.</p> <p>Use the form to request resources and decide what partition your job should run on. Use <code>devel</code> (<code>interactive</code> on Milgram) or your lab's partition.</p> <p></p> <p>Once you launch the job, you will be presented with a notification that your job has been queued. Depending on the resources requested, you may need to wait for a bit. When the job starts you will see the option to launch the Remote Desktop:</p> <p></p> <p>Note you can share a view only link for your session if you would like to share your screen. After you click on Launch Remote Desktop, a standard desktop interface will open in a new tab. </p>"},{"location":"clusters-at-yale/access/ood/#copypaste","title":"Copy/Paste","text":"<p>In some browsers, you may have to use a special text box to copy and paste from the Remote Desktop App. Click the arrow on the left side of your window for a menu, then click the clipboard icon to get access to your Remote Desktop's clipboard.</p> <p></p>"},{"location":"clusters-at-yale/access/ood/#jupyter","title":"Jupyter","text":"<p>One of the most common uses of Open OnDemand is the Jupyter interface for Python and R. You can choose either Jupyter Notebook or Jupyter Lab. By default, this app will try to launch Jupyter Notebook, unless the <code>Start JupyterLab</code> checkbox is selected. </p> <p></p> <p>Make sure that you chose the right Conda environment for you from the drop-down menu. If you have not yet set one up, follow our instructions on how to create a new one. After specifying the required resources (number of CPUs/GPUs, amount of RAM, etc.), you can submit the job. When it launches you can open the standard Jupyter interface where you can start working with notebooks.</p>"},{"location":"clusters-at-yale/access/ood/#root-directory","title":"Root directory","text":"<p>The Jupyter root directory is set to your Home when started. Project and Scratch can be accessed via their respective symlinks in Home. If you want to access a directory that cannot be acessed through your home directory, for example Gibbs, you need to create a symlink to that directory in your home directory. </p>"},{"location":"clusters-at-yale/access/ood/#ycrc_default","title":"ycrc_default","text":"<p>The <code>ycrc_default</code> conda environment will be automatically built when you select it for the first time from Jupyter. You can also build your own Jupyter and make it available to OOD:</p> <pre><code>module load miniconda\nconda create -n env_name jupyter jupyter-lab\nycrc_conda_env.sh update\n</code></pre> <p>Once created, <code>ycrc_default</code> will not be updated by OOD automatically. It must be updated by the user manually. To update <code>ycrc_default</code>, run the following command from a shell command line: <pre><code>module load miniconda\nconda update -n  ycrc_default jupyter jupyter-lab\n</code></pre></p>"},{"location":"clusters-at-yale/access/ood/#vscode-server","title":"VSCode Server","text":"<p>Visual Studio Code is a popular development tool that is widely used by our researchers. While there are several extensions that allow users to connect to remote servers over SSH, these are imperfect and often drop connection.  Additionally, these remote sessions connect to the clusters' login nodes, where resources are limited. We have developed an application for OOD that launches VS Code in a job on a compute node and opens in a web-browser.  This application is called <code>code_server</code> and is available on all clusters.</p>"},{"location":"clusters-at-yale/access/ood/#rstudio-server","title":"RStudio Server","text":"<p>RStudio Server works with R from an R module or from an R Conda environment. Selected R modules on the cluster (ususally the two most recent versions installed on the cluster) are available in the user form. If you want to use Conda R, you need to run <code>ycrc_conda_env.sh update</code> to have it listed in the user form. </p>"},{"location":"clusters-at-yale/access/ood/#change-user-r-package-path","title":"Change User R Package Path","text":"<p>To change the default path where packages installed by the user are stored, you need to add the following line of code in your <code>$HOME/.bashrc</code>:</p> <pre><code>export R_LIBS_USER=path_to_your_local_r_packages\n</code></pre>"},{"location":"clusters-at-yale/access/ood/#configure-the-graphic-device","title":"Configure the Graphic Device","text":"<p>When you plot in a RStudio session, you may encounter the following error:</p> <pre><code>Error in RStudioGD() : \n  Shadow graphics device error: r error 4 (R code execution error)\nIn addition: Warning message:\nIn grDevices:::png(\"/tmp/RtmpcRxRaB/4v3450e3627g4432fa27f516348657267.png\",  :\n  unable to open connection to X11 display ''\n</code></pre> <p>To fix the problem, you need to configure your RStudio session to use <code>Cairo</code> for plotting.  You can do it in your code as follows: </p> <pre><code>options(bitmapType='cairo')\n</code></pre> <p>Alternatively, you can put the above code in <code>.Rprofile</code> in your home directory and the option will be picked up automatically. </p>"},{"location":"clusters-at-yale/access/ood/#clean-rstudio","title":"Clean RStudio","text":"<p>If RStudio becomes slow to respond or completely stops responding, please stop the RStudio session and then run the following script at a shell command line:</p> <pre><code>ycrc_clean_rstudio.sh\n</code></pre> <p>This will remove any temporary files created by RStudio and allow it to start anew.</p>"},{"location":"clusters-at-yale/access/ood/#run-rstudio-in-remote-desktop","title":"Run RStudio in Remote Desktop","text":"<p>While we don't generally encourage our users to run a production R code in RStudio, there are cases that it could be benificial.  For example, when a user needs to monitor the R code's progress constantly.</p> <p>RStudio Server is not user friendly for long-running R code. When your CAS session timeout, you won't be able to reconnect while the code is running. You'll need to wait until the code finishes before you can connect to the same session again. </p> <p>If you need to monitor your R code's progress continuously within the same R session without concerns about disconnection,  you can run RStudio Desktop within a Remote Desktop environment.</p>"},{"location":"clusters-at-yale/access/ood/#using-r-module-with-rstudio-desktop","title":"Using R module with RStudio Desktop","text":"<p>First, start a Remote Desktop instance in OOD. From the terminal in the Remote Desktop, run the following commands:</p> <pre><code>module load R\nmodule load RStudio\nrstudio\n</code></pre>"},{"location":"clusters-at-yale/access/ood/#using-r-conda-with-rstudio-desktop","title":"Using R Conda with RStudio Desktop","text":"<p>If you want to use R in a Conda environment, start a Remote Desktop instance in OOD first.  From the terminal in the Remote Desktop, please don't load the modules for R and RStudio.  Instead, please install 'rstudio-desktop' into your R Conda environment if you haven't done so,  and then call <code>rstudio</code>.  </p> <pre><code>module load miniconda\nconda activate my_r_env\nconda install rstudio-desktop\nrstudio\n</code></pre>"},{"location":"clusters-at-yale/access/ood/#troubleshoot-ood","title":"Troubleshoot OOD","text":""},{"location":"clusters-at-yale/access/ood/#an-ood-session-is-started-and-then-completed-immediately","title":"An OOD session is started and then completed immediately","text":"<ol> <li>Check if your quota is full</li> <li>Reset your <code>.bashrc</code> and <code>.bash_profile</code> to their original contents (you can backup the startup files before resetting them. Add the changes back one at a time to see if one or more of the changes would affect OOD from starting properly)  </li> <li>Remove the default module collection file <code>$HOME/.lmod.d/default.cluster-rhel8</code> (cluster is one of the following: grace, mccleary) or <code>$HOME/.lmod.d/default.milgram-rhel7</code> for Milgram.</li> </ol>"},{"location":"clusters-at-yale/access/ood/#remote-desktop-or-matlab-mathematica-etc-cannot-be-started-properly","title":"Remote Desktop (or MATLAB, Mathematica, etc) cannot be started properly","text":"<ol> <li>Make sure there is no initialization left by <code>conda init</code> in your <code>.bashrc</code>. Clean it with  <pre><code>sed -i.bak -ne '/# &gt;&gt;&gt; conda init/,/# &lt;&lt;&lt; conda init/!p' ~/.bashrc\n</code></pre></li> <li>Run <code>dbus-launch</code> and make sure you see the following output: <pre><code>[pl543@grace1 ~]$ which dbus-launch\n/usr/bin/dbus-launch\n</code></pre></li> </ol>"},{"location":"clusters-at-yale/access/ood/#jupyter-cannot-be-started-properly","title":"Jupyter cannot be started properly","text":"<ol> <li>If you are trying to launch <code>jupyter-notebook</code>, make sure it is available in your jupyter conda environment: <pre><code>(ycrc_default)[pl543@grace1 ~]$ which jupyter-notebook\n/gpfs/gibbs/project/support/pl543/conda_envs/ycrc_default/bin/jupyter-notebook\n</code></pre></li> <li>If you are trying to launch <code>jupyter-lab</code>, make sure it is available in your jupyter conda environment: <pre><code>(ycrc_default)[pl543@grace1 ~]$ which jupyter-lab\n/gpfs/gibbs/project/support/pl543/conda_envs/ycrc_default/bin/jupyter-notebook\n</code></pre></li> </ol>"},{"location":"clusters-at-yale/access/ood/#rstudio-with-conda-r","title":"RStudio with Conda R","text":"<p>If you see <code>NOT_FOUND</code> in \"Conda R Environment\", it means your Conda R environment has not been properly installed. You may need to reinstall your Conda R environment and make sure <code>r-base r-essentials</code> are both included.</p>"},{"location":"clusters-at-yale/access/ood/#rstudio-server-does-not-respond","title":"RStudio Server does not respond","text":"<p>If you encounter a grey screen after clicking the \"Connect to RStudio Server\" button, please stop the RStudio session and run <code>clean-rstudio.sh</code> at a shell command line.</p>"},{"location":"clusters-at-yale/access/ssh/","title":"SSH Connection","text":"<p>For more advanced use cases that are not well supported by the Web Portal (Open OnDemand), you can connect to the cluster over the more traditional SSH connection.</p>"},{"location":"clusters-at-yale/access/ssh/#overview","title":"Overview","text":"<ul> <li> <p>Request an account (if you do not already have one).</p> </li> <li> <p>Send us your public SSH key with our SSH key uploader. Allow up to ten minutes for it to propagate.</p> </li> <li> <p>Once we have your public key you can connect with <code>ssh netid@clustername.ycrc.yale.edu</code>.</p> </li> <li> <p>Login node addresses and other details of the clusters, such as scheduler partitions and storage, can be found on the clusters page.</p> </li> <li> <p>To use graphical programs on the clusters, please see our guides on Open OnDemand or X11 Forwarding.</p> </li> </ul> <p>If you are having trouble logging in: please read the rest of this page and our Troubleshoot Login page, then contact us if you're still having issues.</p>"},{"location":"clusters-at-yale/access/ssh/#what-are-ssh-keys","title":"What are SSH keys","text":"<p>SSH (Secure Shell) keys are a set of two pieces of information that you use to identify yourself and encrypt communication to and from a server. Usually this takes the form of two files: a public key (often saved as <code>id_rsa.pub</code>) and a private key (<code>id_rsa</code> or <code>id_rsa.ppk</code>). To use an analogy, your public key is like a lock and your private key is what unlocks it. It is ok for others to see the lock (public key), but anyone who knows the private key can open your lock (and impersonate you).</p> <p>When you connect to a remote server in order to sign in, it will present your lock. You prove your identity by unlocking it with your secret key. As you continue communicating with the remote server, the data sent to you is also locked with your public key such that only you can unlock it with your private key.</p> <p>We use an automated system to distribute your public key onto the clusters, which you can log in to here. It is only accessible on campus or through the Yale VPN. All the public keys that are authorized to your account are stored in the file <code>~/.ssh/authorized_keys</code> on the clusters you have been given access to. If you use multiple computers, you can either keep the same ssh key pair on every one or have a different set for each. Having only one is less complicated, but if your key pair is compromised you have to be worried about everywhere it is authorized.</p> <p>Warning</p> <p>Keep your private keys private! Anyone who has them can assume your identity on any server where your keys are authorized. We will never ask for your private key.</p> <p>For further reading we recommend starting with the Wikipedia articles about public-key cryptography and challenge-response authentication.</p>"},{"location":"clusters-at-yale/access/ssh/#macos-and-linux","title":"macOS and Linux","text":""},{"location":"clusters-at-yale/access/ssh/#generate-your-key-pair-on-macos-and-linux","title":"Generate Your Key Pair on macOS and Linux","text":"<p>To generate a new key pair, first open a terminal/xterm session. If you are on macOS, open Applications -&gt; Utilities -&gt; Terminal.</p> <p>Generate your public and private ssh keys. Type the following into the terminal window:</p> <pre><code>ssh-keygen\n</code></pre> <p>Your terminal should respond:</p> <pre><code>Generating public/private rsa key pair.\nEnter file in which to save the key (/home/yourusername/.ssh/id_rsa):\n</code></pre> <p>Press Enter to accept the default value. Your terminal should respond:</p> <pre><code>Enter passphrase (empty for no passphrase):\n</code></pre> <p>Choose a secure passphrase. Your passphrase will prevent access to your account in the event your private key is stolen. You will not see any characters appear on the screen as you type. The response will be:</p> <pre><code>Enter same passphrase again:\n</code></pre> <p>Enter the passphrase again. The key pair is generated and written to a directory called <code>.ssh</code> in your home directory. The public key is stored in <code>~/.ssh/id_rsa.pub</code>. If you forget your passphrase, it cannot be recovered. Instead, you will need to generate and upload a new SSH key pair.</p> <p>Next, upload your public SSH key on the cluster. Run the following command in a terminal:</p> <pre><code>cat ~/.ssh/id_rsa.pub\n</code></pre> <p>Copy and paste the output to our SSH key uploader. Note: It can take a few minutes for newly uploaded keys to sync out to the clusters so your login may not work immediately.</p>"},{"location":"clusters-at-yale/access/ssh/#connect-on-macos-and-linux","title":"Connect on macOS and Linux","text":"<p>Once your key has been copied to the appropriate places on the clusters, you can log in with the command:</p> <pre><code>ssh netid@clustername.ycrc.yale.edu\n</code></pre> <p>Check out our Advanced SSH Configuration for tips on maintaining connections and adding tab complete to your ssh commands on linux/macOS.</p>"},{"location":"clusters-at-yale/access/ssh/#windows","title":"Windows","text":"<p>We recommend using the Web Portal (Open OnDemand) to connect to the clusters from Windows. If you need advanced features beyond the web portal, we recommend using MobaXterm.</p>"},{"location":"clusters-at-yale/access/ssh/#mobaxterm","title":"MobaXterm","text":"<p>You can download, extract &amp; install MobaXterm from this page. We recommend using the \"Installer Edition\", but make sure to extract the zip file before running the installer.</p> <p>You can also use one of the Windows Subsystem for Linux (WSL) distributions and follow the Linux instructions above. However, you will probably run into issues if you try to use any graphical applications.</p>"},{"location":"clusters-at-yale/access/ssh/#generate-your-key-pair-on-windows","title":"Generate Your Key Pair on Windows","text":"<p>First, generate an SSH key pair if you haven't already:</p> <ul> <li>Open MobaXterm.</li> <li>From the top menu choose Tools -&gt; MobaKeyGen (SSH key generator).</li> <li>Leave all defaults and click the \"Generate\" button.</li> <li>Wiggle your mouse.</li> <li>Click \"Save public key\" and save your public key as id_rsa.pub.</li> <li>Choose a secure passphrase and enter into the two relevant fields. Your passphrase will prevent access to your account in the event your private key is stolen.</li> <li>Click \"Save private key\" and save your private key as id_rsa.ppk (this one is secret, don't give it to other people).</li> <li>Copy the text of your public key and paste it into the text box in our SSH key uploader.</li> <li>Your key will be synced out to the clusters in a few minutes.</li> </ul>"},{"location":"clusters-at-yale/access/ssh/#connect-with-mobaxterm","title":"Connect with MobaXterm","text":"<p>To make a new connection to one of the clusters:</p> <ul> <li>Open MobaXterm.</li> <li>From the top menu select Sessions -&gt; New Session.</li> <li>Click the SSH icon in the top left.</li> <li>Enter the cluster login node address (e.g. grace.ycrc.yale.edu) as the Remote Host.</li> <li>Check \"Specify Username\" and Enter your netID as the the username.</li> <li>Click the \"Advanced SSH Settings\" tab and check the \"Use private key box\", then click the file icon / magnifying glass to choose where you saved your private key (id_rsa.ppk).</li> <li>Click OK.</li> </ul> <p></p> <p>In the future, your session should be saved in the sessions bar on the left in the main window.</p>"},{"location":"clusters-at-yale/access/vnc/","title":"VNC","text":"<p>As an alternative to X11 Forwarding, using VNC to access the cluster is another way to run graphically intensive applications.</p>"},{"location":"clusters-at-yale/access/vnc/#open-ondemand","title":"Open OnDemand","text":"<p>On the clusters, we have web dashboards set up that can run VNC for you as a job and forward your session back to you via your browser using Open OnDemand. To use the Remote Desktop tab, browse under the \"interactive apps\" drop-down menu item.  We strongly encourage using Open OnDemand unless you have specific requirements otherwise.</p>"},{"location":"clusters-at-yale/access/vnc/#setup-vncserver-on-a-cluster","title":"Setup vncserver on a Cluster","text":"<ol> <li> <p>Connect to the cluster with X11 forwarding enabled. If on Linux or Mac, <code>ssh -Y netid@cluster</code>, or if on Windows, follow our X11 forwarding guide.</p> </li> <li> <p>Start an interactive job on cluster with the <code>--x11</code> flag (see Slurm for more information). For this description, we\u2019ll assume you were given node r801u30n01:</p> </li> </ol> <pre><code>salloc --x11\n</code></pre> <ol> <li>On that node, run the VNCserver. You\u2019ll see something like:</li> </ol> <pre><code>r801u30n01.grace$ vncserver\n\nNew 'r801u30n01.grace.ycrc.yale.edu:1 (kln26)' desktop is r801u30n01.grace.ycrc.yale.edu:1\n\nCreating default startup script /home/kln26/.vnc/xstartup\nStarting applications specified in /home/kln26/.vnc/xstartup\nLog file is /home/kln26/.vnc/r801u30n01.grace.ycrc.yale.edu:1.log\n</code></pre> <p>The :1 means that your DISPLAY is :1. You\u2019ll need that later, so note it. The first time you run \"vncserver\", you\u2019ll also be asked to select a password for allowing access.</p> <p>On MacOS, if connecting with TurboVNC throws a security exception such as \"javax.net.ssl.SSLHandshakeException\", try adding the SecurityTypes option when starting vncserver on the cluster:</p> <pre><code>vncserver -SecurityTypes VNC,OTP,UnixLogin,None\n</code></pre>"},{"location":"clusters-at-yale/access/vnc/#connect-from-your-local-machine-laptopdesktop","title":"Connect from your local machine (laptop/desktop)","text":""},{"location":"clusters-at-yale/access/vnc/#macoslinux","title":"macOs/Linux","text":"<p>From a shell on your local machine, run the following ssh command:</p> <pre><code>ssh -Y -L7777:r801u30n01:5901 YourNetID@cluster_login_node\n</code></pre> <p>This will set up a tunnel from your local port 7777 to port 5901 on r801u30n01. You will need to customize this command to your situation. The 5901 is for display :1. In general, you should put 5900+DISPLAY. The 7777 is arbitrary; any number above 3000 will likely work. You\u2019ll need the number you chose for the next step.</p> <p>On your local machine, start the vncviewer application. Depending on your local operating system, you may need to install this. We recommend TurboVNC for Mac.</p> <p>When you start the viewer, you\u2019ll need to tell it which host and port to attach to. You want to specify the local end of the tunnel. In the above case, that would be localhost::7777. Exactly how you specify this will depend a bit on which viewer you use. E.g:</p> <pre><code>vncviewer localhost::7777\n</code></pre> <p>You should be prompted for the password you set when you started the server.</p> <p>Now you are in a GUI environment and can run IGV or any other rich GUI application.</p> <pre><code>/home/bioinfo/software/IGV/IGV_2.2.0/igv.sh\n</code></pre>"},{"location":"clusters-at-yale/access/vnc/#windows","title":"Windows","text":"<p>In MobaXterm, create a new Session (available in the menu bar) and then select the VNC session.</p> <p>To fill out the VNC Session setup, click the \"Network settings\" tab and check the box for \"Connect through SSH gateway (jump host). Then fill out the boxes as follows:</p> <ul> <li>Remote hostname or IP Address: name of the node running your VNC server (e.g. r801u30n01)</li> <li>Port: 5900 + the <code>DISPLAY</code> number from above (e.g. 5901 for <code>DISPLAY = 1</code>)</li> <li>Gateway SSH server: ssh address of the cluster (e.g. grace.ycrc.yale.edu)</li> <li>Port: 22 (should be default)</li> <li>User: netid</li> <li>Use private key: check this box and click to point to your private key file you use to connect to the cluster</li> </ul> <p>When you are done, click OK. If promoted for a password for \"localhost\", provide the vncserver password you specified in the previous step.</p> <p>If the VNC server looks very pixelated and your mouse movements seem laggy, try clicking the \"Toggle scaling\" button at the top of the VNC window.</p> <p>Example Configuration:</p> <p></p>"},{"location":"clusters-at-yale/access/vnc/#clean-up","title":"Clean Up","text":"<p>When you are all finished, you can kill the vncserver by doing this in the same shell you used to start it (replace :1 by your display number):</p> <pre><code>vncserver -kill :1\n</code></pre>"},{"location":"clusters-at-yale/access/vpn/","title":"Access from Off Campus (VPN)","text":"<p>Yale's clusters can only be accessed on the Yale network. Therefore, in order to access a cluster from off campus, you will need to first connect to Yale's VPN. More information about Yale's VPN can be found on the ITS website.</p>"},{"location":"clusters-at-yale/access/vpn/#vpn-software","title":"VPN Software","text":""},{"location":"clusters-at-yale/access/vpn/#windows-and-macos","title":"Windows and macOS","text":"<p>We recommend the Cisco AnyConnect VPN Client, which can be downloaded from the ITS Software Library.</p>"},{"location":"clusters-at-yale/access/vpn/#linux","title":"Linux","text":"<p>On Linux, you can use openconnect to connect to one of Yale's VPNs. If you are using the standard Gnome-based distros, use the commands below to install.</p> <p>Ubuntu/Debian</p> <pre><code>sudo apt install network-manager-openconnect-gnome\n</code></pre> <p>Fedora/CentOS</p> <pre><code>sudo yum install NetworkManager-openconnect\n</code></pre>"},{"location":"clusters-at-yale/access/vpn/#connect-via-vpn","title":"Connect via VPN","text":"<p>You will need to connect via the VPN client using the profile \"access.yale.edu\".</p> <p></p>"},{"location":"clusters-at-yale/access/vpn/#multi-factor-authentication-mfa","title":"Multi-factor Authentication (MFA)","text":"<p>Authentication for the VPN requires multi-factor authentication via Duo in addition to your normal Yale credentials (email address  and netid password). After you select \"Connect\" in the above dialog box, it will launch a web page with a prompt to login with your email address, netid password and MFA method.</p> <p>You can click \"Other options\" to choose your authentication method. </p> <ul> <li>If you choose \"Duo Push\", simply tap \"Approve\" on your mobile device.</li> <li>If you choose \"Duo Mobile passcode\", enter the passcode from the Duo Mobile app.</li> <li>If you choose \"Phone call\", follow the prompts when you are called.</li> </ul> <p>Once you successfully authenticate with MFA, you will be connected to the VPN and should be able to log in the clusters via SSH and Open OnDemand as usual.</p> <p>More information about MFA at Yale can be found on the ITS website.</p>"},{"location":"clusters-at-yale/access/x11/","title":"Graphical Interfaces (X11)","text":"<p>To use a graphical interface on the clusters and you choose not to use the web portal, your connection needs to be set up for X11 forwarding, which will transmit the graphical window from the cluster back to your local machine.  A simple test to see if your setup is working is to run the command <code>xclock</code> .  You should see a simple analog clock window pop up.</p>"},{"location":"clusters-at-yale/access/x11/#on-macos","title":"On macOS","text":"<ol> <li>Download and install the latest X-Quartz release.</li> <li>Log out and log back in to your Mac to reset some variables</li> <li>When using ssh to log in to the clusters, use the <code>-Y</code> option to enable X11 forwarding. Example: <code>ssh -Y netid@grace.ycrc.yale.edu</code></li> </ol> <p>Note: if you get the error \"cannot open display\", please open an X-Quartz terminal and run the following command, and then log in to the cluster from the X-Quartz terminal:</p> <pre><code>launchctl load -w /Library/LaunchAgents/org.macosforge.xquartz.startx.plist\n</code></pre>"},{"location":"clusters-at-yale/access/x11/#on-windows","title":"On Windows","text":"<p>We recommend MobaXterm for connecting to the clusters from Windows. It is configured for X11 forwarding out of the box and should require no additional configuration or software.</p>"},{"location":"clusters-at-yale/access/x11/#quick-test","title":"Quick Test","text":"<p>A quick and simple test to check if X11 forwarding is working is to run the command <code>xclock</code> in the session you expect to be forwarding. After a short delay, you should see a window with a simple clock pop up.</p>"},{"location":"clusters-at-yale/access/x11/#submit-an-x11-enabled-job","title":"Submit an X11 enabled Job","text":"<p>Once configured, you'll usually want to use X11 forwarding on a compute node to do your work. To allocate a simple interactive session with X11 forwarding:</p> <pre><code>salloc --x11\n</code></pre> <p>For more Slurm options, see our Slurm documentation.</p>"},{"location":"clusters-at-yale/guides/","title":"Guides to Software &amp; Tools","text":"<p>The YCRC installs and manage commonly used software. These software are available as modules, which allow you to add or remove different combinations and versions of software to your environment as needed. See our software module guide for more information. To see all pre-installed software, you can run <code>module avail</code> on a cluster to page through all available software.</p> <p>For certain software packages, we provide guides for running on our clusters. If you have tips for running a commonly used software and would like to contribute them to our Software Guides, contact us or submit a pull request on the docs repo.</p>"},{"location":"clusters-at-yale/guides/#additional-guides","title":"Additional Guides","text":"<p>For additional guides and tutorials, see our catalog of recommended online tutorials on Python, R, unix commands and more.</p>"},{"location":"clusters-at-yale/guides/atlas/","title":"ATLAS Computing Environment","text":"<p>Software for the ATLAS experiment is available on our clusters via CVMFS. The ATLAS user interface can be set up by adding these lines to your <code>.bashrc</code> file:</p> <pre><code>export ATLAS_LOCAL_ROOT_BASE=/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase\nalias setupATLAS='source ${ATLAS_LOCAL_ROOT_BASE}/user/atlasLocalSetup.sh'\n</code></pre> <p>Then simply running <code>setupATLAS -h</code> will show all the options available:</p> <p><pre><code>[testuser@login2.grace ~]$ setupATLAS -h\n\nUsage: atlasLocalSetup.sh [options]\n       or setupATLAS [options]\n\n    This sets up the ATLAS environment for a cluster user\n\n    You need to set the environment variable ATLAS_LOCAL_ROOT_BASE first.\n\n    Options (to override defaults) are:\n     -3                           Use python3 in tools (if available)\n     -2               Use python/python2 in tools (if available)\n     -h  --help                   Print this help message\n     -q  --quiet                  Print no output\n     -p  --noLocalPostSetup       Skip running local/site post-setup script\n     -r  --relocateCvmfs          Use relocated cvmfs\n     -t  --test=STRING            Comma delimited strings for dev/test flags\n     -c  --container=name         setupATLAS in a container\n                                   Type setupATLAS -c -h for help\n\nComma delimited arguments to -t/--test option are:\n cmtSL6-dev Use dev version of cmtSL6\n devatlr    Use CERN FRONTIER servlet for developers\n postRel-dev    Uses a dev version of the post release setup for ATLAS releases\n tokens     Use tokens for validation instead of X509 (where available)\n</code></pre> We recommend referring to the ATLAS Canada TWiki for more information and detailed start-up guides. </p>"},{"location":"clusters-at-yale/guides/cesm/","title":"CESM/CAM","text":"<p>This is a quick start guide for CESM at Yale. You will still need to read the CESM User Guide and work with your fellow research group members to design and run your simulations, but this guide covers the basics that are specific to running CESM at Yale.</p>"},{"location":"clusters-at-yale/guides/cesm/#cesm-user-guides","title":"CESM User Guides","text":"<ul> <li>CESM1.0.4 User\u2019s Guide</li> <li>CESM1.1.z User\u2019s Guide</li> <li>CESM User\u2019s Guide (CESM1.2 Release Series User\u2019s Guide) (PDF)</li> </ul>"},{"location":"clusters-at-yale/guides/cesm/#modules","title":"Modules","text":"<p>CESM 1.0.4, 1.2.2, 2.x are available on Grace.</p> <p>For CESM 2.1.0, load the following modules</p> <pre><code>module load CESM/2.1.0-iomkl-2018a\n</code></pre> <p>For older versions of CESM, you will need to use the old modules. These old version of CESM do not work with the new modules</p> <pre><code>module use /vast/palmer/apps/old.grace/Modules\nmodule avail CESM\n</code></pre> <p>Once you have located your module, run</p> <pre><code>module load &lt;module-name&gt;\n</code></pre> <p>with the module name from above. </p> <p>With either module, the module will configure your environment with the Intel compiler, OpenMPI and NetCDF libraries as well as set the location of the Yale\u2019s repository of CESM input data.</p> <p>If you will be primarily using CESM, you can avoid rerunning the module load command every time you login by saving it to your default environment:</p> <pre><code>module load &lt;module-name&gt;\nmodule save\n</code></pre>"},{"location":"clusters-at-yale/guides/cesm/#input-data","title":"Input Data","text":"<p>To reduce the amount of data duplication on the cluster, we keep one centralized repository of CESM input data. The YCRC staff are only people who can add to that directory. If your build fails due to missing inputdata, contact us with your <code>create_newcase</code> line and we will download that data for you.</p>"},{"location":"clusters-at-yale/guides/cesm/#run-cesm","title":"Run CESM","text":"<p>CESM needs to be rebuilt separately for each run. As a result, running CESM is more complicated than a standard piece of software where you would just run the executable.</p>"},{"location":"clusters-at-yale/guides/cesm/#create-your-case","title":"Create Your Case","text":"<p>Each simulation is called a \u201ccase\u201d. Loading a CESM module will put the create_newcase script in your path, so you can call it as follows. This will create a directory with your case name, that we will refer to as <code>$CASE</code>  through out the rest of the guide.</p> <pre><code>create_newcase -case $CASE -compset=&lt;compset&gt; -res=&lt;resolution&gt; -mach=&lt;machine&gt;\ncd $CASE\n</code></pre> <p>The mach parameters for Grace is <code>yalegrace</code> for CESM 1.0.4 and <code>gracempi</code> for CESM 1.2.2 and CESM 2.x , respectively. For example</p> <pre><code>create_newcase --case $CASE --compset=B1850 --res=f09_g17 --mach=gracempi\n\ncd $CASE\n</code></pre>"},{"location":"clusters-at-yale/guides/cesm/#setup-your-case","title":"Setup Your Case","text":"<p>If you are making any changes to the namelist files (such as increasing the duration of the simulation), do those before running the setup scripts below.</p>"},{"location":"clusters-at-yale/guides/cesm/#cesm-10x","title":"CESM 1.0.X","text":"<pre><code>./configure -case\n</code></pre>"},{"location":"clusters-at-yale/guides/cesm/#cesm-11x-and-cesm-12x","title":"CESM 1.1.X and CESM 1.2.X","text":"<pre><code>./cesm_setup\n</code></pre>"},{"location":"clusters-at-yale/guides/cesm/#cesm-2x","title":"CESM 2.X","text":"<pre><code>./case.setup\n</code></pre>"},{"location":"clusters-at-yale/guides/cesm/#build-your-case","title":"Build Your Case","text":"<p>After you run the setup script, there will be a set of the scripts in your case directory that start with your case name. To compile your simulation executable, first move to an interactive job and then run the build script corresponding to your case.</p> <pre><code># CESM 1.x\nsalloc -c 4\nmodule load &lt;module-name&gt; # &lt;module-name&gt; = the appropriate module for your CESM version\n./$CASE.$mach.build\n</code></pre> <pre><code># CESM 2.x\nsalloc -c 4\nmodule load &lt;module-name&gt; # &lt;module-name&gt; = the appropriate module for your CESM version\n./case.build --skip-provenance-check\n</code></pre> <p>Note the <code>--skip-provenance-check</code> flag is required with CESM 2.x due to the changes made to port the code to Grace.</p> <p>For more details on interactive jobs, see our Slurm documentation.</p> <p>During the build, CESM will create a corresponding directory in your scratch60 or project directory at</p> <pre><code>ls ~/scratch60/CESM/$CASE\n</code></pre> <p>This directory will contain all the outputs from your simulation as well as logs and the cesm.exe executable.</p>"},{"location":"clusters-at-yale/guides/cesm/#common-build-issues","title":"Common Build Issues","text":"<p>Make sure you compile on an interactive node as described above. If you build fails, it will direct you to look in a bldlog file. If that log complains that it can\u2019t find mpirun, NetCDF or another library or executable, make sure you have the correct CESM module loaded. It can helpful to run <code>module purge</code> before the <code>module load</code> to ensure a reproducible environment.</p> <p>If you get an error saying <code>ERROR: Error gathering provenance information from manage_externals</code>, rerun the build using the suggested flag, e.g. <code>./case.build --skip-provenance-check</code>.  </p>"},{"location":"clusters-at-yale/guides/cesm/#submit-your-case","title":"Submit Your Case","text":"<p>Once the build is complete, which can take 5-15 minutes, you can submit your case with the submit script.</p> <pre><code># CESM 1.x\n./$CASE.$mach.submit\n</code></pre> <pre><code># CESM 2.x\n./case.submit\n</code></pre> <p>For more details on monitoring your submitted jobs, see our Slurm documentation.</p>"},{"location":"clusters-at-yale/guides/cesm/#changing-slurm-partition","title":"Changing Slurm Partition","text":"<p>In CESM 2.x, to change the partition in which your main jobs will run, use the following command:</p> <pre><code>./xmlchange JOB_QUEUE=scavenge --subgroup case.run\n</code></pre> <p>The associated archive job will still be submitted to the day partition.</p>"},{"location":"clusters-at-yale/guides/cesm/#troubleshoot-your-run","title":"Troubleshoot Your Run","text":"<p>If your run doesn\u2019t complete, there are a few places to look to identify the error. CESM writes to multiple log files for the different components and you will likely have to look in a few to find the root cause of your error.</p>"},{"location":"clusters-at-yale/guides/cesm/#slurm-log","title":"Slurm Log","text":"<p>In your case directory, there will be a file that looks like <code>slurm-&lt;job_id&gt;.log</code>. Check that file first to make sure the job started up properly. If the last few lines in the file redirect you to look at <code>cpl.log.&lt;some_number&gt;</code> file in your scratch directory, see below. If there is another error, try to address it and resubmit.</p>"},{"location":"clusters-at-yale/guides/cesm/#cesm-run-logs","title":"CESM Run Logs","text":"<p>If the last few lines of the slurm log direct you to look at <code>cpl.log.&lt;some_number&gt;</code> file, change directory to your case \u201crun\u201d directory (usually in your project directory):</p> <pre><code>cd ~/project/CESM/$CASE/run\n</code></pre> <p>The pointer to the cpl file is often misleading as I have found the error is usually located in one of the other logs. Instead look in the <code>cesm.log.xxxxxx</code> file. Towards the end there may be an error or it may signify which component was running. Then look in the log corresponding to that component to track down the issue.</p> <p>One shortcut to finding the relevant logs is to sort the log files by the time to see which ones were last updated:</p> <pre><code>ls -ltr *log*\n</code></pre> <p>Look at the end of the last couple logs listed and look for an indication of the error.</p>"},{"location":"clusters-at-yale/guides/cesm/#resolve-errors","title":"Resolve Errors","text":"<p>Once you have identified the lines in the logs corresponding to your error:</p> <p>If your log says something like <code>Disk quota exceeded</code>, your group is out of space in the fileset you are writing to. You can run the <code>getquota</code> script to get details on your disk usage. Your group will need to reduce their usage before you will be able to run successfully.</p> <p>If it looks like a model error and you don\u2019t know how to fix it, we strongly recommend Googling your error and/or looking in the CESM forums.</p> <p>If you are still experiencing issues, contact us.</p>"},{"location":"clusters-at-yale/guides/cesm/#alternative-submission-parameters","title":"Alternative Submission Parameters","text":"<p>By default, the submission script will submit to the \"mpi\" partition for 1 day. </p>"},{"location":"clusters-at-yale/guides/cesm/#cesm-1x","title":"CESM 1.x","text":"<p>To change this in CESM 1.x, edit your case\u2019s <code>run</code> script and change the partition and time. The maximum walltime in the mpi and scavenge partitions is 24 hours. For example:</p> <pre><code>## scavenge partition\n#SBATCH --partition=scavenge\n#SBATCH --time=1-\n</code></pre>"},{"location":"clusters-at-yale/guides/cesm/#cesm-2x_1","title":"CESM 2.x","text":"<p>To change this in CESM 2.x, use <code>./xmlchange</code> in your run directory.</p> <pre><code># Change partition to scavenge\n./xmlchange JOB_QUEUE=scavenge\n# Change walltime limit to 2 days (&gt; 24 hours is only available on PI partitions)\n./xmlchange JOB_WALLCLOCK_TIME 2-00:00:00\n</code></pre>"},{"location":"clusters-at-yale/guides/cesm/#further-reading","title":"Further Reading","text":"<ul> <li>We recommend referencing the User Guides listed at the top of this page.</li> <li>CESM User Forum</li> <li>Our Slurm Documentation</li> <li>CESM is a very widely used package, you can often find answers by simply using Google. Just make sure that the solutions you find correspond to the approximate version of CESM you are using. CESM changes in subtle but significant ways between versions.</li> </ul>"},{"location":"clusters-at-yale/guides/checkpointing/","title":"Checkpoint Long-running Jobs","text":"<p>When working with long-running jobs and work-flows, it becomes very important to establish checkpoints along the way. This will ensure that if your job is interrupted you will be able to restart it without having to go back to the begining of the job.</p> <p>DMTCP \"Distributed Multithreaded Checkpointing\" allows you to easily save the state of your running job and restart it from  that point.  This can be very useful if your job fails for any number of reasons: it exceeds the time limit, is preempted from scavenge, the compute node crashes, etc.</p> <p>DMTCP does not require any changes to your code or recompilation.  It should work on most sequential or multithreaded/multiprocessing programs as is.</p> <pre><code>module load DMTCP\n</code></pre>"},{"location":"clusters-at-yale/guides/checkpointing/#run-your-job-interactively-under-dmtcp","title":"Run Your Job Interactively Under DMTCP","text":"<p>For this simple example, we'll use this python script <code>count.py</code> <pre><code>import time\ni=0\nwhile True:\n  print(i,flush=True)\n  i+=1\n  time.sleep(1)\n</code></pre></p> <p>Run the script interactively using <code>dmtcp_launch</code>:</p> <pre><code>dmtcp_launch -i 5 python3 count.py \n</code></pre> <p>It will begin printing to the terminal.  In the background, DMTCP will be writing a checkpoint file every 5 seconds. Let it count for a while, then kill it with Ctrl+c.</p> <p>If you look in that directory, you'll see several files related to DMTCP. The *.dmtcp file is the checkpoint file. To restart the job from the last checkpoint, do:</p> <pre><code>dmtcp_restart -i 5 *.dmtcp \n</code></pre> <p>In practice, you'll most likely want to use DMTCP to checkpoint batch jobs, rather than interactive sessions.     </p>"},{"location":"clusters-at-yale/guides/checkpointing/#checkpoint-a-batch-job","title":"Checkpoint a Batch Job","text":"<p>This script will submit the job under DMTCP's checkpointing.  Here we use a more reasonable checkpoint interval of 300 seconds.  You will want to experiment to see how long it takes to write your application's checkpoint file, and tune your interval accordingly.  </p> <pre><code>#!/bin/bash\n\nmodule load DMTCP\n\ndmtcp_launch -i 300 python count.py\n</code></pre> <p>Then, if the job fails, you can resubmit it with this script: <pre><code>#!/bin/bash\n\nmodule load DMTCP\n\ndmtcp_restart -i 300 *.dmtcp\n</code></pre></p> <p>Note</p> <p>We are using wildcards to name the DMTCP file, which will obviously only work correctly if there is only one checkpoint file in the directory.  Alternatively you can edit the script each time and explicitly name the correct checkpoint file.</p>"},{"location":"clusters-at-yale/guides/checkpointing/#restart-a-job-that-timed-out-or-was-preempted","title":"Restart a job that timed out or was preempted","text":"<p>Note</p> <p>Timeouts and preemptions are subtlely different.  Slurm will automatically requeue a job that has been declared requeue-able (--requeue) and was preempted.  It will NOT automatically requeue a timed out job.  Jobs that time out require some additional signal handling.  The script requests signal 10 be sent to the script just before the job times out, and traps that signal and requests a requeue.  It is important to run the actual job in the background using &amp; and wait.</p> <p>Here is an example job script that will start a job running, periodically checkpoint it, and automatically requeue the job if it is preempted or times out:</p> <pre><code>#!/bin/bash\n\n#SBATCH -t 30:00\n#SBATCH --requeue\n#SBATCH -p scavenge\n#SBATCH --signal=B:10@30 # send the signal `10` at 30s before job times out\n#SBATCH --open-mode=append\n\ntrap \"echo -n 'TIMEOUT @ '; date; echo 'Resubmitting...'; scontrol requeue ${SLURM_JOBID}  \" 10\n\n#edit following line to put the appropriate module\nmodule load DMTCP\n\ncnt=${SLURM_RESTART_COUNT:-0}\necho \"SLURM_RESTART_COUNT = $cnt\"\n\ndmtcp_coordinator -i 5 --daemon --port 0 --port-file /tmp/port\nexport DMTCP_COORD_PORT=$(&lt;/tmp/port)\n\nif [[ $cnt == 0 ]]\nthen\n    echo \"doing launch\"\n    rm -f *.dmtcp &amp;\n    dmtcp_launch -j python count.py \nelif [[ $cnt &gt; 0 ]]; then\n    echo \"doing restart\"\n    dmtcp_restart -j *.dmtcp &amp;\nelse\n    echo \"Failed to restart the job, exit\"; exit\nfi\nwait\n</code></pre> <p>Launch the job with sbatch, and watch the numbers appear in the slurm*.out file. Then, simulate preemption by doing:</p> <pre><code>$ scontrol requeue 123456789\n</code></pre> <p>Because the script specified --requeue, the job will be returned to pending.  Slurm automatically sets a \"Begin Time\" a couple of minutes in the future, so the job will pend until then, at which point it will begin running again, so be patient.  This time the script will invoke dmtcp_restart, and will continue from the checkpoint.  If you look at the output, you'll see from the numbers that the job backed up to the previous checkpoint and restarted. You can requeue the job several times, and each time it will restart from the last checkpoint.</p> <p>You should be able to adapt this script to your own job by loading any required modules and  replacing \"python count.py\" with your program's invocation.</p> <p>This example is much more complicated than our previous examples. Some notes:</p> <ul> <li> <p>DMTCP uses a \"controller\" to manage the checkpointing.  In the simple example, dmtcp_launch transparently started a controller on the default port 7779.  In this case, we explicitly start a \"controller\" on a random port and communicate the port number via an environment variable. This prevents collisions if multiple DMTCP sessions run on the same node.</p> </li> <li> <p>The -j flag to dmtcp_launch tells it to join the existing controller.</p> </li> <li> <p>On initial launch we remove existing checkpoint files.  This may not be a good idea in practice. </p> </li> <li> <p>The env var SLURM_RESTART_COUNT is used to determine if this is a restart or not.</p> </li> </ul>"},{"location":"clusters-at-yale/guides/checkpointing/#parallel-execution-with-dmtcp","title":"Parallel Execution with DMTCP","text":"<p>DMTCP can checkpoint multithreaded/multiprocess parallel applications. In this example we run NAMD (a molecular dynamics simulation), using 6 threads on 6 cpus.  We also restart automatically on preemption, as above.</p> <pre><code>#!/bin/bash\n\n#SBATCH -c 6 \n#SBATCH -t 30:00\n#SBATCH --requeue\n#SBATCH -p scavenge\n#SBATCH --signal=B:10@30 # send the signal `10` at 30s before job times out\n#SBATCH --open-mode=append\n\ntrap \"echo -n 'TIMEOUT @ '; date; echo 'Resubmitting...'; scontrol requeue ${SLURM_JOBID}  \" 10\n\n#edit following line to put the appropriate module\nmodule load NAMD/2.12-multicore\nmodule load DMTCP\n\ncnt=${SLURM_RESTART_COUNT:-0}\necho \"SLURM_RESTARTCOUNT = $cnt\"\n\ndmtcp_coordinator -i 90 --daemon --port 0 --port-file /tmp/port\nexport DMTCP_COORD_HOST=`hostname`\nexport DMTCP_COORD_PORT=$(&lt;/tmp/port)\n\nif [[ $cnt == 0 ]]\nthen\n    echo \"doing launch\"\n    dmtcp_launch namd2 +ppn $SLURM_CPUS_ON_NODE stmv.namd &amp;\nelif [[ $cnt &gt; 0 ]]; then\n    echo \"doing restart\"\n    dmtcp_restart *.dmtcp &amp;\nelse\n    echo \"Failed to restart the job, exit\"; exit\nfi\nwait\n</code></pre>"},{"location":"clusters-at-yale/guides/checkpointing/#additional-notes","title":"Additional notes","text":"<ul> <li>dmtcp reopens files when recovering from checkpoints, so most file writes should just work.  However, when requeuing jobs as shown above, you should take care to do <code>#SBATCH --open-mode=append</code></li> <li>keep in mind that recovery from checkpoints does imply backing up to the point of the previous checkpoint.  If your program is continuously  writing output, the output since the last checkpoint will be replicated.  For many programs (like NAMD) the output is really just logging, so this is not a problem.</li> <li> <p>by default, dmtcp compresses checkpoint files.  For large files this can take a long time.  You can turn off compression with <code>dmtcp_launch --no-gzip</code>.</p> </li> <li> <p>dmtcp creates a convenience restart script called restart_dmtcp_script.sh with every checkpoint.  In theory you can simply call it to restart: <code>./restart_dmtcp_script.sh</code> rather than  <code>restart_dmtcp *.dmtcp</code> However, we have found it to be unreliable.  Your mileage may vary.</p> </li> </ul> <p>The above examples just scratch the surface.  For more information:</p> <p>A DMTCP quickstart and  documentation</p> <p>A very helpful page at NERSC</p>"},{"location":"clusters-at-yale/guides/clustershell/","title":"ClusterShell","text":"<p>ClusterShell is a useful Python package for executing arbitrary commands across multiple hosts. On the Yale clusters it provides a relatively simple way for you to run commands on nodes your jobs are running on, and collect the results. The two most useful commands provided are <code>nodeset</code>, which can show and manipulate node lists and <code>clush</code>, which can run commands on multiple nodes at once.</p>"},{"location":"clusters-at-yale/guides/clustershell/#configuration","title":"Configuration","text":"<p>To set up ClusterShell, make sure you have a <code>.config</code> directory and a copy our <code>groups.conf</code> file there. For more info about ClusterShell configuration for Slurm, see the official docs.</p> <pre><code>mkdir -p ~/.config/clustershell\nwget https://docs.ycrc.yale.edu/_static/files/clustershell_groups.conf -O ~/.config/clustershell/groups.conf \n</code></pre> <p>We provide ClusterShell as a module, but you can also install it with <code>conda</code>.</p>"},{"location":"clusters-at-yale/guides/clustershell/#module","title":"Module","text":"<pre><code>module load ClusterShell\n</code></pre>"},{"location":"clusters-at-yale/guides/clustershell/#conda","title":"Conda","text":"<pre><code>module load miniconda\nconda create -yn clustershell python pip\nsource activate clustershell\npip install ClusterShell\n</code></pre>"},{"location":"clusters-at-yale/guides/clustershell/#examples","title":"Examples","text":""},{"location":"clusters-at-yale/guides/clustershell/#nodeset","title":"<code>nodeset</code>","text":"<p>The <code>nodeset</code> command uses <code>sinfo</code> underneath but has slightly different syntax. You can use it to ask about node states and nodes your job is running on. The nice difference is you can ask for folded (e.g. <code>c[01-02]n[12,15,18]</code>) or expanded (e.g. <code>c01n01 c01n02 ...</code>) node lists. The groups useful to you that we have configured are <code>@user</code>, <code>@job</code> and <code>@state</code>.</p>"},{"location":"clusters-at-yale/guides/clustershell/#user-group","title":"User group","text":"<p>List expanded node names where user abc123 has jobs running</p> <pre><code># similar to squeue -h -u abc123 -o \"%N\"\nnodeset -e @user:abc123\n</code></pre>"},{"location":"clusters-at-yale/guides/clustershell/#job-group","title":"Job group","text":"<p>List folded nodes where job 1234567 is running</p> <pre><code># similar to squeue -h -j 1234567 -o \"%N\"\nnodeset -f @job:1234567\n</code></pre>"},{"location":"clusters-at-yale/guides/clustershell/#state-group","title":"State group","text":"<p>List expanded node names that are idle according to slurm</p> <pre><code># similar to sinfo -t IDLE -o \"%N\"\nnodeset -e @state:idle\n</code></pre>"},{"location":"clusters-at-yale/guides/clustershell/#clush","title":"<code>clush</code>","text":"<p>The <code>clush</code> command uses the node grouping syntax from nodeset to allow you to run commands on those nodes. <code>clush</code> uses ssh to connect to each of these nodes. You can use the <code>-b</code> option to gather output from nodes with same output into the same lines. Leaving this out will report on each node separately.</p> <p>Info</p> <p>You can only ssh to, and therefore run <code>clush</code> on, nodes where you have active jobs.</p>"},{"location":"clusters-at-yale/guides/clustershell/#local-storage","title":"Local storage","text":"<p>Get a list of files in /tmp/abs on all nodes where job <code>654321</code> is running.</p> <pre><code>clush -bw @job:654321 ls /tmp/abc123\n\n# don't gather identical output\nclush -w @job:654321 ls /tmp/abc123\n</code></pre>"},{"location":"clusters-at-yale/guides/clustershell/#cpu-usage","title":"CPU usage","text":"<p>Show %cpu, memory usage, and command for all nodes running any jobs owned by user <code>abc123</code>.</p> <pre><code>clush -bw @user:abc123 ps -uabc123 -o%cpu,rss,cmd\n</code></pre>"},{"location":"clusters-at-yale/guides/clustershell/#gpu-usage","title":"GPU usage","text":"<p>Show what's running on all the GPUs on the nodes associated with your job <code>654321</code>.</p> <pre><code>clush -bw @job:654321 nvidia-smi --format=csv --query-compute-apps=process_name,used_gpu_memory\n</code></pre>"},{"location":"clusters-at-yale/guides/cmd-line-args/","title":"Pass Values into Jobs","text":"<p>A useful tool when running jobs on the clusters is to be able to pass variables into a script without modifying any code. This can include specifying the name of a data file to be processed, or setting a variable to a specific value. Generally, there are two ways of achieving this: environment variables and command-line arguments. Here we will work through how to implement these two approaches in both Python and R.</p>"},{"location":"clusters-at-yale/guides/cmd-line-args/#python","title":"Python","text":""},{"location":"clusters-at-yale/guides/cmd-line-args/#environment-variables","title":"Environment Variables","text":"<p>In python, environment variables are accessed via the <code>os</code> package (docs page). In particular, we can use <code>os.getenv</code> to retrieve environment variables set prior to launching the python script.</p> <p>For example, consider a python script designed to process a data file:</p> <pre><code>def file_cruncher(file_name):\n\n    f = open(file_name)\n    data = f.read()\n    output = process(data)\n    # processing code goes here\n    return output\n</code></pre> <p>We can use an environment variable (<code>INPUT_DATA_FILE</code>) to provide the filename of the data to be processed. The python script (<code>my_script.py</code>) is modified to retrieve this variable and analyze the given datafile:</p> <pre><code>import os\n\nfile_name = os.getenv(\"INPUT_DATA_FILE\")\n\ndef file_cruncher(file_name):\n\n    f = open(file_name)\n    data = f.read()\n    output = process(data)\n    # processing code goes here\n    return output\n</code></pre> <p>To process this data file, you would simply run:</p> <pre><code>export INPUT_DATA_FILE=/path/to/file/input_0.dat\npython my_script.py\n</code></pre> <p>This avoids having to modify the python script to change which datafile is processed, we only need to change the environment variable.</p>"},{"location":"clusters-at-yale/guides/cmd-line-args/#command-line-arguments","title":"Command-line Arguments","text":"<p>Similarly, one can use command-line arguments to pass values into a script. In python, there are two main packages designed for handling arguments. First is the simple <code>sys.argv</code> function which parses command-line arguments into a list of strings:</p> <pre><code>import sys\n\nfor a in sys.argv:\n    print(a)\n</code></pre> <p>Running this with a few arguments:</p> <p><pre><code>$ python my_script.py a b c\nmy_script.py\na\nb\nc\n</code></pre> The first element in <code>sys.argv</code> is the name of the script, and then all subsequent arguments follow.</p> <p>Secondly, there is the more fully-featured <code>argparse</code> package (docs page)which offers many advanced tools to manage command-line arguments. Take a look at their documentation for examples of how to use <code>argparse</code>.</p>"},{"location":"clusters-at-yale/guides/cmd-line-args/#r","title":"R","text":"<p>Just as with Python, R provides comparable utilities to access command-line arguments and environment variables.</p>"},{"location":"clusters-at-yale/guides/cmd-line-args/#environment-variables_1","title":"Environment Variables","text":"<p>The <code>Sys.getenv</code> utility (docs page) works nearly identically to the Python implementation.</p> <pre><code>&gt; Sys.getenv('HOSTNAME')\n[1] \"grace2.grace.hpc.yale.internal\"\n</code></pre> <p>Just like Python, these values are always returned as <code>string</code> representations, so if the variable of interest is a number it will need to be cast into an integer using <code>as.numeric()</code>.</p>"},{"location":"clusters-at-yale/guides/cmd-line-args/#command-line-arguments_1","title":"Command-line Arguments","text":"<p>To collect command-line arguments in R use the <code>commandArgs</code> function:</p> <pre><code>args = commandArgs(trailingOnly=TRUE)\n\nfor (x in args){\n   print(x)\n}\n</code></pre> <p>The <code>trailingOnly=TRUE</code> option will limit <code>args</code> to contain only those arguments which follow the script:</p> <pre><code>Rscript my_script.R a b c\n[1] \"a\"\n[1] \"b\"\n[1] \"c\"\n</code></pre> <p>There is a more advanced and detailed package for managing command-line arguments called <code>optparse</code> (docs page). This can be used to create more featured scripts in a similar way to Python's <code>argparse</code>.</p>"},{"location":"clusters-at-yale/guides/cmd-line-args/#slurm-environment-variables","title":"Slurm Environment Variables","text":"<p>Slurm sets a number of environment variables detailing the layout of every job. These include:</p> <ul> <li><code>SLURM_JOB_ID</code>: the unique jobid given to each job. Useful to set unique output directories</li> <li><code>SLURM_CPUS_PER_TASK</code>: the number of CPUs allocated for each task. Useful as a replacement for R's <code>detectCores</code> or Python's <code>multiprocessing.cpu_count</code> which report the physical number of CPUs and not the number allocated by Slurm.</li> <li><code>SLURM_ARRAY_TASK_ID</code>: the unique array index for each element of a job array. Useful to un-roll a loop or to set a unique random seed for parallel simulations.</li> </ul> <p>These can be leveraged within batch scripts using the above techniques to either pass on the command-line or directly reading the environment variable to control how a script runs.</p> <p>For example, if a script previously looped over values ranging from 0-9, we can modify the script and create a job array which runs each iteration separately in parallel using <code>SLURM_ARRAY_TASK_ID</code> to tell each element of the job array which value to use.</p>"},{"location":"clusters-at-yale/guides/comsol/","title":"COMSOL","text":"<p>YCRC has COMSOL Multiphysics 5.2a available on Grace. It can be used to run basic physical and multiphysics models on one node utilizing multiple cores. If you need to run run models across multiple nodes or need to run COMSOL on your local machine, please contact us.</p>"},{"location":"clusters-at-yale/guides/comsol/#use-comsol","title":"Use COMSOL","text":"<p>To use COMSOL on the cluster, load the COMSOL module by running <code>module load COMSOL/5.2a-classkit</code>. For more information on our modules, please see our software modules documentation. </p> <p>COMSOL has a resource intenstive GUI and, therefore, we strongly recommend using COMSOL in a Remote Desktop session on the Open OnDemand web portal.</p> <p>To launch COMSOL in your Remote Desktop, open the terminal application in the session and enter the following commands:</p> <pre><code>module load COMSOL/5.2a-classkit\ncomsol -np $SLURM_CPUS_ON_NODE &amp;\n</code></pre>"},{"location":"clusters-at-yale/guides/comsol/#run-comsol-in-batch-mode","title":"Run COMSOL in Batch Mode","text":"<p>Comsol can be run without the graphical interface assuming you have a model file and a study defined beforehand.  This is particularly useful for parametric sweeps or scanning over a range of values for specific parameters. For example:</p> <pre><code>comsol batch -configuration /tmp -data /tmp -prefsdir /tmp -inputfile mymodel.mph -outputfile out.mph -study std1 \n</code></pre> <p>which will run the study <code>std1</code> found within the <code>mymodel.mph</code> file generated through the COMSOL GUI and save the outputs in  <code>out.mph</code>.  A parameter can be passed into the study like this:</p> <p><pre><code>comsol batch -inputfile  mymodel.mph -outputfile out.mph -pname L -plist 8[cm],10[cm],12[cm]\n</code></pre> Which will run three versions of the model sequentially for each of the three values of <code>L</code> enumerated. When combined with Slurm Job Arrays many COMSOL jobs can be run in parallel. An example <code>dSQ</code> job-file would look like:</p> <p><pre><code>module load COMSOL; comsol batch -inputfile  mymodel.mph -outputfile out_8.mph -pname L -plist 8[cm]\nmodule load COMSOL; comsol batch -inputfile  mymodel.mph -outputfile out_10.mph -pname L -plist 10[cm]\nmodule load COMSOL; comsol batch -inputfile  mymodel.mph -outputfile out_12.mph -pname L -plist 12[cm]\n</code></pre> Which would run three versions of the study using different values of <code>L</code> and save their outputs in separate files. Be careful to provide a different output file for each line to avoid clashes between the separate jobs.</p> <p>More details can be found on the COMSOL documentation site.</p>"},{"location":"clusters-at-yale/guides/comsol/#details-of-comsol-on-ycrc-clusters","title":"Details of COMSOL on YCRC Clusters","text":"<p>Two COMSOL modules (Heat Transfer and Structural Mechanics) are included in addition to the main multiphysics engine.</p> <p>The following models might be solved using our COMSOL package both in stationary and time dependent studies.</p> <ol> <li> <p>AC/DC. Electric Currents and Electrostatics in 1D, 2D, 3D models. Magnetic Fields in 2D.</p> </li> <li> <p>Acoustics. Pressure acoustics in frequency domain in 1D, 2D, 3D models.</p> </li> <li> <p>Chemical Species Transport. Transport of Diluted Species in 1D, 2D, 3D models. Transport and reactions of the species dissolved in a gas, liquid, or solid can be handled with this interface. The driving forces for transport can be diffusion, convection when coupled to a flow field, and migration, when coupled to an electric field. Moisture Transport in 1D, 2D, 3D is used to model moisture transfer in a porous medium.</p> </li> <li> <p>Fluid Flow. Single Phase Laminar and Turbulent Flow including non-isothermal flow in 2D, 3D models. Fluid-Structure Interaction in 2D, 3D models for both fixed geometry and deformed solid.</p> </li> <li> <p>Heat Transfer in 1D, 2D, 3D models. HT in Solids and Fluids. HT in porous media including non-equilibrium transfer. Bioheat transfer. Surface to Surface Radiation. Joule Heating. HT in thin structures (2D, 3D) like shells, films, fractures. Conjugate HT from laminar and turbulent flows (2D, 3D). Heat and moisture transport. Thermoelastic effect.</p> </li> <li> <p>Plasma in 1D. Equilibrium DC Discharges that are sustained by a static or slow-varying electric field where induction currents and fluid flow effects are negligible.</p> </li> <li> <p>Structural Mechanics in 2D, 3D models. Solid Mechanics (elastic). Plate Truss in 2D. Beam, Truss (2D, 3D). Membrane (2D axisymmetric, 3D). Shell (3D). Thermal stress. Thermal expansion. Piezoelectricity.</p> </li> <li> <p>General Mathematics equations in 1D, 2D, 3D models. Classic PDE. Coefficient based and general form PDE. Wave form PDE. Weak form PDE. Ordinary differential equations and algebraic equations. Deformed geometry and moving mesh. Curvilinear coordinates. </p> </li> </ol> <p>All above models can be used in the Multiphysics approach of coupling them together. They can be solved in Full Couple mode or by using Segregated Solver (solving one physical model and using resulting field to model another, and so on).</p>"},{"location":"clusters-at-yale/guides/comsol/#backward-compatibility","title":"Backward Compatibility","text":"<p>COMSOL is not backwards compatible. If you have a project file from a newer version of COMSOL (e.g. 5.3), it will not open in 5.2a. However, in some circumstances, we can assist with porting the project file back to version 5.2a. If you have any questions about this, please contact us.</p>"},{"location":"clusters-at-yale/guides/comsol/#limitations-of-available-license","title":"Limitations of Available License","text":"<p>Please note that some commonly used COMSOL features such as CAD Import Module, Material Library, and MatLab Link are not included in the license. </p> <p>COMSOL Material Library consists of about 2500 different materials with their physical properties. Many of them are included with temperature dependancies. Without this library you have to specify material parameters manually, however, you can save your new material for future use.  We can help in adding material form COMSOL library to your project file using a different license.</p> <p>You cannot import geometry designed by external CAD program like SolidWorks, Autocad, etc. Instead you have to design it inside COMSOL. However, we can help you to perform such import utilizing different license; we\u2019ll save it in COMSOL project file and you would be able to open it with already imported geometry.</p> <p>More advanced users often use MatLab for automation of COMSOL models and extracting results data for mining them by external methods available in MatLab. Unfortunately, you cannot do this with the license available on the cluster. Please contact us if you feel you need to utilize MatLab.</p> <p>Lastly, our license does not allow to use COMSOL for solving models based on Maxwell Equations (RF, Wave Optics), semiconductor models, particle tracing, ray optics, non-linear mechanics, and some other more advanced modules. To approach such models in COMSOL on your local computer, please contact us to use our more general license with very limited number of licensed seats.</p>"},{"location":"clusters-at-yale/guides/conda/","title":"Conda","text":"<p>Conda is a package, dependency, and environment manager. It allows you to maintain different, often incompatible, sets of applications side-by-side. It has become a popular choice for managing pipelines that involve several tools, especially when multiple languages are involved. These sets of applications and their dependencies are kept in Conda environments, which you can switch between as your work dictates. Compared to the modules that we provide, there are often newer and more varied packages available that you can manage yourself, but they may not be as well optimized for the clusters. See Conda's official command-line reference and the offical docs for managing environments for detailed instructions. Here we present essential instructions and site-specific info.</p> <p>Warning</p> <p>Mixing modules and conda-managed software is almost never a good idea. When constructing an environment for your work you should load either modules or a conda environment. If you get stuck, you can always ask us for help.</p>"},{"location":"clusters-at-yale/guides/conda/#the-miniconda-module","title":"The Miniconda Module","text":"<p>For your convenience, we provide a relatively recent version of Miniconda as a module. This is a read-only environment from which you can create your own. We set some defaults for you in this module, and we keep it relatively up-to-date so you don't have to. If you are using Conda-installed packages, this should be the only module you load in your jobs.</p> <p>Note: If you are on Milgram and run out of space in your home directory for Conda, you can either reinstall your environment in your project space (see below) or contact us for help with your home quota.</p>"},{"location":"clusters-at-yale/guides/conda/#defaults-we-set","title":"Defaults We Set","text":"<p>On all clusters, we set the <code>CONDA_ENVS_PATH</code> and <code>CONDA_PKGS_DIRS</code> environment variables to <code>conda_envs</code> and <code>conda_pkgs</code> in your project directory where there is more quota available. Conda will install to and search in these directories for environments and cached packages.</p> <p>Starting with minconda module version 4.8.3 we set the default channels (the sources to find packages) to <code>conda-forge</code> and <code>bioconda</code>, which provide a wider array of packages than the default channels do. We have found it saves a lot of typing. If you would like to override these defaults, see the Conda docs on managing channels. Below is the <code>.condarc</code> for the miniconda module.</p> <pre><code>env_prompt: '({name})'\nauto_activate_base: false\nchannels:\n  - conda-forge\n  - bioconda\n  - defaults\n</code></pre>"},{"location":"clusters-at-yale/guides/conda/#setup-your-environment","title":"Setup Your Environment","text":""},{"location":"clusters-at-yale/guides/conda/#load-the-miniconda-module","title":"Load the <code>miniconda</code> Module","text":"<pre><code>module load miniconda\n</code></pre> <p>You can save this to your default module collection by using <code>module save</code>. See our module documentation for more details.</p>"},{"location":"clusters-at-yale/guides/conda/#create-a-conda-environment","title":"Create a <code>conda</code> Environment","text":"<p>To create an environment use the <code>conda create</code> command. Environment files are saved to the first path in <code>$CONDA_ENVS_PATH</code>, or where you specify with the <code>--prefix</code> option. You should give your environments names that are meaningful to you, so you can more easily keep track of their purposes.</p> <p>Because dependency resolution is hard and messy, we find specifying as many packages as possible at environment creation time can help minimize broken dependencies. Although sometimes unavoidable for Python, we recommend against heavily mixing the use of <code>conda</code> and <code>pip</code> to install applications. If needed, try to get as much installed with <code>conda</code>, then use <code>pip</code> to get the rest of the way to your desired environment.</p> <p>Tip</p> <p>For added reproducibility and control, specify versions of packages to be installed using <code>conda</code> with <code>packagename=version</code> syntax. E.g. <code>numpy=1.14</code></p> <p>Warning</p> <p>You will need to request an interactive job with the <code>salloc</code> command when you create a new <code>conda</code> environment or install packages into an existing <code>conda</code> environment.   </p> <p>For example, if you have a legacy application that needs Python 2 and OpenBLAS:</p> <pre><code>module load miniconda\nconda create -n legacy_application python=2.7 openblas\n</code></pre> <p>If you want a good starting point for interactive data science in R/Python Jupyter Notebooks:</p> <pre><code>module load miniconda\nconda create -n ds_notebook python numpy scipy pandas matplotlib ipython jupyter r-irkernel r-ggplot2 r-tidyverse\n</code></pre> <p>Note that you can also install jupyterlab instead of, or alongside jupyter. </p>"},{"location":"clusters-at-yale/guides/conda/#conda-channels","title":"Conda Channels","text":"<p>Community-lead collections of packages that you can install with <code>conda</code> are provided with channels. Some labs will provide their own software using this method. A few popular examples are Conda Forge and Bioconda, which we set for you by default. See the Conda docs for more info about managing channels.</p> <p>You can create a new environment called <code>brian2</code> (specified with the <code>-n</code> option) and install Brian2 into it with the following:</p> <pre><code>module load miniconda\nconda create -n brian2 brian2\n# normally you would need this:\n# conda create -n brian2 --channel conda-forge brian2\n</code></pre> <p>You can also install packages from Bioconda, for example:</p> <pre><code>module load miniconda\nconda create -n bioinfo biopython bedtools bowtie2 repeatmasker\n# normally you would need this:\n# conda create -n bioinfo --channel conda-forge --channel bioconda biopython bedtools bowtie2 repeatmasker\n</code></pre>"},{"location":"clusters-at-yale/guides/conda/#use-your-environment","title":"Use Your Environment","text":"<p>To use the applications in your environment, run the following:</p> <pre><code>module load miniconda\nconda activate env_name\n</code></pre> <p>Warning</p> <p>We recommend against putting <code>source activate</code> or <code>conda activate</code> commands in  your <code>~/.bashrc</code> file. This can lead to issues in interactive or batch jobs. If you have issues with an environment, trying re-loading the environment by calling <code>conda deactivate</code> before rerunning <code>conda activate env_name</code>.</p>"},{"location":"clusters-at-yale/guides/conda/#interactive","title":"Interactive","text":"<p>Your Conda environments will not follow you into job allocations. Make sure to activate them after your interactive job begins.</p>"},{"location":"clusters-at-yale/guides/conda/#in-a-job-script","title":"In a Job Script","text":"<p>To make sure that you are running in your project environment in a submission script, make sure to include the following lines in your submission script before running any other commands or scripts (but after your Slurm directives):</p> <pre><code>#!/bin/bash\n#SBATCH --partition=general\n#SBATCH --job-name=my_conda_job\n#SBATCH --cpus-per-task 4\n#SBATCH --mem-per-cpu=6000\n\n\nmodule load miniconda\n\nconda activate env_name\npython analyses.py\n</code></pre>"},{"location":"clusters-at-yale/guides/conda/#find-and-install-additional-packages","title":"Find and Install Additional Packages","text":"<p>You can search Anaconda Cloud or use <code>conda search</code> to find the names of packages you would like to install:</p> <pre><code>module load miniconda\nconda search numpy\n</code></pre>"},{"location":"clusters-at-yale/guides/conda/#compiling-codes","title":"Compiling Codes","text":"<p>You may need to compile codes in a conda environment, for example, installing an R package in a conda R env. This requires you to have the GNU C compiler and its development libraries installed in the conda env before compiling any codes:</p> <pre><code>conda install gcc_linux-64\n</code></pre> <p>Without <code>gcc_linux-64</code>, the code will be compiled using the system compiler and libraries. You will experience run-time errors when running the code in the conda environment. </p>"},{"location":"clusters-at-yale/guides/conda/#troubleshoot","title":"Troubleshoot","text":""},{"location":"clusters-at-yale/guides/conda/#conda-version-doesnt-match-the-module-loaded","title":"Conda version doesn't match the module loaded","text":"<p>If you have run <code>conda init</code> in the past, you may be locked to an old version of <code>conda</code>. You can run the following to fix this:</p> <pre><code>sed -i.bak -ne '/# &gt;&gt;&gt; conda init/,/# &lt;&lt;&lt; conda init/!p' ~/.bashrc\n</code></pre>"},{"location":"clusters-at-yale/guides/conda/#permission-denied","title":"Permission Denied","text":"<p>If you get a permission denied error when running <code>conda install</code> or <code>pip install</code> for a package, make sure you have created an environment and activated it or activated an existing one first.</p>"},{"location":"clusters-at-yale/guides/conda/#bash-conda-no-such-file-or-directory","title":"bash: conda: No such file or directory","text":"<p>If you get the above error, it is likely that you don't have the necessary module file loaded. Try loading the <code>minconda</code> module and rerunning your <code>conda activate env_name</code> command.</p>"},{"location":"clusters-at-yale/guides/conda/#could-not-find-environment","title":"Could not find environment","text":"<p>This error means that the version of miniconda you have loaded doesn't recognize the environment name you have supplied. Make sure you have the <code>miniconda</code> module loaded (and not a different Python module) and have previously created this environment. You can see a list of previously created environments by running:</p> <pre><code>module load miniconda\nconda info --envs\n</code></pre>"},{"location":"clusters-at-yale/guides/conda/#additional-conda-commands","title":"Additional Conda Commands","text":""},{"location":"clusters-at-yale/guides/conda/#list-installed-packages","title":"List Installed Packages","text":"<pre><code>module load miniconda\nconda list --name env_name\n</code></pre>"},{"location":"clusters-at-yale/guides/conda/#delete-a-conda-environment","title":"Delete a Conda Environment","text":"<pre><code>module load miniconda\nconda remove --name env_name --all\n</code></pre>"},{"location":"clusters-at-yale/guides/conda/#save-and-export-environments","title":"Save and Export Environments","text":"<p>There are two concepts for rebuilding conda environments:</p> <ol> <li>a copy of an existing environment, with identical versions of each package</li> <li>a fresh build following the same steps taken to creat the first environment (letting unspecified versions float)</li> </ol> <p>This short doc will walk through recommended approaches to both styles of exporting and rebuilding a generic environment named <code>test</code> containing python, jupyter, numpy, and scipy.</p>"},{"location":"clusters-at-yale/guides/conda/#full-export-including-dependencies","title":"Full Export Including Dependencies","text":"<p>To export the exact versions of each package installed (including all dependencies) run:</p> <pre><code>module load miniconda\nconda env export --no-builds --name test | grep -v prefix &gt; test_export.yaml\n</code></pre> <p>This yaml file is ~230 lines long and contains every package that is installed in the <code>test</code> environment.</p> <p>The conda export command includes information about the path where it was installed (i.e. the <code>prefix</code>). To remove this hard-coded path, we need to remove the line in this print out related to the \"prefix\".</p>"},{"location":"clusters-at-yale/guides/conda/#export-only-specified-packages","title":"Export Only Specified Packages","text":"<p>If we simply wish to rebuild the environment using the steps previously employed to create it, we can replace <code>--no-builds</code> with <code>--from-history</code>. <pre><code>module load miniconda\nconda env export --from-history --name test | grep -v prefix &gt; test_export.yaml\n</code></pre></p> <p>This is a much smaller file, ~10 lines, and only lists the packages explicitly installed:</p> <pre><code>name: test\nchannels:\n  - conda-forge\n  - defaults\n  - bioconda\ndependencies:\n  - scipy\n  - numpy=1.21\n  - jupyter\n  - python=3.8\n</code></pre> <p>In this environment, the versions of python and numpy were pinned during installation, but scipy and jupyter were left to get the most recent compatible version.</p>"},{"location":"clusters-at-yale/guides/conda/#build-a-new-environment","title":"Build a New Environment","text":"<p>To create a new environment using all the enumerated pacakges:</p> <pre><code>module load miniconda\nconda env create --file test_export.yaml\n</code></pre> <p>This will create a new environment with the same name <code>test</code>. The yaml file can be edited to change the name of the new environment.</p>"},{"location":"clusters-at-yale/guides/containers/","title":"Containers","text":"<p>Warning</p> <p>The Singularity project has been renamed Apptainer. Everything should still work the same, including the 'singularity' command.  If you find it not working as expected, please contact us.</p> <p>Warning</p> <p>On the Yale clusters, Apptainer is not installed on login nodes. You will need to run it from compute nodes.</p> <p>Apptainer (formerly Singularity) is a Linux container technology that is well suited to use in shared-user environments such as the clusters we maintain at Yale. It is similar to Docker; You can bring with you a stack of software, libraries, and a Linux operating system that is independent of the host computer you run the container on. This can be very useful if you want to share your software environment with other researchers or yourself across several computers. Because Apptainer containers run as the user that started them and mount home directories by default, you can usually see the data you're interested in working on that is stored on a host computer without any extra work. Additionally, a container containing a program of interest will come preinstalled if built correctly, removing the need to install the program of interest yourself.</p> <p>Below we will outline some common use cases covering the creation and use of containers. There is also excellent documentation available on the full and official user guide for Apptainer. We are happy to help, just contact us with your questions.</p>"},{"location":"clusters-at-yale/guides/containers/#apptainer-containers","title":"Apptainer Containers","text":"<p>Images are the file(s) you use to run your container. Apptainer images are single files that usually end in <code>.sif</code> and are read-only by default, meaning changes you make to the environment inside the container are not persistent.</p>"},{"location":"clusters-at-yale/guides/containers/#use-a-pre-existing-container","title":"Use a Pre-existing Container","text":"<p>If someone has already built a container that suits your needs, you can use it directly. Apptainer images are single files that can be transferred to the clusters. You can fetch images from container registries such as Docker Hub or NVidia Container Registry. Some common resources for finding existing resources is dockerhub and singularity hub. Although, singularity hub is no longer maintained. Once on the website, you can search for containers by typing in the name of the program or library you are interested in obtaining. This will take you to a list of different containers that match the keyword in the search. From there, you can select a container and follow the instructions below to create the container on our clusters:</p> <pre><code># from Docker Hub (https://hub.docker.com/)\napptainer build ubuntu-18.10.sif docker://ubuntu:18.10\napptainer build tensorflow-10.0-py3.sif docker://tensorflow/tensorflow:1.10.0-py3\n\n# from Singularity Hub (no longer updated)\napptainer build bioconvert-latest.sif shub://biokit/bioconvert:latest\n</code></pre> <p>Container images can take up a lot of disk space (dozens of gigabytes), so you may want to change the default location Apptainer uses to cache these files. To do this before getting started, you should add something like the example below to to your <code>~/.bashrc</code> file:</p> <pre><code># set APPTAINER_CACHEDIR if you want to pull files (which can get big) somewhere other than $HOME/.apptainer\n# e.g.\nexport APPTAINER_CACHEDIR=~/scratch60/.apptainer\n</code></pre>"},{"location":"clusters-at-yale/guides/containers/#use-a-container-image","title":"Use a Container Image","text":"<p>Once you have a container image, you can run it as a part of a batch job, or interactively.</p>"},{"location":"clusters-at-yale/guides/containers/#interactively","title":"Interactively","text":"<p>To get a shell in a container so you can interactively work in its environment:</p> <pre><code>apptainer shell --shell /bin/bash containername.sif\n</code></pre>"},{"location":"clusters-at-yale/guides/containers/#in-a-job-script","title":"In a Job Script","text":"<p>You can also run applications from your container non-interactively as you would in a batch job. If I wanted to run a script called <code>my_script.py</code> using my container's python:</p> <pre><code>apptainer exec containername.sif python my_script.py\n</code></pre>"},{"location":"clusters-at-yale/guides/containers/#additional-notes","title":"Additional Notes","text":""},{"location":"clusters-at-yale/guides/containers/#mpi","title":"MPI","text":"<p>MPI support for Apptainer is relatively straight-forward. The only thing to watch is to make sure that you are using the same version of MPI inside your container as you are on the cluster.</p>"},{"location":"clusters-at-yale/guides/containers/#gpus","title":"GPUs","text":"<p>You can use GPU-accelerated code inside your container, which will need most everything also installed in your container (e.g. CUDA, cuDNN). In order for your applications to have access to the right drivers on the host machine, use the <code>--nv</code> flag. For example:</p> <pre><code>apptainer exec --nv tensorflow-10.0-py3.sif python ./my-tf-model.py\n</code></pre>"},{"location":"clusters-at-yale/guides/containers/#home-directories","title":"Home Directories","text":"<p>Sometimes the maintainer of a Docker container you are trying to use installed software into a special user's home directory. If you need access to someone's home directory that exists in the container and not on the host, you should add the <code>--contain</code> option. Unfortunately, you will also then have to explicitly tell Apptainer about the paths that you want to use from inside the container with the <code>--bind</code> option.</p> <pre><code>apptainer shell --shell /bin/bash --contain --bind /gpfs/gibbs/project/support/be59:/home/be59/project bioconvert-latest.sif\n</code></pre>"},{"location":"clusters-at-yale/guides/containers/#environment-variables","title":"Environment Variables","text":"<p>If you are unsure if you are running inside or outside your container, you can run:</p> <pre><code>echo $APPTAINER_NAME\n</code></pre> <p>If you get back text, you are in your container.</p> <p>If you'd like to pass environment variables into your container, you can do so by defining them prefixed with <code>APPTAINERENV_</code> . For Example:</p> <pre><code>export APPTAINERENV_BLASTDB=/home/me/db/blast\napptainer exec my_blast_image.sif env | grep BLAST\n</code></pre> <p>Should return <code>BLASTDB=/home/me/db/blast</code>, which means you set the <code>BLASTDB</code> environment variable in the container properly.</p>"},{"location":"clusters-at-yale/guides/containers/#build-your-own-container-with-definition-files","title":"Build Your Own Container with Definition Files","text":"<p>You can define a container image to be exactly how you want/need it to be, including applications, libraries, and files of your choosing with a definition file. Apptainer definition files are similar to Docker's <code>Dockerfile</code>, but use different syntax. For full definition files and more documentation please see the Apptainer site.</p>"},{"location":"clusters-at-yale/guides/containers/#header","title":"Header","text":"<p>Every container definition must begin with a header that defines what image to start with, or bootstrap from. This can be an official Linux distribution or someone else's container that gets you nearly what you want.</p> <p>To start from Ubuntu Bionic Beaver (18.04 LTS):</p> <pre><code>Bootstrap: docker\nFrom: ubuntu:18.04\n</code></pre> <p>Or an Nvidia developer image</p> <pre><code>Bootstrap: docker\nFrom: nvidia/cuda:9.2-cudnn7-devel-ubuntu18.04\n</code></pre> <p>The rest of the sections all begin with <code>%</code> and the section name. You will see section contents indented by convention, but this is not required.</p>"},{"location":"clusters-at-yale/guides/containers/#labels","title":"%labels","text":"<p>The labels section allows you to define metadata for your container:</p> <pre><code>%labels\n    Name\n    Maintainer \"YCRC Support Team\" &lt;hpc@yale.edu&gt;Version v99.9\n    Architecture x86_64\n    URL https://research.computing.yale.edu/&lt;/hpc@yale.edu&gt;\n</code></pre> <p>You can examine container metadata with the <code>apptainer inspect</code> command.</p>"},{"location":"clusters-at-yale/guides/containers/#files","title":"%files","text":"<p>If you'd like to copy any files from the system you are building on, you do so in the %files section. Each line in the files section is a pair of source and destination paths, where the source is on your host system, and destination is a path in the container.</p> <pre><code>%files\n    sample_data.tar /opt/sample_data/\n    example_script.sh /opt/sample_data/\n</code></pre>"},{"location":"clusters-at-yale/guides/containers/#post","title":"%post","text":"<p>The post section is where you can run updates, installs, etc in your container to customize it.</p> <pre><code>%post\n    echo \"Customizing Ubuntu\"\n    apt-get update\n    apt-get -y install software-properties-common build-essential cmake\n    add-apt-repository universe\n    apt-get update\n    apt-get -y libboost-all-dev libgl1-mesa-dev libglu1-mesa-dev\n    cd /tmp\n    git clone https://github.com/gitdudette/myapp &amp;&amp; cd myapp\n    # ... etc etc\n</code></pre>"},{"location":"clusters-at-yale/guides/containers/#environment","title":"%environment","text":"<p>The environment section allows you to define environment variables for your container. These variables are available when you run the built container, not during its build.</p> <pre><code>%environment\n    export PATH=/opt/my_app/bin:$PATH\n    export LD_LIBRARY_PATH=/opt/my_app/lib:$LD_LIBRARY_PATH\n</code></pre>"},{"location":"clusters-at-yale/guides/containers/#building","title":"Building","text":"<p>To finally build your container after saving your definition file as <code>my_app.def</code>, for example, you would run</p> <pre><code>apptainer build my_app.sif my_app.def\n</code></pre>"},{"location":"clusters-at-yale/guides/cryoem/","title":"Cryogenic Electron Microscopy (Cryo-EM) Data Processing on McCleary","text":"<p>Below is a work in progress collection of general hints, tips and tricks for running your work on McCleary. As always, if anything below is unclear or could use updating, please let us know during office hours, via email or through our web ticketing system.</p>"},{"location":"clusters-at-yale/guides/cryoem/#storage","title":"Storage","text":"<p>Be wary of you and your group's storage quotas. Run <code>getquota</code> from time to time to make sure there isn't usage you aren't expecting. We strongly recommend that you archive raw data off-cluster, as only home directories are backed up. Let us know if you need extra space and we can work with you to find a solution that is right for your project and your group.</p> <p>On most GPU nodes there is a fast SSD mounted at <code>/tmp</code>. You can use this as a fast local cache if your program can take advantage of it.</p>"},{"location":"clusters-at-yale/guides/cryoem/#schedule-jobs","title":"Schedule Jobs","text":"<p>Many Cryo-EM applications can make use of GPUs as co-processors. In order to use a GPU on McCleary you must allocate a job on a partition with GPUs available and explicitly request GPU(s). Make sure to familiarize yourself with our documentation on scheduling jobs and requesting specific resources.</p> <p>In addition to public partitions that give you access to GPUs, there are <code>pi_cryoem</code> and <code>pi_tomo</code> partitions which are limited to users of the Cryo-EM resources on campus. Please coordinate with the staff from West Campus and CCMI (See here for contact info) for access.</p>"},{"location":"clusters-at-yale/guides/cryoem/#software","title":"Software","text":"<p>Many Cryo-EM applications are meant to be viewed and interacted with in real-time. This mode of working is not ideal for the way most HPC clusters are set up, so where possible try to prototype a job you would like to run with a smaller dataset or subset of your data. Then develop a script to submit with <code>sbatch</code>.</p>"},{"location":"clusters-at-yale/guides/cryoem/#relion","title":"RELION","text":"<p>The RELION pipeline operates in two modes. You can use it as a more familiar and beginner-friendly graphical interface, or call the programs involved directly. Once you are comfortable, using the commands directly in scripts submitted with <code>sbatch</code> will allow you to get the most work done the fastest.</p> <p>The authors provide up-to-date hints about performance on their Benchmarks page. If you need technical help (jobs submit fine but having other issues) you should search and submit to their mailing list.</p>"},{"location":"clusters-at-yale/guides/cryoem/#module","title":"Module","text":"<p>We have GPU-enabled versions of RELION available on McCleary as software modules. To check witch versions are available, run <code>module avail relion</code>. To see specific notes about a particular install, you can use <code>module help</code>, e.g. <code>module help RELION/4.0.0-fosscuda-2020b</code> . </p>"},{"location":"clusters-at-yale/guides/cryoem/#example-job-parameters","title":"Example Job Parameters","text":"<p>RELION reserves one worker (slurm task) for orchestrating an MPI-based job, which they call the \"master\". This can lead to inefficient jobs where there are tasks that could be using a GPU but are stuck being the master process. You can request a better layout for your job with a heterogenous job, allocating CPUs on a cpu-only compute node for the task that will not use GPUs. Here is an example 3D refinement job submission script (replace <code>choose_a_version</code> with the version you want to use):</p> <pre><code>#!/bin/bash\n#SBATCH --partition=general --ntasks 1 -c2 --job-name=class3D_hetero_01 --mem=10G --output=\"class3D_hetero_01-%j.out\"\n#SBATCH hetjob\n#SBATCH --partition=gpu --ntasks 4 -c2 -N1 --mem-per-cpu=16G --gpus-per-task=1 \n\nmodule load RELION/choose_a_version\n\nsrun --pack-group=0,1 relion_refine_mpi --o hetero/refine3D/job0001 ... --dont_combine_weights_via_disc --j ${SLURM_CPUS_PER_TASK} --gpu\n</code></pre> <p>This job submission request will result in RELION using a single task/worker on a general purpose CPU node, and efficiently find four GPUs even if they aren't all available on the same compute node. Each GPU node task/worker will have a dedicated GPU, two CPU cores, and 30GiB total memory. </p>"},{"location":"clusters-at-yale/guides/cryoem/#eman2","title":"EMAN2","text":"<p>EMAN2 has always been a bit of a struggle for us to install properly on the clusters. Below are a few options</p>"},{"location":"clusters-at-yale/guides/cryoem/#conda-install","title":"Conda Install","text":"<p>The EMAN2 authors offer some instructions on how to get EMAN2 running in a cluster environment on their install page. The default install may work as well if you avoid using MPI.</p>"},{"location":"clusters-at-yale/guides/cryoem/#container","title":"Container","text":"<p>At present, we have a mostly working apptainer container for EMAN2.3 available here: </p> <p><code>/gpfs/ysm/datasets/cryoem/eman2.3_ubuntu18.04.sif</code></p> <p>To run a program from EMAN2 using this container you would use a command like:</p> <pre><code>apptainer exec /gpfs/ysm/datasets/cryoem/eman2.3_ubuntu18.04.sif e2projectmanager.py\n</code></pre>"},{"location":"clusters-at-yale/guides/cryoem/#cryosparc","title":"Cryosparc","text":"<p>We have a whole separate page about this one, it is a bit involved.</p>"},{"location":"clusters-at-yale/guides/cryoem/#other-software","title":"Other Software","text":"<p>We have CCP4, Phenix and some other software modules of interest installed. Run <code>module avail</code> and the software name to search for them. If you can't find one you need, please contact us.</p>"},{"location":"clusters-at-yale/guides/cryosparc/","title":"CryoSPARC on McCleary","text":"<p>Getting CryoSPARC set up and running on the YCRC clusters is something of a task. This guide is meant for intermediate/advanced users. YCRC staff are working on a CryoSPARC application for Open OnDemand Until then, venture below at your own peril.</p>"},{"location":"clusters-at-yale/guides/cryosparc/#install","title":"Install","text":"<p>Before you get started, you will need to request a licence from Structura from their website. These instructions are somewhat modified from the official CryoSPARC documentation. </p>"},{"location":"clusters-at-yale/guides/cryosparc/#1-set-up-environment","title":"1. Set up Environment","text":"<p>First, get an interactive session or an Open Ondemand Remote Desktop in a partition with GPUs that you have access to. \u00a0Remember to request a non-zero number of GPUs.</p> <p>Then choose a location for installing the software, such as under your project directory.</p> <pre><code>export install_path=${HOME}/project/cryosparc\nmkdir -p ${install_path}\n</code></pre>"},{"location":"clusters-at-yale/guides/cryosparc/#2-set-up-directories-download-installers","title":"2. Set up Directories, Download installers","text":"<pre><code>export LICENSE_ID=Your-cryosparc-license-code-here\n\n #go get the installers\ncd $install_path\ncurl -L https://get.cryosparc.com/download/master-latest/$LICENSE_ID -o cryosparc_master.tar.gz\ncurl -L https://get.cryosparc.com/download/worker-latest/$LICENSE_ID -o cryosparc_worker.tar.gz\n\ntar -xf cryosparc_master.tar.gz\ntar -xf cryosparc_worker.tar.gz\n</code></pre>"},{"location":"clusters-at-yale/guides/cryosparc/#3-install-the-standalone-serverworker","title":"3. Install the Standalone Server/Worker","text":"<pre><code># Load a cluster CUDA module\n\ncd ${install_path}/cryosparc_master\nmodule load CUDA/12.0.0\nexport cuda_path=${EBROOTCUDA}\n\n# Set a temporary password\nexport cryosparc_passwd=Password123\n\nexport db_path=${install_path}/cryosparc_database\nexport worker_path=${install_path}/cryosparc_worker\nexport ssd_path=/tmp/${USER}\nmkdir $ssd_path\nexport user_email=\"Your email address\"\n\n# Run the installation script\ncd ${install_path}/cryosparc_master\n\n./install.sh --standalone \\\n--license $LICENSE_ID \\\n--worker_path $worker_path \\\n--ssdpath $ssd_path \\\n--initial_email $user_email \\\n--initial_password $cryosparc_passwd \\\n--initial_username ${USER} \\\n--initial_firstname \"Firstname\" \\\n--initial_lastname \"Lastname\"\n</code></pre> <p>Warning</p> <p>If you are installing a version of CryoSPARC older than 4.4.0, add the additional line</p> <pre><code>--cudapath $cuda_path \\\n</code></pre> <p>after the --ssdpath line.</p> <pre><code># Set location of CryoSPARC executables\nsource ~/.bashrc\n</code></pre>"},{"location":"clusters-at-yale/guides/cryosparc/#4-test","title":"4. Test","text":"<p>The installation process will normally attempt to launch CryoSPARC automatically.  Check its status and launch manually if need be.</p> <pre><code>cryosparcm status\ncryosparcm start (if not running) \n</code></pre> <p>If everything is running, you should be able to launch Firefox and see the CryoSPARC login interface at http://localhost:39000.</p> <p>When you are done testing, shut down CryoSPARC,</p> <pre><code>cryosparcm stop\n</code></pre> <p>and exit your interactive session.</p>"},{"location":"clusters-at-yale/guides/cryosparc/#run","title":"Run","text":"<p>There are two approaches to running CryoSPARC for data processing: directly in an interactive session as above, and in a remote session controlled by a batch script. Due to the current time limitations on the interactive public partitions, it is recommended to use the batch script method. If you are running in the pi_cryoem or pi_tomography partition, you have a longer interactive time limit, but the batch script method is still preferable, as it cleans up the CryoSPARC session automatically. </p> <p>You are not guaranteed to get the same GPU node every time, so you need to set CryoSPARC to use whichever one you are running on. The first step in the following directions reconfigures the node name to match the current compute node.</p>"},{"location":"clusters-at-yale/guides/cryosparc/#1-submit-a-batch-script","title":"1. Submit a batch script","text":"<p>The following batch script template illustrates how to set up a remote CryoSPARC session.  By default, it will create an output file whose name contains the compute node that CryoSPARC has been launched on.</p>"},{"location":"clusters-at-yale/guides/cryosparc/#a-copy-the-batch-script-template-below-into-a-file-in-the-desired-directory-on-the-cluster","title":"a. Copy the batch script template below into a file in the desired directory on the cluster.","text":"<pre><code>#!/bin/bash\n#SBATCH --partition=gpu\n#SBATCH --time=2-00:00:00\n#SBATCH --mem=64G\n#SBATCH -N 1 -c 8\n#SBATCH --gpus=4\n#SBATCH --signal=B:10@60 # send the signal '10' at 60s before job finishes\n#SBATCH --job-name=CryoSPARC-batch\n#SBATCH --output=\"cryosparc-%N-%j.out\"\n\nfunction cleanup()\n{\n    date\n    echo -n \"Shutting down CryoSPARC @ \"; date\n    cryosparcm start\n    cryosparcm cli \"remove_scheduler_target_node('$worker_host')\"\n    cryosparcm stop\n    echo \"Done\"\n}\n\n# Shut down CryoSPARC cleanly when timeout is imminent\ntrap cleanup 10\n\n# Shut down CryoSPARC cleanly when scancel is invoked\ntrap cleanup 15\n\nmkdir /tmp/${USER}\nexport master_host=$(hostname)\nexport worker_host=$(hostname)\nexport base_dir=$(dirname \"$(dirname \"$(which cryosparcm)\")\")\n\nsed -i.bak 's/export CRYOSPARC_MASTER_HOSTNAME.*$/export CRYOSPARC_MASTER_HOSTNAME=\\\"'\"$master_host\"'\\\"/g' $base_dir/config.sh\n\nsource $base_dir/config.sh\n\ncryosparcm start\n\n# Forcibly add the current node as a worker\ncryosparcw connect \\\n        --worker $worker_host \\\n        --master $master_host \\\n        --port 39000 \\\n        --ssdpath /tmp/${USER} \\\n        --cpus $SLURM_CPUS_PER_TASK\n\nsleep infinity &amp;\nwait\n</code></pre>"},{"location":"clusters-at-yale/guides/cryosparc/#b-adjust-the-script-contents-as-desired-for-memory-cpu-time-and-partition","title":"b. Adjust the script contents as desired for memory, CPU, time, and partition.","text":""},{"location":"clusters-at-yale/guides/cryosparc/#c-submit-the-script-to-slurm","title":"c. Submit the script to SLURM.","text":"<pre><code>sbatch YourScriptName\n</code></pre>"},{"location":"clusters-at-yale/guides/cryosparc/#d-check-the-contents-of-the-job-output-file-to-make-sure-that-cryosparc-has-launched","title":"d. Check the contents of the job output file to make sure that CryoSPARC has launched.","text":""},{"location":"clusters-at-yale/guides/cryosparc/#e-in-an-open-ondemand-remote-desktop-session-open-the-firefox-web-browser-and-enter-the-name-of-the-compute-node-your-batch-job-is-running-on","title":"e. In an Open OnDemand Remote Desktop session, open the Firefox web browser, and enter the name of the compute node your batch job is running on","text":"<p>(obtainable from the output filename, or from squeue).</p> <p>http://YourComputeNode:39000</p> <p>This should present you with the CryoSPARC login screen.</p>"},{"location":"clusters-at-yale/guides/cryosparc/#f-you-can-exit-your-remote-desktop-session-without-terminating-cryosparc-and-reconnect-to-your-cryosparc-instance-later","title":"f. You can exit your Remote Desktop session without terminating CryoSPARC, and reconnect to your CryoSPARC instance later.","text":""},{"location":"clusters-at-yale/guides/cryosparc/#g-cryosparc-will-shut-down-automatically-when-time-runs-out-if-you-want-to-stop-it-before-this-simply-cancel-the-batch-job","title":"g. CryoSPARC will shut down automatically when time runs out.  If you want to stop it before this, simply cancel the batch job.","text":"<pre><code>scancel YourJobID\n</code></pre>"},{"location":"clusters-at-yale/guides/cryosparc/#2-relaunching-in-an-interactive-session-not-generally-recommended-as-noted-above","title":"2. Relaunching in an interactive session (not generally recommended, as noted above)","text":"<p>Once you have started a new GPU node session,</p> <pre><code>mkdir -p /tmp/${USER}\nexport master_host=$(hostname)\nexport worker_host=$(hostname)\nexport base_dir=$(dirname \"$(dirname \"$(which cryosparcm)\")\")\n\nsed -i.bak 's/export CRYOSPARC_MASTER_HOSTNAME.*$/export CRYOSPARC_MASTER_HOSTNAME=\\\"'\"$master_host\"'\\\"/g' $base_dir/config.sh\n\nsource $base_dir/config.sh\n\ncryosparcm start\n\n# Forcibly add the current node as a worker\ncryosparcw connect \\\n        --worker $worker_host \\\n        --master $master_host \\\n        --port 39000 \\\n        --ssdpath /tmp/${USER} \\\n        --cpus $SLURM_CPUS_PER_TASK\n</code></pre> <p>and then connect to https://localhost:39000 as above.</p> <p>Among other things, this will not perform the automatic cleanup of CryoSPARC lockfiles or of previous worker nodes, so you will want to manually run shutdown and worker removal when you are done with CryoSPARC:</p> <pre><code>cryosparcm cli \"remove_scheduler_target_node('$worker_host')\"\ncryosparcm stop\n</code></pre>"},{"location":"clusters-at-yale/guides/cryosparc/#running-on-multiple-nodes","title":"Running on multiple nodes","text":"<p>In principle, you can request multiple nodes for your job session, and configure CryoSPARC to use additional nodes as worker nodes. Contact YCRC staff for assistance.</p>"},{"location":"clusters-at-yale/guides/cryosparc/#topaz-support-optional","title":"Topaz support (Optional)","text":"<p>Topaz is a pipeline for particle picking in cryo-electron microscopy images using convolutional neural networks. It can optionally be used from inside CryoSPARC using our cluster module.</p>"},{"location":"clusters-at-yale/guides/cryosparc/#1-in-an-interactive-cluster-session-load-the-desired-topaz-module-and-determine-the-location-of-the-executable-eg","title":"1. In an interactive cluster session, load the desired Topaz module and determine the location of the executable; e.g.,","text":"<pre><code>module load topaz/0.2.5.20240417-foss-2022b-CUDA-12.0.0\nwhich topaz\n/vast/palmer/apps/avx2/software/topaz/0.2.5.20240417-foss-2022b-CUDA-12.0.0/bin/topaz\n</code></pre>"},{"location":"clusters-at-yale/guides/cryosparc/#2-in-a-running-cryosparc-instance-add-this-executable-to-the-general-settings","title":"2. In a running CryoSPARC instance, add this executable to the General settings:","text":""},{"location":"clusters-at-yale/guides/cryosparc/#3-consult-the-cryosparc-guide","title":"3. Consult the CryoSPARC guide","text":"<p>for details on using Topaz with CryoSPARC.</p>"},{"location":"clusters-at-yale/guides/cryosparc/#troubleshoot","title":"Troubleshoot","text":"<p>If you are unable to start a new CryoSPARC instance, the likeliest reason is leftover files from a previous run that was not shut down properly. Check /tmp and /tmp/${USER} for the existence of a cryosparc.sock file or a mongo.sock file.  If they are owned by you, you can just remove them, and CryoSPARC should start normally.  If they are not owned by you, and you have reserved the GPUs, then it is likely due to another user's interrupted job.  Contact YCRC staff for assistance.</p> <p>If your database won't start and you're sure there isn't another server running, you can remove lock files and try again.</p> <pre><code># rm -f $CRYOSPARC_DB_PATH/WiredTiger.lock $CRYOSPARC_DB_PATH/mongod.lock\n</code></pre>"},{"location":"clusters-at-yale/guides/easybuild/","title":"EasyBuild","text":"<p>The YCRC uses a framework called EasyBuild to build and install the software you access via the module system. EasyBuild can also be used to install software locally within your storage space.</p> <p>When you install a new software package, EasyBuild performs the following steps:</p> <ul> <li>finds the easyconfig file matching the name you specify </li> <li>gets sources(either downloaded or found in source path)</li> <li>configures</li> <li>builds</li> <li>installs</li> <li>generates module</li> </ul>"},{"location":"clusters-at-yale/guides/easybuild/#get-started","title":"Get Started","text":"<p>To get started with installing software with EasyBuild, request an interactive session on the oldest available nodes and load the EasyBuild module:</p> <pre><code>salloc --cpus-per-task 4 --constraint oldest \nmodule load EasyBuild\n</code></pre>"},{"location":"clusters-at-yale/guides/easybuild/#configure-easybuild","title":"Configure EasyBuild","text":"<p>Some of the important configurations settings are:</p> <ul> <li>Source path: parent path of the directory for software source and install files</li> <li>Build path: parent path of the temporaty directory for building software packages</li> <li>Software and module install path: by default, software install path is <code>$HOME/.local/easybuild/software</code> and module install path is <code>$HOME/.local/easybuild/modules/all</code>. </li> </ul> <p>You can look up the current configuration of EasyBuild with the following command: <pre><code>eb --show-config\n\n#\n# Current EasyBuild configuration\n# (C: command line argument, D: default value, E: environment variable, F: configuration file)\n#\nbuildpath      (D) = /home/testuser/.local/easybuild/build\ncontainerpath  (D) = /home/testuser/.local/easybuild/containers\ninstallpath    (D) = /home/testuser/.local/easybuild\nrepositorypath (D) = /home/testuser/.local/easybuild/ebfiles_repo\nrobot-paths    (D) = /vast/palmer/apps/avx2/software/EasyBuild/4.9.3/easybuild/easyconfigs\nsourcepath     (D) = /home/testuser/.local/easybuild/sources\n</code></pre> If you wish to change any of these paths, you can do so using several methods including command-line arguments, environment variables and configuration files. For more details, take a look at the documentation. For example, if you would like to use a configuration file, please create a <code>config.cfg</code> file in <code>$HOME/.config/easybuild</code>: <pre><code>cat $HOME/.config/easybuild/config.cfg\n\n[config]\nbuildpath=/dev/shm/%(USER)s/build\ninstallpath=/gpfs/gibbs/project/testuser/testuser/apps\nsourcepath=/gpfs/gibbs/project/testuser/testuser/source\n</code></pre> This configuration will change the output of <code>eb --show-config</code> to: <pre><code># Current EasyBuild configuration\n# (C: command line argument, D: default value, E: environment variable, F: configuration file)\n#\nbuildpath      (F) = /dev/shm/testuser/build\ncontainerpath  (D) = /home/testuser/.local/easybuild/containers\ninstallpath    (F) = /gpfs/gibbs/project/testuser/testuser/apps\nrepositorypath (D) = /home/testuser/.local/easybuild/ebfiles_repo\nrobot-paths    (D) = /vast/palmer/apps/avx2/software/EasyBuild/4.9.3/easybuild/easyconfigs\nsourcepath     (F) = /gpfs/gibbs/project/testuser/testuser/source\n</code></pre></p>"},{"location":"clusters-at-yale/guides/easybuild/#search-for-easyconfig-files","title":"Search for Easyconfig files","text":"<p>To install software with EasyBuild, you need easyconfig files. Easyconfig files specify build parameters such as name, toolchain, sources, and dependencies. To searc for an existing easyconfig file for a specific softwar in the EasyBuild repository, you can use the <code>--search</code> or <code>-S</code> option: <pre><code>eb -S HPCG\n\n== found valid index for /vast/palmer/apps/avx2/software/EasyBuild/4.9.3/easybuild/easyconfigs, so using it...\nCFGS1=/vast/palmer/apps/avx2/software/EasyBuild/4.9.3/easybuild/easyconfigs\n * $CFGS1/h/HPCG/HPCG-3.0-foss-2016b.eb\n * $CFGS1/h/HPCG/HPCG-3.0-foss-2018b.eb\n * $CFGS1/h/HPCG/HPCG-3.0-intel-2018b.eb\n * $CFGS1/h/HPCG/HPCG-3.1-foss-2018b.eb\n * $CFGS1/h/HPCG/HPCG-3.1-foss-2021a.eb\n * $CFGS1/h/HPCG/HPCG-3.1-foss-2021b.eb\n * $CFGS1/h/HPCG/HPCG-3.1-foss-2022a.eb\n * $CFGS1/h/HPCG/HPCG-3.1-foss-2022b.eb\n * $CFGS1/h/HPCG/HPCG-3.1-foss-2023a.eb\n * $CFGS1/h/HPCG/HPCG-3.1-intel-2018b.eb\n * $CFGS1/h/HPCG/HPCG-3.1-intel-2021a.eb\n * $CFGS1/h/HPCG/HPCG-3.1-intel-2021b.eb\n * $CFGS1/h/HPCG/HPCG-3.1-intel-2022a.eb\n * $CFGS1/h/HPCG/HPCG-3.1-intel-2022b.eb\n * $CFGS1/h/HPCG/HPCG-3.1-intel-2023a.eb\n * $CFGS1/h/HPCG/HPCG-3.1_fix-loop-upper-bound-variable-to-be-explicitly-designated-as-shared.patch\n\nNote: 2 matching archived easyconfig(s) found, use --consider-archived-easyconfigs to see them\n</code></pre> The config file typically has the name format <code>software-version-toolchain-version.eb</code>. Make sure to choose easyconfig files that uses the available toolchains on our clusters. If you wish to edit the easyconfig file, copy and paste it in your own cluster space. For example: <pre><code>cp /vast/palmer/apps/avx2/software/EasyBuild/4.9.3/easybuild/easyconfigs/h/HPCG/HPCG-3.1-intel-2022b.eb $HOME\n</code></pre> Let's say we added a new patch and <code>versionsuffix = '-patched'</code> to the easyconfig file. If you change source files and patches in the easyconfig files, please update the checksums: <pre><code>eb --inject-checksums --force HPCG-3.1-intel-2022b.eb\n</code></pre></p>"},{"location":"clusters-at-yale/guides/easybuild/#install-with-easybuild","title":"Install with EasyBuild","text":"<p>Before you install software with EasyBuild, you can perform a dry-run installation of software: <pre><code>eb HPCG-3.1-intel-2022b.eb --dry-run\n</code></pre> This will list all dependent software packages and indicate whether they are already installed as modules on the cluster.  </p> <p>To install a software and create a module, run the following command: <pre><code>eb HPCG-3.1-intel-2022b.eb\n</code></pre></p>"},{"location":"clusters-at-yale/guides/easybuild/#use-locally-installed-modules","title":"Use locally installed modules","text":"<p>You can use the command: <pre><code>module use $HOME/.local/easybuild/modules/all\nmodule load HPCG/3.1-intel-2022b-patched\n</code></pre> then <code>module avail</code> command will display modules installed locally.</p> <p>You can also update $MODULEPATH in your $HOME/.bashrc file. Add this line: <pre><code>export MODULEPATH=$MODULEPATH:$HOME/.local/easybuild/modules/all\n</code></pre></p>"},{"location":"clusters-at-yale/guides/gaussian/","title":"Gaussian","text":"<p>Note</p> <p>Access to Gaussian on the Yale clusters is free, but available by request only. To gain access to the installations of Gaussian, please contact us to be added to the <code>gaussian</code> group.</p> <p>Gaussian is an electronic structure modeling program that Yale has licensed for its HPC clusters. The latest version of Gaussian is Gaussian 16, which also includes GaussView 6. Older versions of both applications are also available. To see a full list of available versions of Gaussian on the cluster, run:</p> <pre><code>module avail gaussian\n</code></pre>"},{"location":"clusters-at-yale/guides/gaussian/#running-gaussian-on-the-cluster","title":"Running Gaussian on the Cluster","text":"<p>The examples here are for Gaussian 16. In most cases, you could run the older version Gaussian 09 by replacing \"g16\" with \"g09\" wherever it occurs.</p> <p>When running Gaussian, it is recommended that users request exclusive access to allocated nodes (e.g., by requesting all the cpus on the node) and that they specify the largest possible memory allocation for the number of nodes requested. In addition, in most cases, the scratch storage location (set by the environment variable <code>GAUSS_SCRDIR</code>) should be on the local parallel scratch file system (e.g., scratch60)  of the cluster, rather than in the user\u2019s home directory. (This is the default in the Gaussian module files.)</p> <p>Before running Gaussian, you must set up a number of environment variables. This is accomplished most easily by loading the Gaussian module file using:</p> <pre><code>module load Gaussian\n</code></pre> <p>To run Gaussian interactively, you need to create an interactive session on a compute node. You could start an interactive session using 4 cores for 4 hours using</p> <pre><code>salloc -c 4 -t 4:00:00\n</code></pre> <p>See our Slurm documentation for more detailed information on requesting resources for interactive jobs.</p>"},{"location":"clusters-at-yale/guides/gaussian/#gaussview","title":"GaussView","text":"<p>In connection with Gaussian 16, we have also installed GaussView 6, Gaussian Inc.'s\u00a0most advanced and powerful graphical interface for Gaussian. With GaussView, you can import or build the molecular structures that interest you; set up, launch, monitor and control Gaussian calculations; and retrieve and view the results, all without ever leaving the application. GaussView 6 includes many new features designed to make working with large systems of chemical interest convenient and straightforward. It also provides full support for all of the new modeling methods and features in Gaussian 16.</p> <p>In order to use GaussView, you must run an X Server on your desktop or laptop, and you must enable X forwarding when logging into the cluster. See our X11 forwarding documentation for instructions.</p> <p>Loading the module file for Gaussian sets up your environment for GaussView as well. Then you can start GaussView by typing the command <code>gv</code>. GaussView 6 may not be compatible with certain versions of the X servers you may run on your desktop or laptop. If you encounter problems, these can often be overcome by starting GaussView with the command <code>gv -mesagl</code> or <code>gv -soft</code>.</p>"},{"location":"clusters-at-yale/guides/github/","title":"Version control with Git and GitHub","text":""},{"location":"clusters-at-yale/guides/github/#what-is-version-control","title":"What is version control?","text":"<p>Version contol is an easy and powerful way to track changes to your work.  This extends from code to writing documents (if using LaTeX/Tex).  It produces and saves \"tagged\" copies of your project so that you don't need to worry about breaking your  code-base. This provides a \"coding safety net\" to let you try new features while retaining the ability to roll-back to a  working version.</p> <p>Whether developing large frameworks or simply working on small scripts, version control is an important tool to ensure that your work is never lost.  We recommend using <code>git</code> for its flexibility and versatility and GitHub for its power in enabling research and collaboration. <sup>1</sup></p> <p>Here we will cover the basics of version control and how to use <code>git</code> and GitHub. </p>"},{"location":"clusters-at-yale/guides/github/#what-is-git-and-how-does-it-work","title":"What is <code>git</code> and how does it work?","text":"<p>Git is a tool that tracks changes to a file (or set of files) through a series of snapshots called \"commits\" or \"revisions\".  These snapshots are stored in \"repositories\" which contain the history of all the changes to that file.  This helps prevent repetative naming or <code>project_final_final2_v3.txt</code> problems.  It acts as a record of all the edits, along with the ability to compare the current version to previous commits. </p>"},{"location":"clusters-at-yale/guides/github/#how-to-create-a-git-repository","title":"How to create a <code>git</code> repository","text":"<p>You can create a repository at any time by running the following commands:</p> <pre><code>cd /path/to/your/project\n\n# initialize the repository\ngit init\n\n# add files to be tracked\ngit add main.py input.txt \n\n# commit the files to the repository, creating the first snapshot\ngit commit -m \"Initial Commit\"\n</code></pre> <p>This sets up a repository containing a single snapshot of the project's two files. We can then edit these files and commit the changes into a new snapshot:</p> <p><pre><code># edit files\necho \"changed this file\" &gt;&gt; input.txt\n$ git status\nOn branch main\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git checkout -- &lt;file&gt;...\" to discard changes in working directory)\n\n    modified:   input.txt\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n</code></pre> Finally, we can stage <code>input.txt</code> and then commit the changes:</p> <pre><code># stage changes for commit\ngit add input.txt\ngit commit -m \"modified input file\"\n</code></pre>"},{"location":"clusters-at-yale/guides/github/#configuring-git","title":"Configuring <code>git</code>","text":"<p>It's very helpful to configure your email and username with <code>git</code>:</p> <p><pre><code>git config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@yale.edu\"\n</code></pre> This will then tag your changes with your name and email when collaborating with people on a larger  project. </p>"},{"location":"clusters-at-yale/guides/github/#working-with-remote-repositories-on-github","title":"Working with remote repositories on GitHub","text":"<p>We recommend using an off-site repository like GitHub that provides a secure and co-located  backup of your local repositories. </p> <p>To start, create a repository on GitHub by going to https://github.com/new and providing a name and  choose either public or private access. Then you can connect your local repository to the GitHub repo (named <code>my_new_repo</code>):</p> <pre><code>git remote add origin git@github.com:user_name/my_new_repo.git\ngit push -u origin main\n</code></pre> <p>Alternatively, a repository can be created on GitHub and then cloned to your local machine: <pre><code>$ git clone git@github.com:user_name/my_new_repo.git\nCloning into 'my_new_repo'...\nremote: Enumerating objects: 3, done.\nremote: Counting objects: 100% (3/3), done.\nremote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\nReceiving objects: 100% (3/3), done.\n</code></pre> This creates a new directory (<code>my_new_repo</code>) where you can place all your code.</p> <p>After making any changes and commiting them to the local repository, you can \"push\" them to a remote repository:</p> <pre><code># commit to local repository\ngit commit -m \"new changes\"\n\n# push commits to remote repository on GitHub\ngit push\n</code></pre>"},{"location":"clusters-at-yale/guides/github/#educational-github","title":"Educational GitHub","text":"<p>All students and research staff are able to request free Educational discounts from GitHub.  This provides a \"Pro\" account for free, including unlimited private repositories. </p> <p>To get started, create a free GitHub account with your Yale email address.  Then go to https://education.github.com and request the educational discount.  It normally takes less than 24 hours for them to grant the discount.</p> <p>Educational discounts are also available for teams and collaborations.  This is perfect for a research group or collaboration and can include non-Yale affiliated people. </p>"},{"location":"clusters-at-yale/guides/github/#resources-and-links","title":"Resources and links","text":"<ul> <li>YCRC Version Control Bootcamp</li> <li>Educational GitHub</li> <li>GitHub's Try-it</li> <li>Instruqt Getting Started With Git</li> </ul> <ol> <li> <p>We do not recommend the use of https://git.yale.edu, which is an internal-only tool not designed  for research use.\u00a0\u21a9</p> </li> </ol>"},{"location":"clusters-at-yale/guides/github_pages/","title":"GitHub Pages","text":""},{"location":"clusters-at-yale/guides/github_pages/#personal-website","title":"Personal Website","text":"<p>A personal website is a great way to build an online presence for both academic and professional activities. We recommend using GitHub Pages as a tool to maintain and host static websites and blogs. Unlike other hosting platforms, the whole website can be written using Markdown, a simple widely-used markup language. GitHub provides a tutorial to get started with Markdown (link).</p> <p>To get started, you're going to need a GitHub account. You can follow the instructions on our GitHub guide to set up a free account. Once you have an account, you will need to create a repository for your website. It's important that you name your repository <code>username.github.io</code>  where username is replaced with your actual account name (<code>ycrc-test</code> in this example).</p> <p></p> <p>Make sure to initialize the repo with a README, which will help get things started. After clicking \"Create\" your repository will look like this:</p> <p></p> <p>From here, you can click on \"Settings\" to enable GitHub Pages publication of your site. Scroll down until you see GitHub Pages:</p> <p></p> <p>GitHub provides a number of templates to help make your website look professional. Click on \"Choose a Theme\" to see examples of these themes:</p> <p></p> <p>Pick one that you like and click \"Select theme\". Note, some of these themes are aimed at blogs versus project sites, pick one that best fits your desired style. You can change this later, so feel free to try one out and see what you think.</p> <p>After selecting your theme, you will be directed back to your repository where the README.md has been updated with some basics about how Markdown works and how you can start creating your website.</p> <p></p> <p>Scroll down and commit these changes (leaving the sample text in place).</p> <p></p> <p>You can now take a look at how GitHub is rendering your site:</p> <p></p> <p>That's it, this site is now hosted at ycrc-test.github.io! You now have a simple-to-edit and customize site that can be used to host your CV, detail your academic research, or showcase your independent projects.</p>"},{"location":"clusters-at-yale/guides/github_pages/#project-website","title":"Project website","text":"<p>In addition to hosting a stand-alone website, GitHub Pages can be used to create pages for specific projects or repositories. Here we will take an existing repository amazing-python-project and add a GitHub Pages website on a new branch.</p> <p></p> <p>Click on the Branch pull-down and create a new branch titled <code>gh-pages</code>:</p> <p></p> <p>Remove any files from that branch and create a new file called <code>index.md</code>:</p> <p></p> <p>Add content to the page using Markdown syntax:</p> <p></p> <p>To customize the site, click on <code>Settings</code> and then scroll down to <code>GitHub Pages</code>:</p> <p></p> <p></p> <p>Click on the <code>Theme Chooser</code> and select your favorite style:</p> <p></p> <p>Finally, you can navigate to your website and see it live!</p> <p></p>"},{"location":"clusters-at-yale/guides/github_pages/#conclusions","title":"Conclusions","text":"<p>We have detailed two ways to add static websites to your work, either as a professional webpage or a project-specific site. This can help increase your works impact and give you a platform to showcase your work.</p>"},{"location":"clusters-at-yale/guides/github_pages/#further-reading","title":"Further Reading","text":"<ul> <li>Jekyll: the tool that powers GitHub Pages</li> <li>GitHub Learning Lab</li> <li>Academic Pages: forkable template for academic websites</li> <li>Jekyll Academic</li> </ul>"},{"location":"clusters-at-yale/guides/github_pages/#example-github-pages-websites","title":"Example GitHub Pages Websites","text":"<ul> <li>GitHub and Government, https://github.com/github/government.github.com</li> <li>ElectronJS, https://github.com/electron/electronjs.org</li> <li>Twitter GitHub, https://github.com/twitter/twitter.github.io</li> <li>React, https://github.com/facebook/react</li> </ul>"},{"location":"clusters-at-yale/guides/gpus-cuda/","title":"GPUs and CUDA","text":"<p>There are GPUs available for general use on the YCRC clusters. In order to use them, you must request them for your job. See the Grace, McCleary, and Milgram pages for hardware and partition specifics. Please do not use nodes with GPUs unless your application or job can make use of them. Any jobs submitted to a GPU partition without having requested a GPU may be terminated without warning.</p>"},{"location":"clusters-at-yale/guides/gpus-cuda/#monitor-activity-and-drivers","title":"Monitor Activity and Drivers","text":"<p>The CUDA libraries you load will allow you to compile code against them. To run CUDA-enabled code you must also be running on a node with a gpu allocated and a compatible driver installed. The minimum driver versions are listed on this nvidia developer site.</p> <p>You can check the available GPUs, their current usage, installed version of the nvidia drivers, and more with the command <code>nvidia-smi</code>. Either in an interactive job or after connecting to a node running your job with <code>ssh</code>,  <code>nvidia-smi</code> output should look something like this:</p> <pre><code>[user@gpu01 ~]$ nvidia-smi\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 108...  On   | 00000000:02:00.0 Off |                  N/A |\n| 23%   34C    P8     9W / 250W |      1MiB / 11178MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre> <p>Here we see that the node <code>gpu01</code> is running driver version 460.32.03 and is compatible with CUDA version 11.2. There are no processes using the GPU allocated to this job.</p>"},{"location":"clusters-at-yale/guides/gpus-cuda/#software","title":"Software","text":""},{"location":"clusters-at-yale/guides/gpus-cuda/#cuda-cudnn-tensorflow-and-pytorch-availability-on-cluster","title":"Cuda, cuDNN, tensorflow, and pytorch availability on cluster","text":"<p>We have built certain versions of CUDA, cuDNN, tensorflow, and pytorch on all the clusters YCRC maintains. If one of the versions of these modules aligns with the version needed for your research, then there may be no need to install these programs yourself. To list all the modules available for these programs:</p> <pre><code>module avail cuda/\nmodule avail cudnn/\nmodule avail tensorflow\nmodule avail pytorch\n</code></pre>"},{"location":"clusters-at-yale/guides/gpus-cuda/#tensorflow","title":"Tensorflow","text":"<p>Instructions for installing tensorflow on our clusters can be found here</p>"},{"location":"clusters-at-yale/guides/gpus-cuda/#pytorch","title":"PyTorch","text":"<p>Instructions for installing PyTorch on our clusters can be found here</p>"},{"location":"clusters-at-yale/guides/gpus-cuda/#compile-c-or-cpp-files-with-cuda-code","title":"Compile <code>.c</code> or <code>.cpp</code> Files with CUDA code","text":"<p>By default, <code>nvcc</code> expects that host code is in files with a <code>.c</code> or <code>.cpp</code> extension, and device code is in files with a <code>.cu</code> extension. When you mix device code in a <code>.c</code> or <code>.cpp</code> file with host code, the device code will not be recoganized by <code>nvcc</code> unless you add this flag: <code>-x cu</code>.  </p> <pre><code>nvcc -x cu mycuda.cpp -o mycuda.exe\n</code></pre>"},{"location":"clusters-at-yale/guides/isca/","title":"Isca","text":"<p>Isca is a framework used for idealized global circulation modelling. We recommend that you install it for yourself individually as the code expects to be able to modify its source code files. It is relatively straighforward to install into a conda environment as described below.</p>"},{"location":"clusters-at-yale/guides/isca/#install-isca","title":"Install Isca","text":"<p>Install it for just your user as a Python conda environment called \"isca\".</p> <pre><code>module load netCDF-Fortran/4.5.3-gompi-2020b\n\nmodule load miniconda  \n\nmodule save isca  \nmkdir ~/programs  \ncd ~/programs  \ngit clone https://www.github.com/execlim/isca.git\nconda create -n isca python=3.7\n\nconda activate isca\n\nconda install tqdm  \n\ncd isca/src/extra/python  \npip install -e .\n</code></pre> <p>Then add the following to your <code>.bashrc</code> file</p> <pre><code># Isca\n\n# directory of the Isca source code\nexport GFDL_BASE=$HOME/programs/isca\n# \"environment\" configuration for grace\nexport GFDL_ENV=gfortran\n# temporary working directory used in running the model\nexport GFDL_WORK=$PALMER_SCRATCH/gfdl_work\n# directory for storing model output\nexport GFDL_DATA=$GIBBS_PROJECT/gfdl_data\n</code></pre>"},{"location":"clusters-at-yale/guides/isca/#update-the-environment","title":"Update the Environment","text":"<p>Open <code>$HOME/programs/isca/postprocessing/mppnccombine_run.sh</code> and remove the following line:</p> <pre><code>module purge\n</code></pre>"},{"location":"clusters-at-yale/guides/isca/#select-an-experiment-and-update-the-flags","title":"Select an Experiment and Update the Flags","text":"<p>We are using GCC version 10.x for this build, so a slight modification needs to made to Isca for it to build. Add the following line to the experiment script (e.g. <code>$GFDL_BASE/exp/test_cases/held_suarez/held_suarez_test_case.py</code>), after <code>cb</code> is defined (so about line 13 in that file).</p> <pre><code>cb.compile_flags.extend(['-fallow-argument-mismatch', '-fallow-invalid-boz'])\n</code></pre>"},{"location":"clusters-at-yale/guides/isca/#run-isca","title":"Run Isca","text":"<p>The above commands only need to be run once to set everything up. To use it, you will first always need to run:</p> <pre><code>module restore isca\nconda activate isca\n</code></pre> <p>Then you should be able to compile and launch your ISCA models.</p>"},{"location":"clusters-at-yale/guides/jupyter/","title":"Jupyter Notebooks","text":""},{"location":"clusters-at-yale/guides/jupyter/#open-ondemand","title":"Open OnDemand","text":"<p>We provide a simple way to start Jupyter Notebook interfaces for Python and R using Open OnDemand. Jupyter notebooks provide a flexible way to interactively work with code and plots presented in-line together. To get started choose Jupyter Notebook from the OOD Interactive Apps menu or click on the link on the dashboard.</p> <p>Before you get started, you will need to be on campus or logged in to the Yale VPN and you will need to set up a Jupyter environment.</p>"},{"location":"clusters-at-yale/guides/jupyter/#set-up-an-environment","title":"Set up an environment","text":"<p>We recommend you use miniconda to manage your Jupyter environments. You can create Conda environments from the OOD shell interface or from a terminal-based login to the clusters. For example, if you want to create an environment with many commonly used scientific computing Python packages you would run:</p> <pre><code>module load miniconda\nconda create -y -n notebook_env python jupyter numpy pandas matplotlib\n</code></pre>"},{"location":"clusters-at-yale/guides/jupyter/#specify-your-resource-request","title":"Specify your resource request","text":"<p>You can use the <code>ycrc_default</code> environment or chose one of your own from the drop-down menu. After specifying the required resources (number of CPUs/GPUs, amount of RAM, etc.), you can submit the job. When it launches you can open the standard Jupyter interface where you can start working with notebooks.</p> <p>Tip</p> <p>If you have installed and want to use Jupyter Lab click the <code>Start JupyterLab</code> checkbox.</p> <p> If there is a specific workflow which OOD does not satisfy, let us know and we can help.</p>"},{"location":"clusters-at-yale/guides/jupyter/#command-line-execution-of-jupyter-notebooks","title":"Command-Line Execution of Jupyter Notebooks","text":"<p>Many scientific workflows start as interactive Jupyter notebooks, and our Open OnDemand portal has dramatically simplified deploying these notebooks on cluster resources.  However, the step from running notebooks interactively to running jobs as a batch script can be challenging and is often a barrier to migrating to using <code>sbatch</code> to run workflows non-interactively.</p> <p>To help solve this problem, there are a handful of utilities that can execute a notebook as if you were manually hitting \"shift-Enter\" for each cell. Of note is Papermill which provides a powerful set of tools to bridge between interactive and batch-mode computing.</p> <p>To get started, install papermill into your <code>conda</code> environments:</p> <pre><code>module load miniconda\nconda activate my_env\nconda install papermill\n</code></pre> <p>Then you can simply evaluate a notebook, preserving figures and output inside the notebook, like this:</p> <pre><code>papermill /path/to/notebook.ipynb /path/to/output.ipynb\n</code></pre> <p>This can be run inside a batch job that might look like this:</p> <pre><code>#!/bin/bash\n#SBATCH -p day\n#SBATCH -c 1\n#SBATCH -t 6:00:00\n\nmodule purge\nmodule load miniconda\nconda activate my_env\npapermill /path/to/notebook.ipynb /path/to/output.ipynb\n</code></pre> <p>Variables can also be parameterized and passed in as command-line options so that you can run multiple copies simultaneously with different input variables. For more information see the Papermill docs pages.</p>"},{"location":"clusters-at-yale/guides/jupyter_ssh/","title":"Jupyter Notebooks over SSH Port Forwarding","text":"<p>If you want finer control over your notebook job, or wish to use something besides <code>conda</code> for your Python environment, you can manually configure a Jupyter notebook and connect manually.</p> <p>The main steps are:</p> <ol> <li>Start a Jupyter notebook job.</li> <li>Start an ssh tunnel.</li> <li>Use your local browser to connect.</li> </ol>"},{"location":"clusters-at-yale/guides/jupyter_ssh/#start-the-server","title":"Start the Server","text":"<p>Here is a template for submitting a jupyter-notebook server as a batch job.  You may need to edit some of the slurm options, including the time limit or the partition.  You will also need to either load a module that contains <code>jupyter-notebook</code>.</p> <p>Tip</p> <p>If you are using a Conda environment, please follow the instructions for launching a Jupyter session via Open OnDemand.</p> <p>Save your edited version of this script on the cluster, and submit it with <code>sbatch</code>.</p> <pre><code>#!/bin/bash\n#SBATCH --partition devel\n#SBATCH --cpus-per-task 1\n#SBATCH --mem-per-cpu 8G\n#SBATCH --time 6:00:00\n#SBATCH --job-name jupyter-notebook\n#SBATCH --output jupyter-notebook-%J.log\n\n# get tunneling info\nXDG_RUNTIME_DIR=\"\"\nport=$(shuf -i8000-9999 -n1)\nnode=$(hostname -s)\nuser=$(whoami)\ncluster=$(hostname -f | awk -F\".\" '{print $2}')\n\n# print tunneling instructions jupyter-log\necho -e \"\nFor more info and how to connect from windows,\n   see https://docs.ycrc.yale.edu/clusters-at-yale/guides/jupyter/\nMacOS or linux terminal command to create your ssh tunnel\nssh -N -L ${port}:${node}:${port} ${user}@${cluster}.ycrc.yale.edu\nWindows MobaXterm info\nForwarded port:same as remote port\nRemote server: ${node}\nRemote port: ${port}\nSSH server: ${cluster}.ycrc.yale.edu\nSSH login: $user\nSSH port: 22\nUse a Browser on your local machine to go to:\nlocalhost:${port}  (prefix w/ https:// if using password)\n\"\n\n# load modules or conda environments here\n\njupyter-notebook --no-browser --port=${port} --ip=${node}\n</code></pre>"},{"location":"clusters-at-yale/guides/jupyter_ssh/#start-the-tunnel","title":"Start the Tunnel","text":"<p>Once you have submitted your job and it starts, your notebook server will be ready for you to connect.  You can run <code>squeue -u${USER}</code> to check. You will see an \"R\" in the ST or status column for your notebook job if it is running.  If you see a \"PD\" in the status column, you will have to wait for your job to start running to connect.  The log file with information about how to connect will be in the directory you submitted the script from, and be named jupyter-notebook-[jobid].log where jobid is the slurm id for your job.</p>"},{"location":"clusters-at-yale/guides/jupyter_ssh/#macos-and-linux","title":"MacOS and Linux","text":"<p>On a Mac or Linux machine, you can start the tunnel with an SSH command.  You can check the output from the job you started to get the specifc info you need.</p>"},{"location":"clusters-at-yale/guides/jupyter_ssh/#windows","title":"Windows","text":"<p>On a Windows machine, we recommend you use MobaXterm.  See our guide on connecting with MobaXterm for instructions on how to get set up.  You will need to take a look at your job's log file to get the details you need.  Then start MobaXterm:</p> <ol> <li>Under Tools choose \"MobaSSHTunnel (port forwarding)\".</li> <li>Click the \"New SSH Tunnel\" button.</li> <li>Click the radio button for \"Local port forwarding\".</li> <li>Use the information in your jupyter notebook log file to fill out the boxes.</li> <li>Click Save.</li> <li>On your new tunnel, click the key symbol under the settings column and choose your ssh private key.</li> <li>Click the play button under the Start/Stop column.</li> </ol>"},{"location":"clusters-at-yale/guides/jupyter_ssh/#browse-the-notebook","title":"Browse the Notebook","text":"<p>Finally, open a web browser on your local machine and enter the address <code>http://localhost:port</code> where port is the one specified in your log file.  The address Jupyter creates by default (the one with the name of a compute node) will not work outside the cluster's network.  Since version 5 of jupyter, the notebook will automatically generate a token that allows you to authenticate when you connect.  It is long, and will be at the end of the url jupyter generates.  It will look something like</p> <p><code>http://c14n06:9230/?token=**ad0775eaff315e6f1d98b13ef10b919bc6b9ef7d0605cc20**</code></p> <p>If you run into trouble or need help, contact us.</p>"},{"location":"clusters-at-yale/guides/mathematica/","title":"Mathematica","text":""},{"location":"clusters-at-yale/guides/mathematica/#open-ondemand","title":"Open OnDemand","text":"<p>We strongly recommend using Open OnDemand to launch Mathematica. </p> <p>First, open OOD in a browser and navigate to the <code>Apps</code> button. Select <code>All Apps</code> from the drop-down menu and then select Mathematica from the list. Fill in your resource requests and launch your job. Once started, click <code>Launch Mathematica</code> and Mathematica will be opened in a new tab in the browser.</p>"},{"location":"clusters-at-yale/guides/mathematica/#interactive-job","title":"Interactive Job","text":"<p>Alternatively, you could start an interacgive session with X11 forwarding. </p> <p>Warning</p> <p>The Mathematica program is too large to fit on a login node. If you try to run it there, it will crash. Instead, launch it in an interactive job (see below).</p> <p>To run\u00a0Mathematica\u00a0interactively, you need to request an interactive session on a compute node.</p> <p>You could start an interactive session using Slurm. For example, to use 4 cores on 1 node:</p> <pre><code>salloc --x11 -c 4 -t 4:00:00\n</code></pre> <p>Note that if you are on macOS, you will need to install an additional program to use the GUI. See our X11 Forwarding documentation for instructions.</p> <p>See our Slurm documentation for more detailed information on requesting resources for interactive jobs.</p> <p>To launch Mathematica, you will first need to make sure you have the correct module loaded. You can search for all available Mathematica versions:</p> <pre><code>module avail mathematica\n</code></pre> <p>Load the appropriate module file. For example, to run version\u00a012.0.0:</p> <pre><code>module load Mathematica/12.0.0\n</code></pre> <p>The module load command sets up your environment, including the PATH to find the proper version of the\u00a0Mathematica\u00a0program. If you would like to avoid running the load command every session, you can run <code>module save</code> and then the Mathematica module will be loaded every time you login.</p> <p>Once you have the appropriate module loaded in an interactive job, start Mathematica. The <code>&amp;</code> will put the program in the background so you can continue to use your terminal session.</p> <pre><code>Mathematica &amp;\n</code></pre>"},{"location":"clusters-at-yale/guides/mathematica/#configure-environment-for-parallel-jobs","title":"Configure Environment for Parallel Jobs","text":"<p>Mathematica installed on Yale HPC clusters includes our proprietary scripts to run parallel jobs in SLURM environments. These scripts are designed in a way to allow users to access up to 450 parallel kernels.</p> <p>When a user asks for a specific number of kernels, the wait time to get them might differ dramatically depending on requested computing resources as well as on how busy the HPC cluster is at that moment. To reduce waiting time, our scripts try to launch as many kernels as possible at the moment the user asks for them. Most of the time you will not get launched with the same number of kernels as you requested. We recommend checking the final number of parallel kernels you\u2019ve gotten after the launching command has completed no matter if you run a Front End Mathematica session or execute Wolfram script. One of the ways to check this would be the Mathematica command <code>Length[Kernels[]]</code>.</p> <p>In order to run parallel Mathematica jobs on our cluster, you will need to configure your Mathematica environment. You have to do this within a Front End session. If you run Wolfram script you need to run a Front End session to set your parallel environment before executing your script.</p> <p>Once Mathematica is started, open a new document in the Mathematica window and go to <code>Edit</code> &gt; <code>Preferences</code>. From there, go to <code>Evaluate/Parallel</code> Kernel Configuration and change the following settings:</p> <ol> <li>Under <code>Local Kernels</code>, disable <code>Local Kernels</code> if it is enabled</li> <li>Go in <code>Cluster Integration</code> and first <code>enable cluster integration</code> it if it is not enabled</li> <li>Under the <code>Cluster Integration</code> tab, expand the <code>Advanced Settings</code> arrow. When you configure parallel kernels for the first time, please select <code>SLURM</code> from the <code>Cluster Engine</code> pull-down menu<ul> <li>Matching parallel kernel versions with your main Mathematica version is important, especially if you\u2019ve already had SLURM selected by running different Mathematica versions previously (you might see different versions in Kernel program) In this case, select Windows CCS from Cluster Engine and a red error will appear in Advanced Settings. After this select SLURM again as this should set the correct engine for you.</li> </ul> </li> <li>Under <code>Kernels</code>, set your desired number (we recommend to set it lower first to test)</li> <li>In <code>Advanced Settings</code> under <code>Native specification</code>, specify time and RAM per kernel, such as <code>\u2014time=02:00:00 \u2014mem=20G</code> (please note that this is RAM per one kernel)</li> <li>If you are using Mathematica 12.3 and above, and if <code>RemoteKernel Objects</code> is enabled, disable it and restart your Mathematica session</li> <li>We recommend to use these commands to start kernels and to check how many kernels have actually been launched (please keep them in the same Mathematica cell and separate by semicolons; Do not use semicolon at the end) <pre><code>$DefaultKernels=$ConfiguredKernels; LaunchKernels[]; Length[Kernels[]]\n</code></pre></li> </ol>"},{"location":"clusters-at-yale/guides/mathematica/#request-help-or-access-to-wolfram-alpha-pro","title":"Request Help or Access to Wolfram Alpha Pro","text":"<p>If you need any assistance with your Mathematica program, contact us.</p>"},{"location":"clusters-at-yale/guides/matlab/","title":"MATLAB","text":""},{"location":"clusters-at-yale/guides/matlab/#matlab-gui","title":"MATLAB GUI","text":"<p>To use the MATLAB GUI, we recommend our web portal, Open OnDemand. Once logged in, click MATLAB pinned on the dashboard, or select \"MATLAB\" from the \"Interactive Apps\" list.</p>"},{"location":"clusters-at-yale/guides/matlab/#command-line-matlab","title":"Command Line MATLAB","text":""},{"location":"clusters-at-yale/guides/matlab/#find-matlab","title":"Find MATLAB","text":"<p>Run one of the commands below, which will list available versions and the corresponding module files:</p> <pre><code>module avail matlab\n</code></pre> <p>Load the appropriate module file. For example, to run version\u00a0R2021a:</p> <pre><code>module load MATLAB/2021a\n</code></pre> <p>The module load command sets up your environment, including the PATH to find the proper version of the\u00a0MATLAB\u00a0program.</p>"},{"location":"clusters-at-yale/guides/matlab/#run-matlab","title":"Run MATLAB","text":"<p>Warning</p> <p>If you try to run MATLAB on a login node, it will likely crash. Instead, launch it in an interactive or batch job (see below).</p>"},{"location":"clusters-at-yale/guides/matlab/#interactive-job-without-a-gui","title":"Interactive Job (without a GUI)","text":"<p>To run\u00a0MATLAB\u00a0interactively, you need to create an interactive session on a compute node.</p> <p>You could start an interactive session using 4 cores, 16GiB of RAM for 4 hours with:</p> <pre><code>salloc -c 4 --mem 16G -t 4:00:00\n</code></pre> <p>Once your interactive session starts, you can load the appropriate module file and start MATLAB</p> <pre><code>module load MATLAB/2021a\n\n# launch the MATLAB command line prompt\nmaltab -nodisplay\n\n# launch a script on the command line\nmatlab -nodisplay &lt; runscript.m\n</code></pre> <p>See our Slurm documentation for more detailed information on requesting resources for interactive jobs.</p>"},{"location":"clusters-at-yale/guides/matlab/#batch-mode-without-a-gui","title":"Batch Mode (without a GUI)","text":"<p>Create a batch script with the resource requests appropriate to your MATLAB function(s) and script(s). In it load the MATLAB module version you want, then run <code>matlab</code> with the <code>-b</code> option and your function/script name. Here is an example that requests 4 CPUs and 18GiB of memory for 8 hours: </p> <pre><code>#!/bin/bash\n#SBATCH --job-name myjob\n#SBATCH --cpus-per-task 4\n#SBATCH --mem 18G\n#SBATCH -t 8:00:00\n\nmodule load MATLAB/2021a\n# assuming you have your_script.m in the current directory\nmatlab -batch \"your_script\"\n\n# if using MATLAB older than R2019a\n# matlab -nojvm -nodisplay -nosplash &lt; your_script.m\n</code></pre>"},{"location":"clusters-at-yale/guides/matlab/#using-more-than-12-cores-with-matlab","title":"Using More than 12 Cores with MATLAB","text":"<p>In MATLAB, 12 workers is a poorly documented default limit (seemingly for historical reasons) when setting up the parallel environment. You can override it by explicitly setting up your parpool before calling parfor or other parallel functions.</p> <pre><code>parpool(feature('NumCores'));\n</code></pre>"},{"location":"clusters-at-yale/guides/matlab_compile/","title":"Compile MATLAB program","text":"<p>By compiling your MATLAB code into standalone executables, you eliminate the need to load the MATLAB module each time you run the code. This can be particularly beneficial on clusters where loading the MATLAB module involve loading all the installed packages and can be time-consuming. This can reduce the startup time and improve the overall performance of executing the code. If you choose to compile your MATLAB code into standalone executables, one disadvantage is that every time you make changes to your program, you will need to recompile it. Recompiling is necessary to ensure that any modifications or updates you\u2019ve made to the code are incorporated into the executable. </p>"},{"location":"clusters-at-yale/guides/matlab_compile/#compile-matlab","title":"Compile MATLAB","text":"<p>When compiling MATLAB code, the compilation process generates several additional files. To maintain organization in your current directory, it is recommended to create a new directory specifically for the compiled files.</p> <p>For example, to compile function1.m, function2.m, and function3.m, where function1.m is the main MATLAB function you call: </p> <pre><code>mkdir compiled_scripts\ncd compiled_scripts\nsalloc \nmodule load MATLAB/2022b\nmcc -m function1 function2 function3\n</code></pre> <p>This will generate multiple files, including the executable named  <code>function1</code>. Please note that there are functions that are not supported by MATLAB compiler.</p>"},{"location":"clusters-at-yale/guides/matlab_compile/#run-the-executable","title":"Run the Executable","text":"<p>Load the MATLAB Runtime and GCCcore modules to run the executable. Always ensure that you load the same version of the MATLAB Runtime as the version of MATLAB you used for compilation.</p> <pre><code>module load MCR/R2022b.5 GCCCore \n./function1 &lt;argument_list&gt; \n</code></pre> <p>The argument list is the list of input parameters for function1.m.</p>"},{"location":"clusters-at-yale/guides/matlab_compile/#job-arrays","title":"Job Arrays","text":"<p>If you are running a large number of jobs with job arrays, we recommend setting up a unique directory for each job to use as a cache location. You can use the /tmp space on the compute node. Here is an example of a batch script:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name myjob\n#SBATCH --array 1-100\n#SBATCH --mem 5G\n#SBATCH -t 1:00:00\n\nmcr_cache_root=/tmp/$USER/MCR_CACHE_ROOT_${SLURM_ARRAY_JOB_ID}_${SLURM_ARRAY_TASK_ID}\nmkdir -p $mcr_cache_root\nexport MCR_CACHE_ROOT=$mcr_cache_root\n\nmodule load MCR/2022b.5 GCCcore \n./function1.sh ${SLURM_ARRAY_TASK_ID}\n</code></pre>"},{"location":"clusters-at-yale/guides/mpi4py/","title":"MPI Parallelism with Python","text":"<p>Note</p> <p>Before venturing into MPI-based parallelism, consider whether your work can be resturctured to make use of dSQ or more \"embarrassingly parallel\" workflows. MPI can be thought of as a \"last resort\" for parallel programming.</p> <p>There are many computational problems that can be have increased performance by running pieces in parallel.  These often require communication between the different steps and need a way to send messages between processes.</p> <p>Examples of this include simulations of galaxy formation and electric field simulations, analysis of a single large dataset, or complex <code>search</code> or <code>sort</code> algorithms.</p>"},{"location":"clusters-at-yale/guides/mpi4py/#mpi-and-mpi4py","title":"MPI and <code>mpi4py</code>","text":"<p>There is a standard protocol, called <code>MPI</code>, that defines how messages are passed between processes, including one-to-one and broadcast communications.</p> <p>The Python module for this is called <code>mpi4py</code>:</p> <p>mpi4py Read The Docs</p> <p>Message Passing Interface implemented for Python.</p> <p>Supports point-to-point (sends, receives) and collective (broadcasts, scatters, gathers) communications of any picklable Python object, as well as optimized communications of Python object exposing the single-segment buffer interface (NumPy arrays, builtin bytes/string/array objects)</p> <p>We will go over a few simple examples here.</p>"},{"location":"clusters-at-yale/guides/mpi4py/#definitions","title":"Definitions","text":"<p><code>COMM</code>: The communication \"world\" defined by MPI</p> <p><code>RANK</code>: an ID number given to each internal process to define communication</p> <p><code>SIZE</code>: total number of processes allocated</p> <p><code>BROADCAST</code>: One-to-many communication</p> <p><code>SCATTER</code>: One-to-many data distribution</p> <p><code>GATHER</code>: Many-to-one data distribution</p>"},{"location":"clusters-at-yale/guides/mpi4py/#mpi4py-on-the-clusters","title":"<code>mpi4py</code> on the clusters","text":"<p>On the clusters, the easiest way to start using <code>mpi4py</code> is to use the module-based software for OpenMPI and Python:</p> <pre><code># toolchains 2020b and before\nmodule load SciPy-bundle/2020.11-foss-2020b\n\n# toolchains starting with 2022b\nmodule load mpi4py/3.1.4-gompi-2022b\n</code></pre> <p>Warning</p> <p><code>mpi4py</code> installed via Conda is unaware of the cluster infrastructure and therefore will likely only work on a single compute node. If you wish to get a conda environment working across multiple nodes, please reach out to hpc@yale.edu for assistance.</p>"},{"location":"clusters-at-yale/guides/mpi4py/#cluster-resource-requests","title":"Cluster Resource Requests","text":"<p>MPI utilizes Slurm tasks as the individual parallel workers.  Therefore, when requesting resources (either interactively or in batch-mode) the number of tasks will determine the number of parallel workers (or to use MPI's language, the <code>SIZE</code> of the <code>COMM World</code>).</p> <p>To request four tasks (each with a single CPU) interactively run the following:</p> <pre><code>salloc --cpus-per-task=1 --ntasks=4 \n</code></pre> <p>This can also be achieved in batch-mode by including the following directives in your submission script:</p> <pre><code>#SBATCH --cpus-per-task=1\n#SBATCH --ntasks=4\n</code></pre> <p>A more detailed discussion of resource requests can be found here and further examples are available here.</p>"},{"location":"clusters-at-yale/guides/mpi4py/#examples","title":"Examples","text":""},{"location":"clusters-at-yale/guides/mpi4py/#ex-1-rank","title":"Ex 1: Rank","text":"<p>This is a simple example where each worker reports their <code>RANK</code> and the process ID running that particular task.</p> <pre><code>from mpi4py import MPI\n\n# instantize the communication world\ncomm = MPI.COMM_WORLD\n\n# get the size of the communication world\nsize = comm.Get_size()\n\n# get this particular processes' `rank` ID\nrank = comm.Get_rank()\n\nPID = os.getpid()\n\nprint(f'rank: {rank} has PID: {PID}')\n</code></pre> <p>We then execute this code (named <code>mpi_simple.py</code>) by running the following on the command line:</p> <pre><code>mpirun -n 4 python mpi_simple.py\n</code></pre> <p>The <code>mpirun</code> command is a wrapper for the MPI interface.</p> <p>Then we tell that to set up a <code>COMM_WORLD</code> with 4 workers.</p> <p>Finally we tell <code>mpirun</code> to run <code>python mpi_simple.py</code> on each of the four workers.</p> <p>Which outputs the following:</p> <pre><code>rank: 0 has PID: 89134\nrank: 1 has PID: 89135\nrank: 2 has PID: 89136\nrank: 3 has PID: 89137\n</code></pre>"},{"location":"clusters-at-yale/guides/mpi4py/#ex-2-point-to-point-communicators","title":"Ex 2: Point to Point Communicators","text":"<p>The most basic communication operators are \"<code>send</code>\" and \"<code>recv</code>\". These can be a bit tricky since they are \"blocking\" commands and can cause the program to hang.</p> <pre><code>comm.send(obj, dest, tag=0)\ncomm.recv(source=MPI.ANY_SOURCE, tag=MPI.ANY_TAG, status=None)\n</code></pre> <ul> <li><code>tag</code> can be used as a filter</li> <li><code>dest</code> must be a rank in the current communicator</li> <li><code>source</code> can be a rank or a wild-card (<code>MPI.ANY_SOURCE</code>)</li> <li><code>status</code> used to retrieve information about recv'd message</li> </ul> <p>We now we create a file (<code>mpi_comm.py</code>) that contains the following:</p> <pre><code>from mpi4py import MPI\ncomm = MPI.COMM_WORLD\nsize = comm.Get_size()\nrank = comm.Get_rank()\n\nif rank == 0:\n    msg = 'Hello, world'\n    comm.send(msg, dest=1)\nelif rank == 1:\n    s = comm.recv()\n    print(f\"rank {rank}: {s}\")\n</code></pre> <p>When we run this on the command line (<code>mpirun -n 4 python mpi_comm.py</code>) we get the following:</p> <pre><code>rank 1: Hello, world\n</code></pre> <p>The <code>RANK=0</code> process sends the message, and the <code>RANK=1</code> process receives it. The other two processes are effectively bystanders in this example.</p>"},{"location":"clusters-at-yale/guides/mpi4py/#ex-3-broadcast","title":"Ex 3: Broadcast","text":"<p>Now we will try a slightly more complicated example that involves sending messages and data between processes. <pre><code># Import MPI\nfrom mpi4py import MPI\n\n# Define world\ncomm = MPI.COMM_WORLD\nsize = comm.Get_size()\nrank = comm.Get_rank()\n\n# Create some data in the RANK_0 worker\nif rank == 0:\n    data = {'key1' : [7, 2.72, 2+3j], 'key2' : ( 'abc', 'xyz')}\nelse:\n    data = None\n\n# Broadcast the data from RANK_0 to all workers\ndata = comm.bcast(data, root=0)\n\n# Append the RANK ID to the data\ndata['key1'].append(rank)\n\n# Print the resulting data\nprint(f\"Rank: {rank}, data: {data}\")\n</code></pre></p> <p>We then execute this code (named <code>mpi_message.py</code>) by running the following on the command line:</p> <pre><code>mpirun -n 4 python mpi_message.py\n</code></pre> <p>Which outputs the following:</p> <pre><code>Rank: 0, data: {'key1': [7, 2.72, (2+3j), 0], 'key2': ('abc', 'xyz')}\nRank: 2, data: {'key1': [7, 2.72, (2+3j), 2], 'key2': ('abc', 'xyz')}\nRank: 3, data: {'key1': [7, 2.72, (2+3j), 3], 'key2': ('abc', 'xyz')}\nRank: 1, data: {'key1': [7, 2.72, (2+3j), 1], 'key2': ('abc', 'xyz')}\n</code></pre>"},{"location":"clusters-at-yale/guides/mpi4py/#ex-4-scatter-and-gather","title":"Ex 4: Scatter and Gather","text":"<p>An effective way of distributing computationally intensive tasks is to <code>scatter</code> pieces of a large dataset to each task. The separate tasks perform some analysis on their chunk of data and then the results are <code>gathered</code> by <code>RANK_0</code>.</p> <p>This example takes a large array of random numbers and splits it into pieces for each task. These smaller datasets are analyzed (taking an average in this example) and the results are returns to the main task with a <code>Gather</code> call.</p> <pre><code># import libraries\nfrom mpi4py import MPI\nimport numpy as np\n\n# set up MPI world\ncomm = MPI.COMM_WORLD\nsize = comm.Get_size() # new: gives number of ranks in comm\nrank = comm.Get_rank()\n\n# generate a large array of data on RANK_0\nnumData = 100000000 # 100milion values each\ndata = None\nif rank == 0:\n    data = np.random.normal(loc=10, scale=5, size=numData)\n\n# initialize empty arrays to receive the partial data\npartial = np.empty(int(numData/size), dtype='d')\n\n# send data to the other workers\ncomm.Scatter(data, partial, root=0)\n\n# prepare the reduced array to receive the processed data\nreduced = None\nif rank == 0:\n    reduced = np.empty(size, dtype='d')\n\n# Average the partial arrays, and then gather them to RANK_0\ncomm.Gather(np.average(partial), reduced, root=0)\n\nif rank == 0:\n    print('Full Average:',np.average(reduced))\n</code></pre> <p>This is executed on the command line:</p> <pre><code>mpirun -n 4 python mpi/mpi_scatter.py\n</code></pre> <p>Which prints:</p> <pre><code>Full Average: 10.00002060397186\n</code></pre>"},{"location":"clusters-at-yale/guides/mpi4py/#key-take-aways-and-further-reading","title":"Key Take-aways and Further Reading","text":"<ol> <li><code>MPI</code> is a powerful tool to set up communication worlds and send data and messages between workers</li> <li>The <code>mpi4py</code> module provides tools for using MPI within Python.</li> <li>This is just the beginning, <code>mpi4py</code> can be used for so much more...</li> </ol> <p>To learn more, take a look at the <code>mpi4py</code> tutorial here.</p>"},{"location":"clusters-at-yale/guides/mysql/","title":"Mysql","text":"<p>Mysql is a popular relational database.  Because a database is usually thought of as a persistent service, it is not ordinarily run on HPC clusters, since allocations on an HPC cluster are temporary.  If you need a persistent mysql database server, we recommend either installing mysql on a server in your lab, or using ITS's Spinup service.  In either case, the mysql server can be accessed remotely from the HPC clusters.</p> <p>Spinup has serverless database servers (mysql and postgres) that can automatically sleep when not being accessed.  While asleep, you only pay for the data storage. </p> <p>However, there are some use cases for running a mysql server on the cluster that do make sense.  For example, some applications store their data in a  mysql database that only needs to run when the application runs.  Most instructions for installing mysql involve creating a persistent server and  require admin privileges.  The instructions that follow walk you through the process of running a mysql server using Apptainer on a cluster compute node  without any special privileges.  It uses an Apptainer container  developed by Robert Grandin at Iowa State (Thanks!) </p> <p>All of the following must be done on an allocated compute node.  Do not do this on the login node!</p>"},{"location":"clusters-at-yale/guides/mysql/#step-1-create-an-installation-directory-somewhere-and-cd-to-it","title":"Step 1: Create an installation directory somewhere, and cd to it","text":"<pre><code>mkdir ~/project/mysql\ncd ~/project/mysql\n</code></pre>"},{"location":"clusters-at-yale/guides/mysql/#step-2-create-two-config-files","title":"Step 2: Create two config files","text":"<p>Put the following in ~/.my.cnf.  Note that you should change the password in both files to something else.</p> <pre><code>[mysqld]\ninnodb_use_native_aio=0\ninit-file=${HOME}/.mysqlrootpw\n\n[client]\nuser=root\npassword='my-secret-pw'\n</code></pre> <p>Put the following in ~/.mysqlrootpw <pre><code>SET PASSWORD FOR 'root'@'localhost' = PASSWORD('my-secret-pw');\n</code></pre></p>"},{"location":"clusters-at-yale/guides/mysql/#step-3-create-data-directories-for-mysql","title":"Step 3: Create data directories for mysql","text":"<pre><code>mkdir -p ${PWD}/mysql/var/lib/mysql ${PWD}/mysql/run/mysqld\n</code></pre>"},{"location":"clusters-at-yale/guides/mysql/#step-4-make-a-link-to-the-mysql-image-file","title":"Step 4: Make a link to the mysql image file","text":"<p>The mysqld image file can be found under the apps tree on each cluster. For example, on Grace:</p> <pre><code>/vast/palmer/apps/apptainer/images/mysqld-5.7.21.simg\n</code></pre> <p>We recommend that you make a link to it in your mysql directory:</p> <pre><code>ln -s /vast/palmer/apps/apptainer/images/mysqld-5.7.21.simg mysql.simg\n</code></pre>"},{"location":"clusters-at-yale/guides/mysql/#step-5-start-the-container-note-that-this-doesnt-actually-start-the-service-yet","title":"Step 5: Start the container.  Note that this doesn't actually start the service yet.","text":"<pre><code>apptainer instance start --bind ${HOME} \\\n    --bind ${PWD}/mysql/var/lib/mysql/:/var/lib/mysql \\\n    --bind ${PWD}/mysql/run/mysqld:/run/mysqld \\\n    ./mysql.simg mysql\n</code></pre> <p>To check that it is running:</p> <pre><code>apptainer instance list\n</code></pre>"},{"location":"clusters-at-yale/guides/mysql/#step-6-start-the-mysqld-server-within-the-container","title":"Step 6: Start the mysqld server within the container","text":"<pre><code>apptainer run instance://mysql\n</code></pre> <p>You'll see lots of output, but at the end you should see a message like this <pre><code>2022-02-21T17:16:21.104527Z 0 [Note] mysqld: ready for connections.\nVersion: '5.7.21'  socket: '/var/run/mysqld/mysqld.sock'  port: 3306  MySQL Community Server (GPL)\n</code></pre></p>"},{"location":"clusters-at-yale/guides/mysql/#step-7-enter-the-running-container","title":"Step 7: Enter the running container","text":"<pre><code>apptainer exec instance://mysql /bin/bash\n</code></pre> <p>Connect locally as root user while in the container, using the password you set in the config files in step 2. <pre><code>Singularity&gt; mysql -u root -p\nEnter password:\nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 3\nServer version: 5.7.21 MySQL Community Server (GPL)\n\nCopyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.\n\nOracle is a registered trademark of Oracle Corporation and/or its\naffiliates. Other names may be trademarks of their respective\nowners.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nmysql&gt;\n</code></pre></p> <p>Success!  The server is working!  Type exit to get out of mysql, but remain in the container:</p>"},{"location":"clusters-at-yale/guides/mysql/#step-8-add-a-database-user-and-permit-it-to-login-remotely","title":"Step 8: Add a database user and permit it to login remotely","text":"<p>Next, in order to connect from outside the container, you need to add a user that is allowed to connect remotely and give that user permissions.  This is one way to do that from the container shell. </p> <p>You should probably substitute your name for elmerfudd and a better password for mypasswd!</p> <pre><code>mysql -e \"GRANT ALL PRIVILEGES ON *.* TO 'elmerfudd'@'%' IDENTIFIED BY 'mypasswd' WITH GRANT OPTION\"\nmysql -e \"FLUSH PRIVILEGES\"\n</code></pre> <p>Type exit to leave the container.  From that compute node, but outside the container, try connecting with: <pre><code>mysql -u elmerfudd -h 127.0.0.1 -p\n</code></pre></p> <p>Now try connecting to that server from a different compute node by using the hostname of the node where the server is running (e.g. c22n01) instead of 127.0.0.1 <pre><code>mysql -u elmerfudd -h c22n01 -p\n</code></pre></p> <p>While connected, you can try actually using the server in the usual way to create a database and table: <pre><code>MySQL [(none)]&gt; create database rob;\nQuery OK, 1 row affected (0.00 sec)\n\nMySQL [(none)]&gt; use rob\nDatabase changed\nMySQL [rob]&gt; create table users (name VARCHAR(20), id INT);\nQuery OK, 0 rows affected (0.11 sec)\n\n...\n</code></pre></p> <p>Success!  You've earned a reward of your choice!</p>"},{"location":"clusters-at-yale/guides/mysql/#step-9-shut-the-container-down","title":"Step 9 Shut the container down.","text":"<pre><code>apptainer instance stop mysql\n</code></pre> <p>Now that everything is installed, the next time you want to start the server, you'll only need to do steps 5 (starting the container) and 6 (starting the mysql server).</p> <p>Note that you'll run into a problem if two mysql instances are run on the same compute node, since by default they each try to use port 3306.  The simplest solution is to specify a non-standard port in your .my.cnf file:</p> <pre><code>[mysqld]\nport=3310\ninnodb_use_native_aio=0\ninit-file=${HOME}/.mysqlrootpw\n\n[client]\nport=3310\nuser=root\npassword='my-secret-pw'\n</code></pre>"},{"location":"clusters-at-yale/guides/namd/","title":"NAMD","text":"<p>NAMD is a parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems. NAMD scales to hundreds of cores for typical simulations. NAMD uses the popular molecular graphics program VMD, for simulation setup and trajectory analysis, but is also file-compatible with AMBER, CHARMM, and X-PLOR.To see a full list of available versions of NAMD on the cluster, run:</p> <pre><code>module avail namd/\n</code></pre> <p>As of this writing, the latest installed version is 2.13.</p>"},{"location":"clusters-at-yale/guides/namd/#running-namd-on-the-cluster","title":"Running NAMD on the Cluster","text":"<p>To set up NAMD on the cluster,</p> <pre><code>module load NAMD/2.13-multicore\n</code></pre> <p>for the standard multicore version, or</p> <pre><code>module load NAMD/2.13-multicore-CUDA\n</code></pre> <p>for the GPU-enabled version (about which there is more information below).</p> <p>NAMD can be run interactively, or as a batch job.</p> <p>To run NAMD interactively, you need to create an interactive session on a compute node. You could start an interactive session using 4 cores for 4 hours using</p> <pre><code>salloc --x11 -c 4 -t 4:00:00\n</code></pre> <p>For longer simulations, you will generally want to run non-interactively via a batch job.</p>"},{"location":"clusters-at-yale/guides/namd/#parallelization","title":"Parallelization","text":"<p>NAMD is most effective when run with parallelization. For running on a single node,</p> <pre><code>namd2 +p${SLURM_CPUS_PER_TASK} YourConfigfile\n</code></pre> <p>where ${SLURM_CPUS_PER_TASK} is set by your \"-c\" job resource request.</p> <p>NAMD uses charm++ parallel objects for multinode parallelization and the program launch uses the charmrun interface. Setting up a multinode run in a way that provides improved performance can be a complicated undertaking.  If you wish to run a multinode NAMD job and are not already familiar with MPI, feel free to contact the YCRC staff for assistance.</p>"},{"location":"clusters-at-yale/guides/namd/#gpus","title":"GPUs","text":"<p>To use the GPU-accelerated version, request GPU resources for your SLURM job using salloc or via a submission script, and load a CUDA-enabled version of NAMD:</p> <pre><code>module load NAMD/2.13-multicore-CUDA\n</code></pre> <p>For a single-node run, you will need at least one thread for each GPU you want to use:</p> <pre><code>#SBATCH -c 4 --gpus=4\n...\n\ncharmrun ++local namd2 +p${SLURM_CPUS_PER_TASK} YourConfigfile\n</code></pre>"},{"location":"clusters-at-yale/guides/nextflow/","title":"Nextflow","text":"<p>Nextflow is a very popular workflow tool, especially in bioinformatics.  It automates workflow processing, is very portable, and has excellent reporting. Nextflow is able to make effective use of slurm when running  on our clusters, using slurm submissions for running processes and achieving a high level of parallelism.  However, there are a few gotchas and things to know about.</p> <p>First, to specify slurm as the executor, add the following executor default to the process specification in nextflow.config. </p> <pre><code>Process { \n    \u2026\n   executor = 'slurm'\n    \u2026\n}\n</code></pre> <p>You can add other slurm-related options, for example:</p> <pre><code>process {\n    executor = 'slurm'\n    queue = 'day'\n    memory = '200 GB'\n    cpus = 32\n    Time = 4h\n}\n</code></pre> <p>This sets the initial default for the slurm partition, memory, cpus, and time.  Note that nextflow uses different names for many of these values than slurm. These same options can be added to specific processes or labels to customize processes more specifically. Arbitrary slurm options can be added using clusterOptions, e.g. clusterOptions = '--qos priority\u2019 More information can be found on nextflow's slurm page.</p> <p>Setting executor to slurm will cause all processes to be submitted as slurm jobs, unless otherwise specified (see below).</p>"},{"location":"clusters-at-yale/guides/nextflow/#nextflow-installation","title":"Nextflow installation","text":"<p>You can either use our installed module, or install your own copy of nextflow (for example if you want the very latest version of nextflow).  If you use your own nextflow, be sure to load the Java module.  Our nextflow module does that automatically.</p>"},{"location":"clusters-at-yale/guides/nextflow/#using-conda-or-apptainersingularity","title":"Using conda or apptainer/singularity","text":"<p>It is common for nextflow pipelines to use a containerization to manage code, such as conda or apptainer (aka singularity).  However, conda is not installed as a system tool on our clusters.  Therefore, if using conda for your process code, you should load the miniconda module in your batch script before invoking nextflow.  The nextflow submissions will inherit this module in the usual way.  Apptainer/singularity is installed as a system tool, but only on compute nodes.  Therefore, you must run nextflow on a properly allocated compute node (which should be the case anyway), not on the login node.</p>"},{"location":"clusters-at-yale/guides/nextflow/#scheduling-quirks","title":"Scheduling quirks","text":"<p>When running nextflow with slurm executor, you may notice some scheduling oddities.  This is due to the fact that multiple barriers can pend jobs.</p> <p>Internally, nextflow limits the number of submitted slurm jobs to the value of \u2018queueSize\u2019, by default 100.  This can be modified in the configuration or using the -qs command line option.   This is why nextflow can report a large number of pending jobs, while squeue only shows 100.  Slurm (squeue) will not show the jobs pended by queueSize, since nextflow has not actually submitted them yet.  </p> <p>Once jobs are actually submitted, the usual slurm queuing will occur.  This can include: per user or group limits, lack of available resources, etc.  These pended jobs will show in squeue as PD.</p>"},{"location":"clusters-at-yale/guides/nextflow/#submission-threshold","title":"Submission threshold","text":"<p>In order to prevent abusive job submission, most of our partitions have a limit of 200 individual submissions per user per hour.  Normally not a problem, this limit can cause problems with nextflow, since by default when using slurm all processes are submissions, and many workflows submit hundreds of very short jobs.  When the threshold is exceeded, subsequent submissions fail, and typically the nextflow workflow fails.  In addition, running very short processes as slurm submissions is very inefficient.</p> <p>We recommend that you configure your workflow to specify small, short jobs as using the local executor, leaving the larger and longer running jobs to the slurm executor. The cleanest way to do this is to give those processes a specific \u2018label\u2019, e.g. process_local, and then set the executor for that label to be \u2018local\u2019.</p> <p>For example:</p> <pre><code> process Fastqc {\n   label 'process_local'\n   \u2026\n}\n\nProcess {\n    \u2026\n     withLabel:process_local {\n          executor = 'local'\n    \u2026\n}\n</code></pre> <p>Oftentimes, if you are using a well developed workflow, the configuration will already use labels to \u201cclassify\u201d each of the processes, so that resource requirements are only mentioned once per class, rather than for every process.  For example, a workflow might define the class \u2018process_single\u2019, which is used to tag small, quick running processes.  In that case, you can simply add executor = local to the withLabel for that label:</p> <pre><code>Process {\n     \u2026\n     withLabel:process_single {\n         executor = 'local'\n         cpus = 1\n         memory = { maxMem(2.GB * task.attempt) }\n     }\n     \u2026\n}\n</code></pre>"},{"location":"clusters-at-yale/guides/nextflow/#running-a-hybrid-workflow","title":"Running a hybrid workflow","text":"<p>When running a workflow that uses both slurm and local executors, you should submit the run as a batch job of a single task and multiple cpus.  Give the batch job a reasonable number of cpus, depending on how many local processes you want to run simultaneously (e.g. 32).  The local processes will all run within the main batch job, and the slurm processes will be submitted as separate slurm jobs.  If done correctly this should keep you below the 200 submissions/hour threshold.</p> <p>You may also try reducing queueSize to a value less than 100.  If you cannot find a way to reduce your submission rate to an acceptable level, we will consider turning off the submission threshold.  Contact us for more information.</p>"},{"location":"clusters-at-yale/guides/parallel/","title":"Parallel","text":"<p>GNU Parallel a simple but powerful way to run independent tasks in parallel.  Although it is possible to run on multiple nodes, it is simplest to run on multiple cpus of a single node, and that is what we will consider here.  Note that what is presented here just scratches the surface of what parallel can do.</p>"},{"location":"clusters-at-yale/guides/parallel/#basic-examples","title":"Basic Examples","text":""},{"location":"clusters-at-yale/guides/parallel/#loop","title":"Loop","text":"<p>Let's parallelize the following bash loop that prints the letters <code>a</code> through <code>f</code> using bash's brace expansion:</p> <pre><code>for letter in {a..f};\ndo\n    echo $letter\ndone\n</code></pre> <p>... which produces the following output:</p> <pre><code>a\nb\nc\nd\ne\nf\n</code></pre> <p>To achieve the same result, <code>parallel</code> starts some number of workers and then runs tasks on them.  The number of workers and tasks need not be the same. You specify the number of workers with <code>-j</code>. The tasks can be generated with a list of arguments specified after the separator <code>:::</code>. For parallel to perform well, you should allocate at least the same number of CPUs as workers with the slurm option <code>--cpus-per-task</code> or more simply <code>-c</code>.</p> <pre><code>salloc -c 4\nmodule load parallel\nparallel -j 4 \"echo {}\" ::: {a..f}\n</code></pre> <p>This runs four workers that each run <code>echo</code>, filling in the argument <code>{}</code> with the next item in the list. This produces the output:</p>"},{"location":"clusters-at-yale/guides/parallel/#nested-loop","title":"Nested Loop","text":"<p>Let's parallelize the following nested bash loop.</p> <pre><code>for letter in {a..c}\ndo\n    for number in {1..7..2}\n    do\n        echo $letter $number\n    done\ndone\n</code></pre> <p>... which produces the following output:</p> <pre><code>a 1\na 2\na 3\nb 1\nb 2\nb 3\nc 1\nc 2\nc 3\n</code></pre> <p>You can use the <code>:::</code> separator with <code>parallel</code> to specify multiple lists of parameters you would like to iterate over. Then you can refer to them by one-based index, e.g. list one is <code>{1}</code>. Using these, you can ask parallel to execute combinations of parameters. Here is a way to recreate the result of the serial bash loop above:</p> <pre><code>parallel -j 4 \"echo {1} {2}\" ::: {a..c} ::: {1..3}\n</code></pre>"},{"location":"clusters-at-yale/guides/parallel/#advanced-examples","title":"Advanced Examples","text":""},{"location":"clusters-at-yale/guides/parallel/#md5sum","title":"<code>md5sum</code>","text":"<p>You have a number of files scattered throughout a directory tree.  Their names end with fastq.gz, e.g. d1/d3/sample3.fastq.gz.  You'd like to run md5sum on each, and put the output in a file in the same directory, with a filename ending with .md5sum, e.g. d1/d3/sample3.md5sum. Here is a script that will do that in parallel, using 16 cpus on one node of the cluster:</p> <pre><code>#!/bin/bash\n#SBATCH -c 16\n\nmodule load parallel\nparallel -j ${SLURM_CPUS_PER_TASK} --plus \"echo {}; md5sum {} &gt; {/fastq.gz/md5sum.new}\" ::: $(find . -name \"*.fastq.gz\" -print)\n</code></pre> <p>The <code>$(find . -name \"*.fastq.gz\" -print)</code> portion of the command returns all of the files of interest. They will be plugged into the {} in the md5sum command. <code>{/fastq.gz/md5sum.new}</code> does a string replacement on the filename, producing the desired output filename.  String replacement requires the --plus flag to parallel, which enables a number of powerful string manipulation features. Finally, we pass <code>-j ${SLURM_CPUS_PER_TASK}</code> so that parallel will use all of the allocated cpus, however many there are.</p>"},{"location":"clusters-at-yale/guides/parallel/#parameter-sweep","title":"Parameter Sweep","text":"<p>You want to run a simulation program that takes a number of input parameters, and you want to sample a variety of values for each parameter.</p> <pre><code>#!/bin/bash\n#SBATCH -c 16\nmodule load parallel\nparallel -j ${SLURM_CPUS_PER_TASK} simulate {1} {2} {3} ::: {1..5} ::: 2 16 ::: {5..50..5}\n</code></pre> <p>This will run 100 jobs, each with parameters that vary as :</p> <pre><code>simulate 1 2 5\nsimulate 1 2 10\nsimulate 1 2 15\n...\nsimulate 5 16 45\nsimulate 5 16 50\n</code></pre> <p>If <code>simulate</code> doesn't create unique output based on parameters, you can use redirection so you can review results from each task. You'll need to use quotes so that the &gt; is seen as part of the command:</p> <pre><code>parallel -j ${SLURM_CPUS_PER_TASK} \"simulate {1} {2} {3} &gt; results_{1}_{2}_{3}.out\" ::: $(seq 1 5) ::: 2 16 ::: $(seq 5 5 50)\n</code></pre>"},{"location":"clusters-at-yale/guides/python/","title":"Python","text":"<p>Python is a language and free software distribution that is used for websites, system administration, security testing, and scientific computing, to name a few.  On the Yale Clusters there are a couple ways in which you can set up Python environments.  The default python provided is the minimal install of Python 3.8 that comes with Red Hat Enterprise Linux 8.  We strongly recommend that you use one of the methods below to set up your own python environment.</p>"},{"location":"clusters-at-yale/guides/python/#the-python-module","title":"The Python Module","text":"<p>We provide a Python as a software module.  We include frozen versions of many common packages used for scientific computing.</p>"},{"location":"clusters-at-yale/guides/python/#find-and-load-python","title":"Find and Load Python","text":"<p>Find the available versions of Python version 3 with:</p> <pre><code>module avail Python/3\n</code></pre> <p>To load version 3.8.6:</p> <pre><code>module load Python/3.8.6-foss-2020b\n</code></pre> <p>To show installed Python packages and their versions for the <code>Python/3.8.6-foss-2020b</code> module:</p> <pre><code>module help Python/3.8.6-foss-2020b\n</code></pre>"},{"location":"clusters-at-yale/guides/python/#install-packages","title":"Install Packages","text":"<p>We recommend against installing python packages with <code>pip</code> after having loaded the Python module.  Doing so installs them to your home directory in a way that does not make it clear to other python installs what environment the packages you installed belong to.  Instead we recommend using virtualenv or Conda environments.  We like conda because of all the additional pre-compiled software it makes available.</p> <p>Warning</p> <p>Grace's login nodes have newer architecture than the oldest nodes on the cluster.  If you do <code>pip</code> install packages, do so in an interactive job submitted with the <code>-C oldest</code> Slurm flag if you want to ensure your code will work on all generations of the compute nodes.</p>"},{"location":"clusters-at-yale/guides/python/#conda-based-python-environments","title":"Conda-based Python Environments","text":"<p>You can easily set up multiple Python installations side-by-side using the <code>conda</code> command. With Conda you can manage your own packages and dependencies for Python, R, etc.  See our guide for more detailed instructions. </p> <pre><code># install once\nmodule load miniconda\nconda create -n py3_env python=3 numpy scipy matplotlib ipython jupyter jupyterlab\n# use later\nmodule purge &amp;&amp; module load miniconda\nconda activate py3_env\n</code></pre>"},{"location":"clusters-at-yale/guides/python/#run-python","title":"Run Python","text":"<p>We will kill Python jobs on the login nodes that are using excessive resources.  To be a good cluster citizen, launch your computation in jobs.  See our Slurm documentation for more detailed information on submitting jobs.</p>"},{"location":"clusters-at-yale/guides/python/#interactive-job","title":"Interactive Job","text":"<p>To run Python interactively, first launch an interactive job on a compute node.  If your Python sessions will need up to 10 GiB of RAM and up to 4 hours, you would submit you job with:</p> <pre><code>salloc --mem=10G -t 4:00:00\n</code></pre> <p>Once your interactive session starts, you can load the appropriate module or Conda environment (see above) and start <code>python</code> or <code>ipython</code> on your command prompt.  If you are happy with your Python commands, save them to a file which can then be submitted and run as a batch job.</p>"},{"location":"clusters-at-yale/guides/python/#batch-mode","title":"Batch Mode","text":"<p>To run Python in batch mode, create a plain-text batch script to submit. In that script, you call your Python script.  In this case <code>myscript.py</code> is in the same directory as the batch script, batch script contents shown below.</p> <pre><code>#!/bin/bash\n#SBATCH -J my_python_program\n#SBATCH --mem=10G\n#SBATCH -t 4:00:00\n\nmodule load miniconda\nconda activate py3_env\npython myscript.py\n</code></pre> <p>To actually submit the job, run <code>sbatch my_py_job.sh</code> where the batch script above was saved as <code>my_py_job.sh</code>.</p>"},{"location":"clusters-at-yale/guides/python/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>You can run Jupyter notebooks &amp; JupyterLab by submitting your notebook server as a job. See our page dedicated to Jupyter for more info.</p>"},{"location":"clusters-at-yale/guides/pytorch/","title":"Pytorch","text":"<p>Pytorch is an open source Machine Learning (ML) framework based on the python programming language.</p>"},{"location":"clusters-at-yale/guides/pytorch/#pytorch-module","title":"Pytorch module","text":"<p>Some pytorch versions are already available on the clusters at yale as modules and will not require any user modification to run successfully. You can search for these versions using the module avail command:</p> <pre><code>module avail pytorch\n</code></pre>"},{"location":"clusters-at-yale/guides/pytorch/#installing-pytorch-in-a-miniconda-environment","title":"Installing Pytorch in a miniconda environment","text":"<p>If you find that there is not a module available on the cluster for the version of pytorch you need, and/or you are using a complex miniconda environment as part of your workflow, then you may benefit from installing pytorch yourself inside a miniconda environment.</p> <p>Click this link, installing pytorch, to be directed to the pytorch website to install the latest version of pytorch.</p> <p>You will be greeted with a table like the one below:</p> <p></p> <p>Please make sure your selections match the image above, i.e., Stable, linux, conda, and Python. Either CUDA version will work on the clusters, and CPU should be selected if you do not plan on using GPUs with your pytorch installation.</p> <p>Then copy the install command to install pytorch into your existing environment. Please make sure you are off the login node or the install will fail.</p> <p>If building a new environment, you can instead use the conda create command as outlined at this site.</p>"},{"location":"clusters-at-yale/guides/pytorch/#installing-older-versions-of-pytorch","title":"Installing older versions of pytorch","text":"<p>To install older versions of pytorch, you can find instructions at this site.</p>"},{"location":"clusters-at-yale/guides/r/","title":"R","text":"<p>R is a free software environment for statistical computing and graphics. On the Yale Clusters there are a couple ways in which you can set up your R environment. There is no R executable provided by default; you have to choose one of the following methods to be able to run R.</p>"},{"location":"clusters-at-yale/guides/r/#the-r-module","title":"The R Module","text":"<p>We provide several versions of R as software modules. These modules provide a broad selection of commonly used packages pre-installed. Notably, this includes a number of geospatial packages like <code>sf</code>, <code>sp</code>, <code>raster</code>, and <code>terra</code>.</p> <p>In addition, we install a collection of the most common <code>bioconductor</code> bioinformatics packages (homepage) called <code>R-bundle-Bioconductor</code>. This can be loaded in addition to the matching R module to provide simple access to these tools.</p>"},{"location":"clusters-at-yale/guides/r/#find-and-load-r","title":"Find and Load R","text":"<p>Find the available versions of R version 4 with:</p> <pre><code>module avail R/4\n</code></pre> <p>To load version\u00a04.2.0:</p> <pre><code>module load R/4.2.0-foss-2020b\n</code></pre> <p>To show installed R packages and their versions for the <code>R/4.2.0</code> module:</p> <pre><code>module help R/4.2.0-foss-2020b\n</code></pre> <p>Between the base R module and the R-bundle-Bioconductor module, there are over 1000 R packages installed and ready to use. </p> <p>To find if your desired package is available in these modules, you can run <code>module spider $PACKAGE/$VERSION</code>:</p> <pre><code>module spider Seurat/4.1.1\n\n--------------------------------------------------------------------------------------------------------------------------------------------------------\n  Seurat: Seurat/4.1.1 (E)\n--------------------------------------------------------------------------------------------------------------------------------------------------------\n    This extension is provided by the following modules. To access the extension you must load one of the following modules. Note that any module names in parentheses show the module location in the software hierarchy.\n\n\n       R-bundle-Bioconductor/3.15-foss-2020b-R-4.2.0\n\n\nNames marked by a trailing (E) are extensions provided by another module.\n</code></pre> <p>So to get this version of Seurat, you can load the R-bundle-Bioconductor module.  Then you simple <code>library(Seurat)</code> to use that tool.</p>"},{"location":"clusters-at-yale/guides/r/#install-packages","title":"Install Packages","text":"<p>The software modules include many commonly used packages, but you can install additional packages specifically for your account. As part of the R software modules we define an environment variable which directs R to install packages to your project space. This helps prevent issues where R cannot install packages due to home-space quotas. To change the location of where R installs packages, the <code>R_LIBS_USER</code> variable can be set in your <code>~/.bashrc</code> file:</p> <pre><code>export R_LIBS_USER=$GIBBS_PROJECT/R/%v\n</code></pre> <p>where <code>%v</code> is a placeholder for the R major and minor version number (e.g. <code>4.2</code>). R will replace this variable with the correct value automatically to segregate packages installed with different versions of R.</p> <p>We recommend you install packages in an interactive job with the slurm option <code>-C oldest</code>. This will ensure the compiled portions of your R library are compatible with all compute nodes on the cluster. If there is a missing library your package of interest needs you should be able to load its module. If you cannot find a dependency or have trouble installing an R package, please get in touch with us.</p> <p>Warning</p> <p>Grace's login nodes have newer architecture than the oldest nodes on the cluster. Always install packages in an interactive job submitted with the <code>-C oldest</code> Slurm flag if you want to ensure your code will work on all generations of the compute nodes.</p> <p>To get started load the R module and start R:</p> <pre><code>salloc\nmodule load R/4.2.0-foss-2020b\nR\n# in R\n&gt; install.packages(\"lattice\", repos=\"http://cran.r-project.org\")\n</code></pre> <p>This will throw a warning like:</p> <pre><code>Warning in install.packages(\"lattice\") :\n'lib = \"/ysm-gpfs/apps/software/R/4.2.0-foss-2020b/lib64/R/library\"' is not writable\nWould you like to create a personal library\n/gpfs/gibbs/project/support/tl397/R/4.1\nto install packages into?  (y/n)\n</code></pre> <p>Note</p> <p>If you encounter a permission error because the installation does not prompt you to create a personal library, create the directory in the default location in your home directory for the version of R you are using; e.g., <pre><code>mkdir /path/to/your/project/space/R/4.2\n</code></pre> You only need the general minor version such as 4.2 instead of 4.2.2.</p> <p>You can customize where packages are installed and accessed for a particular R session using the .libPaths function in R: <pre><code># List current package locations\n&gt; .libPaths()\n\n# Add new default location to the standard defaults, e.g. project/my_R_libs\n&gt; .libPaths(c(\"/home/netID/project/my_R_libs/\", .libPaths()))\n</code></pre></p>"},{"location":"clusters-at-yale/guides/r/#run-r","title":"Run R","text":"<p>We will kill R jobs on the login nodes that are using excessive resources. To be a good cluster citizen, launch your R computation in jobs. See our Slurm documentation for more detailed information on submitting jobs.</p>"},{"location":"clusters-at-yale/guides/r/#interactive-job","title":"Interactive Job","text":"<p>To run R interactively, first launch an interactive job on a compute node. If your R sessions will need up to 10 GiB of RAM and up to 4 hours, you would submit you job with:</p> <pre><code>salloc --mem=10G -t 4:00:00\n</code></pre> <p>Once your interactive session starts, you can load the appropriate module or Conda environment (see above) and start R by entering <code>R</code> on your command prompt. If you are happy with your R commands, save them to a file which can then be submitted and run as a batch job.</p>"},{"location":"clusters-at-yale/guides/r/#batch-mode","title":"Batch Mode","text":"<p>To run R in batch mode, create a plain-text batch script to submit. In that script, you can run your R script. In this case <code>myscript.R</code> is in the same directory as the batch script, batch script contents shown below.</p> <pre><code>#!/bin/bash\n#SBATCH -J my_r_program\n#SBATCH --mem=10G\n#SBATCH -t 4:00:00\n\nmodule load R/4.1.0-foss-2020b\nRscript myscript.R\n</code></pre> <p>To actually submit the job, run <code>sbatch my_r_job.sh</code> where the batch script above was saved as <code>my_r_job.sh</code>.</p>"},{"location":"clusters-at-yale/guides/r/#rstudio","title":"RStudio","text":"<p>You can run RStudio app via Open Ondemand. Here you can select the desired version of R and RStudio and launch an interactive compute session.</p>"},{"location":"clusters-at-yale/guides/r/#parallel-r","title":"Parallel R","text":"<p>On a cluster you may want to use R in parallel across multiple nodes. While there are a few different ways this can be achieved, we recommend using the R software module which already includes <code>Rmpi</code>, <code>parallel</code>, and <code>doMC</code>.</p> <p>To test it, we can create a simple R script named <code>ex1.R</code></p> <pre><code>library(\"Rmpi\")\n\nn&lt;-mpi.comm.size(0)\nme&lt;-mpi.comm.rank(0)\n\nmpi.barrier(0)\nval&lt;-777\nmpi.bcast(val, 1, 0, 0)\nprint(paste(\"me\", me, \"val\", val))\nmpi.barrier(0)\n\nmpi.quit()\n</code></pre> <p>Then we can launch it with an sbatch script (<code>ex1.sh</code>):</p> <pre><code>#!/bin/bash\n\n#SBATCH -n 4 \n#SBATCH -t 5:00\n\nmodule purge\nmodule load R/4.2.0-foss-2020b\n\nsrun Rscript ex1.R\n</code></pre> <p>This script should execute a simple broadcast and complete in a few seconds.</p>"},{"location":"clusters-at-yale/guides/r/#virtual-display-session","title":"Virtual Display Session","text":"<p>It is common for R to require a display session to save certain types of figures. You may see a warning like \"unable to start device PNG\" or \"unable to open connection to X11 display\". There is a tool, <code>xvfb</code>, which can help avoid these issues.</p> <p>The wrapper <code>xvfb-run</code> creates a virtual display session which allows R to create these figures without an X11 session. See the guide for xvfb for more details.</p>"},{"location":"clusters-at-yale/guides/r/#conda-based-r-environments","title":"Conda-based R Environments","text":"<p>If there isn't a module available for the version of R you want, you can set up your own R installation using Conda. With Conda you can manage your own packages and dependencies, for R, Python, etc.</p> <p>Most of the time the best way to install R packages for your Conda R environment is via <code>conda</code>.</p> <pre><code># load miniconda\nmodule load miniconda\n# create the conda environment including r-base and r-essentials package collections\nconda create --name my_r_env r-base r-essentials\n# activate the environment\nconda activate my_r_env\n\n# Install the lattice package (r-lattice)\nconda install r-lattice\n</code></pre> <p>If there are packages that conda does not provide, you can install using the <code>install.packages</code> function, but this may occasionally not work as well. When you install packages with <code>install.packages</code> make sure to activate your Conda environment first.</p> <pre><code>salloc\nmodule load miniconda\nsource activate my_r_env\nR\n# in R\n&gt; install.packages(\"lattice\", repos=\"http://cran.r-project.org\")\n</code></pre> <p>Warning</p> <p>Conda-based R may not work properly with parallel packages like <code>Rmpi</code> when running across multiple compute nodes. In general, it's best to use the module installation of R for anything which requires MPI.</p>"},{"location":"clusters-at-yale/guides/rclone/","title":"Rclone","text":"<p><code>rclone</code> is a command line tool to sync files and directories to and from all major cloud storage sites. You can use <code>rclone</code> to sync files and directories between Yale clusters and Yale Box, google drive, etc. The following instructions cover basics to setup and use <code>rclone</code> on Yale clusters. For more information about Rclone, please visit its website at https://rclone.org. </p>"},{"location":"clusters-at-yale/guides/rclone/#set-up-rclone-on-ycrc-clusters","title":"Set up Rclone on YCRC clusters","text":"<p>Before accessing a remote cloud storage using <code>rclone</code>, you need to run <code>rclone config</code> to configure  the storage for <code>rclone</code>. Since <code>rclone config</code> will try to bring up a browser for you to authorize  the cloud storage, we recommend you to use Open OnDemand. </p> <p>To run <code>rclone config</code> on OOD, first click <code>Remote Desktop</code> from the OOD dashboard.  Once a session starts running, click <code>Connect to Remote Desktop</code> and  you will see a terminal on the desktop in the browser.  Run <code>rclone config</code> at the command line of the terminal. </p> <p></p> <p></p> <p>During configuration, you will see a message similar to the following:  <pre><code>If your browser does not open automatically go to the following link: http://127.0.0.1:53682/auth\nLog in and authorize rclone for access\nWaiting for code...\n</code></pre> If no browser started automatically, then start Firefox manually by clicking  the Firefox icon on the top bar of the Remote Desktop.  Copy the link from the message shown on your screen and paste it to the address bar of Firefox. Log in with your Yale email address, respond to the DUO request, and authorize the access. </p> <p>Tip</p> <p>If you received an error stating that your session has expired for DUO, simply paste the link and reload the page.  If you still get the expired message, log out of CAS in your browser by going to https://secure.its.yale.edu/cas/logout.  After logging out, paste the link and reload.</p>"},{"location":"clusters-at-yale/guides/rclone/#examples","title":"Examples","text":"<p>The following examples show you how to set up rclone for a viriety of different storage types. In the examples, we name our remote cloud storage as 'remote' in the configuration.  You can provide any name you want. </p> Google Drive <p>The example below is a screen dump when setting up <code>rclone</code> for Google Drive.</p> <pre><code>[pl543@c03n06 ~]$ rclone config\nNo remotes found - make a new one\nn) New remote\ns) Set configuration password\nq) Quit config\nn/s/q&gt; n\nname&gt; remote\nType of storage to configure.\nEnter a string value. Press Enter for the default (\"\").\nChoose a number from below, or type in your own value\n 1 / 1Fichier\n   \\ \"fichier\"\n 2 / Alias for an existing remote\n   \\ \"alias\"\n[...]\n15 / Google Drive\n   \\ \"drive\"\n[...]\n42 / seafile\n   \\ \"seafile\"\nStorage&gt; 15\n** See help for drive backend at: https://rclone.org/drive/ **\n\nGoogle Application Client Id\nSetting your own is recommended.\nSee https://rclone.org/drive/#making-your-own-client-id for how to create your own.\nIf you leave this blank, it will use an internal key which is low performance.\nEnter a string value. Press Enter for the default (\"\").\nclient_id&gt; \nOAuth Client Secret\nLeave blank normally.\nEnter a string value. Press Enter for the default (\"\").\nclient_secret&gt; \nScope that rclone should use when requesting access from drive.\nEnter a string value. Press Enter for the default (\"\").\nChoose a number from below, or type in your own value\n 1 / Full access all files, excluding Application Data Folder.\n   \\ \"drive\"\n 2 / Read-only access to file metadata and file contents.\n   \\ \"drive.readonly\"\n   / Access to files created by rclone only.\n 3 | These are visible in the drive website.\n   | File authorization is revoked when the user deauthorizes the app.\n   \\ \"drive.file\"\n   / Allows read and write access to the Application Data folder.\n 4 | This is not visible in the drive website.\n   \\ \"drive.appfolder\"\n   / Allows read-only access to file metadata but\n 5 | does not allow any access to read or download file content.\n   \\ \"drive.metadata.readonly\"\nscope&gt; 1\nID of the root folder\nLeave blank normally.\n\nFill in to access \"Computers\" folders (see docs), or for rclone to use\na non root folder as its starting point.\n\nEnter a string value. Press Enter for the default (\"\").\nroot_folder_id&gt; \nService Account Credentials JSON file path \nLeave blank normally.\nNeeded only if you want use SA instead of interactive login.\n\nLeading `~` will be expanded in the file name as will environment variables such as `${RCLONE_CONFIG_DIR}`.\n\nEnter a string value. Press Enter for the default (\"\").\nservice_account_file&gt; \nEdit advanced config? (y/n)\ny) Yes\nn) No (default)\ny/n&gt; n\nRemote config\nUse auto config?\n * Say Y if not sure\n * Say N if you are working on a remote or headless machine\ny) Yes (default)\nn) No\ny/n&gt; y\nIf your browser doesn't open automatically go to the following link: http://127.0.0.1:53682/auth?state=6glRr_mpEORxHevlOaaYyw\nLog in and authorize rclone for access\nWaiting for code...\nGot code\nConfigure this as a Shared Drive (Team Drive)?\ny) Yes\nn) No (default)\ny/n&gt; n\n--------------------\n[remote]\ntype = drive\nscope = drive\ntoken = {\"access_token\":\"ya29.A0ArdaM-mBYFKBE2gieODvdANCZRV6Y8QHhQF-lY74E9fr1HTLOwwLRuoQQbO9P-Jdip62YYhqXfcuWT0KLKGdhUb9M8g2Z4XEQqoNLwZyA-FA2AAYYBqB\",\"token_type\":\"Bearer\",\"refresh_token\":\"1//0dDu3r6KVakgYIARAAGA0NwF-L9IrWIuG7_f44x-uLR2vvBocf4q-KnQVhlkm18TO2Fn0GjJp-cArWfj5kY84\",\"expiry\":\"2021-02-25T17:28:18.629507046-05\n:00\"}\n--------------------\ny) Yes this is OK (default)\ne) Edit this remote\nd) Delete this remote\ny/e/d&gt; y\nCurrent remotes:\n\nName                 Type\n====                 ====\nremote               drive\n\ne) Edit existing remote\nn) New remote\nd) Delete remote\nr) Rename remote\nc) Copy remote\ns) Set configuration password\nq) Quit config\ne/n/d/r/c/s/q&gt; q\n</code></pre> OneDrive <p>The example below is a screen dump when setting up <code>rclone</code> for Microsoft OneDrive.  This example was done inside a remote desktop in OOD, so rclone was able to start the browser to authenticate.  If you run this on a normal command line, you'll need to say 'N' at that step.</p> <pre><code>[rdb9@r811u30n01.milgram ~]$ rclone config\n2024/08/06 11:09:57 NOTICE: Config file \"/home/rdb9/.config/rclone/rclone.conf\" not found - using defaults\nNo remotes found, make a new one?\nn) New remote\ns) Set configuration password\nq) Quit config\nn/s/q&gt; n\n\nEnter name for new remote.\nname&gt; yaleonedrive\n\nOption Storage.\nType of storage to configure.\nChoose a number from below, or type in your own value.\n 1 / 1Fichier\n   \\ (fichier)\n 2 / Akamai NetStorage\n   \\ (netstorage)\n...\n30 / Microsoft Azure Blob Storage\n   \\ (azureblob)\n31 / Microsoft OneDrive\n   \\ (onedrive)\n32 / OpenDrive\n   \\ (opendrive)\n...\nStorage&gt; 31\n\nOption client_id.\nOAuth Client Id.\nLeave blank normally.\nEnter a value. Press Enter to leave empty.\nclient_id&gt; \n\nOption client_secret.\nOAuth Client Secret.\nLeave blank normally.\nEnter a value. Press Enter to leave empty.\nclient_secret&gt; \n\nOption region.\nChoose national cloud region for OneDrive.\nChoose a number from below, or type in your own string value.\nPress Enter for the default (global).\n 1 / Microsoft Cloud Global\n   \\ (global)\n 2 / Microsoft Cloud for US Government\n   \\ (us)\n 3 / Microsoft Cloud Germany\n   \\ (de)\n 4 / Azure and Office 365 operated by Vnet Group in China\n   \\ (cn)\nregion&gt; \n\nEdit advanced config?\ny) Yes\nn) No (default)\ny/n&gt; \n\nUse web browser to automatically authenticate rclone with remote?\n * Say Y if the machine running rclone has a web browser you can use\n * Say N if running rclone on a (remote) machine without web browser access\nIf not sure try Y. If Y failed, try N.\n\ny) Yes (default)\nn) No\ny/n&gt; y\n\n2024/08/06 11:12:43 NOTICE: If your browser doesn't open automatically go to the following link: http://127.0.0.1:53682/auth?state=Xt0h7NYaFt1F6ogeDRC0iQ\n2024/08/06 11:12:43 NOTICE: Log in and authorize rclone for access\n2024/08/06 11:12:43 NOTICE: Waiting for code...\n2024/08/06 11:13:45 NOTICE: Got code\nOption config_type.\nType of connection\nChoose a number from below, or type in an existing string value.\n\n[ choose defaults from now on]\n\nKeep this \"yaleonedrive\" remote?\ny) Yes this is OK (default)\ne) Edit this remote\nd) Delete this remote\ny/e/d&gt; \n\nCurrent remotes:\n\nName                 Type\n====                 ====\nyaleonedrive         onedrive\n\ne) Edit existing remote\nn) New remote\nd) Delete remote\nr) Rename remote\nc) Copy remote\ns) Set configuration password\nq) Quit config\ne/n/d/r/c/s/q&gt; q\n</code></pre> Box <p>The example below is a screen dump when setting up <code>rclone</code> for Yale Box.</p> <pre><code>[pl543@c14n07 ~]$ rclone config\nNo remotes found - make a new one\nn) New remote\ns) Set configuration password\nq) Quit config\nn/s/q&gt; n\nname&gt; remote\nType of storage to configure.\nEnter a string value. Press Enter for the default (\"\").\nChoose a number from below, or type in your own value\n 1 / 1Fichier\n   \\ \"fichier\"\n[...]\n 6 / Box\n   \\ \"box\"\n[...]\nStorage&gt; box\n** See help for box backend at: https://rclone.org/box/ **\n\nBox App Client Id.\nLeave blank normally.\nEnter a string value. Press Enter for the default (\"\").\nclient_id&gt; \nBox App Client Secret\nLeave blank normally.\nEnter a string value. Press Enter for the default (\"\").\nclient_secret&gt; \nEdit advanced config? (y/n)\ny) Yes\nn) No\ny/n&gt; n\nRemote config\nUse auto config?\n * Say Y if not sure\n * Say N if you are working on a remote or headless machine\ny) Yes\nn) No\ny/n&gt; y\nIf your browser does not open automatically go to the following link: http://127.0.0.1:53682/auth\nLog in and authorize rclone for access\nWaiting for code...\nGot code\n--------------------\n[remote]\ntype = box\ntoken = {\"access_token\":\"PjIXHUZ34VQSmeUZ9r6bhc9ux44KMU0e\",\"token_type\":\"bearer\",\"refresh_token\":\"VumWPWP5Nd0M2C1GyfgfJL51zUeWPPVLc6VC6lBQduEPsQ9a6ibSor2dvHmyZ6B8\",\"expiry\":\"2019-10-21T11:00:36.839586736-04:00\"}\n--------------------\ny) Yes this is OK\ne) Edit this remote\nd) Delete this remote\ny/e/d&gt; y\nCurrent remotes:\n\nName                 Type\n====                 ====\nremote               box\n\ne) Edit existing remote\nn) New remote\nd) Delete remote\nr) Rename remote\nc) Copy remote\ns) Set configuration password\nq) Quit config\ne/n/d/r/c/s/q&gt; q\n</code></pre> S3 <p>The example below is a screen dump when setting up <code>rclone</code> for an S3 provider such as aws.</p> <pre><code>[rdb9@login1.mccleary ~]$ rclone config\nEnter configuration password:\npassword:\nCurrent remotes:\n\nName                 Type\n====                 ====\n\n[...]\n\ne) Edit existing remote\nn) New remote\nd) Delete remote\nr) Rename remote\nc) Copy remote\ns) Set configuration password\nq) Quit config\ne/n/d/r/c/s/q&gt; n\n```bash\n\nEnter name for new remote.\nname&gt; remote\n\nOption Storage.\nType of storage to configure.\nChoose a number from below, or type in your own value.\n\n[...]\n\n 5 / Amazon S3 Compliant Storage Providers including AWS, Alibaba, Ceph, China Mobile, Cloudflare, ArvanCloud, DigitalOcean, Dreamhost, Huawei OBS, IBM COS, IDrive e2, IONOS Cloud, Liara, Lyve Cloud, Minio, Netease, RackCorp, Scaleway, SeaweedFS, StackPath, Storj, Tencent COS, Qiniu and Wasabi\n   \\ (s3)\n[...]\nStorage&gt; 5\n\nOption provider.\nChoose your S3 provider.\nChoose a number from below, or type in your own value.\nPress Enter to leave empty.\n 1 / Amazon Web Services (AWS) S3\n   \\ (AWS)\n[...]\nprovider&gt; 1\n\nOption env_auth.\nGet AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).\nOnly applies if access_key_id and secret_access_key is blank.\nChoose a number from below, or type in your own boolean value (true or false).\nPress Enter for the default (false).\n 1 / Enter AWS credentials in the next step.\n   \\ (false)\n 2 / Get AWS credentials from the environment (env vars or IAM).\n   \\ (true)\nenv_auth&gt; \n\nOption access_key_id.\nAWS Access Key ID.\nLeave blank for anonymous access or runtime credentials.\nEnter a value. Press Enter to leave empty.\naccess_key_id&gt; ***************\n\nOption secret_access_key.\nAWS Secret Access Key (password).\nLeave blank for anonymous access or runtime credentials.\nEnter a value. Press Enter to leave empty.\nsecret_access_key&gt; *************\n\nOption region.\nRegion to connect to.\nChoose a number from below, or type in your own value.\nPress Enter to leave empty.\n   / The default endpoint - a good choice if you are unsure.\n 1 | US Region, Northern Virginia, or Pacific Northwest.\n   | Leave location constraint empty.\n   \\ (us-east-1)\n   / US East (Ohio) Region.\n[...]\n\n[take defaults for all remaining questions\n\nEdit advanced config?\ny) Yes\nn) No (default)\ny/n&gt; n\n\nConfiguration complete.\nOptions:\n- type: s3\n- provider: AWS\n- access_key_id: ***************\n- secret_access_key: ****************\n- region: us-east-1\n</code></pre> <p>Tip</p> <p>if you want to use rclone for a shared google drive, you should answer 'y' when it asks whether you want to configure it as a Shared Drive.</p> <pre><code>Configure this as a Shared Drive (Team Drive)?\ny) Yes\nn) No (default)\ny/n&gt; y\n</code></pre> <p>Tip</p> <p><code>rclone config</code> creates a file storing cloud storage configurations for rclone.  You can check the file name with <code>rclone config file</code>. The config file can be  copied to other clusters so that you can use <code>rclone</code> on the other clusters without running <code>rclone config</code> again.</p>"},{"location":"clusters-at-yale/guides/rclone/#use-rclone-on-yale-clusters","title":"Use Rclone on Yale clusters","text":"<p>As we have used <code>remote</code> as the name of the cloud storage in our examples above,  we will continue using it in the following examples.  You should replace it with the actual name you have picked up for the cloud storage in your configuration. </p> <p>Tip</p> <p>If you forgot the name of the cloud storage you have configured, run <code>rclone config show</code> and the name will be shown in <code>[]</code>.  <pre><code>$ rclone config show\n[remote]\ntype = drive\nscope = drive\ntoken = {\"access_token\":\"mytoken\",\"expiry\":\"2021-07-09T22:13:56.452750648-04:00\"}\nroot_folder_id = myid\n</code></pre></p>"},{"location":"clusters-at-yale/guides/rclone/#list-files","title":"List files","text":"<pre><code>rclone ls remote:/\n</code></pre>"},{"location":"clusters-at-yale/guides/rclone/#copy-files","title":"Copy files","text":"<pre><code># to download a file to the cluster\nrclone copy remote:/path/to/filename .\n\n# to upload a file from the cluster to the cloud storage\nrclone copy filename remote:/path/to/\n</code></pre>"},{"location":"clusters-at-yale/guides/rclone/#help","title":"Help","text":"<pre><code>rclone help\n</code></pre>"},{"location":"clusters-at-yale/guides/spark/","title":"Spark","text":"<p>Apache Spark is a powerful tool that enables distributed processing of large datasets that cannot fit into a single compute node. Spark is installed on all our clusters, including PySpark and Scala as part of the software modules.  Additionally, templates for both interactive and batch-mode configuration are available. </p>"},{"location":"clusters-at-yale/guides/spark/#resource-allocation-and-using-spark-start","title":"Resource allocation and using <code>spark-start</code>","text":"<p>Using Spark on HPC systems requires two steps:</p> <ol> <li>start a Spark instance running on compute node(s)</li> <li>submit work to that instance via web or command-line interfaces</li> </ol> <p>The installations of Spark on the Yale HPC systems include utilities that help with both of these steps, <code>spark-start</code> and <code>spark-submit</code>. </p> <p><code>spark-start</code> Collects specific Slurm environment variables and creates <code>spark-env.sh</code> file in <code>$HOME/.spark-local</code> before starting the Spark server.  <code>spark-submit</code> Submits work to the Spark server, requesting user-defined resources for a specific input script.</p> <p>Below are several examples of common workflows.</p>"},{"location":"clusters-at-yale/guides/spark/#interactive-spark-job","title":"Interactive Spark job","text":"<p>Spark can be set up to run interactively by starting the Spark server and printing out the <code>$SPARK_MASTER_URL</code> where jobs can be submitted via the Spark web-interface. An example sbatch script is shown below which requests two compute nodes, each with 36 CPUs and 180G of memory.  Note, the <code>spark-start</code> script expects the memory request to be <code>--mem</code> and not <code>--mem-per-cpu</code>. </p> <p><pre><code>#!/bin/bash\n#SBATCH --job-name=spark-cluster\n#SBATCH --partition=day\n#SBATCH --nodes=2                # node count, change as needed\n#SBATCH --ntasks-per-node=1      # do not change, leave as 1 task per node\n#SBATCH --cpus-per-task=36       # cpu-cores per task, change as needed\n#SBATCH --mem 180G               # memory per node, change as needed\n#SBATCH --time=00:60:00\n#SBATCH --mail-type=NONE\n\n# These modules are required. You may need to customize the module version\n# depending on which cluster you are on.\nmodule load Spark/3.5.1-foss-2022b-Scala-2.13\n\n# Start the Spark instance.\nspark-start\n\n# Source spark-env.sh to get useful env variables.\nsource ${HOME}/.spark-local/${SLURM_JOB_ID}/spark/conf/spark-env.sh\n\necho \"***** Spark cluster is running. Submit jobs to ${SPARK_MASTER_URL}. *****\"\n\n# set up SSH tunnel for spark web-interface\nnode=$(hostname -s)\nuser=$(whoami)\ncluster=$(hostname -f | awk -F\".\" '{print $2}')\nweb_port=8080\n\n# print tunneling instructions\necho -e \"\nMacOS or linux terminal command to create your ssh tunnel:\nssh -N -L ${web_port}:${node}:${web_port} ${user}@${cluster}.ycrc.yale.edu\n\nForwarded port:same as remote port\nRemote server: ${node}\nRemote port: ${port}\nSSH server: ${cluster}.hpc.yale.edu\nSSH login: $user\nSSH port: 22\n\nUse a Browser on your local machine to go to:\nlocalhost:${port} \n\"\n\nsleep infinity\n</code></pre> Spark jobs can be submitted to <code>spark://$NODE.grace.ycrc.yale.edu:7077</code> (where $NODE is replaced by the node where the job is running) address from anywhere on the cluster.</p> <p>To use the web UI from outside the cluster, an ssh command is needed to connect your local computer to the compute node where the Spark server is running. After making the ssh tunnel connection, navigate to <code>localhost:8080</code> to view the web UI to monitor your jobs.</p> <p>Navigating to the web-interface URL yields an overview like this:</p> <p></p>"},{"location":"clusters-at-yale/guides/spark/#spark-batch-job","title":"Spark batch job","text":"<p>While the interactive workflow is useful for testing, it can be an inefficient use of allocated resources. It is therefore preferrable to submit jobs as part of the Spark sbatch script.</p> <p>This example first starts up the Spark server as before, but then immediately submits work to the server via the <code>spark-submit</code> script. Note, replace the placeholder with the real path to the analysis code.</p> <p><pre><code>#!/bin/bash\n#SBATCH --job-name=spark-cluster\n#SBATCH --partition=day\n#SBATCH --nodes=2                # node count, change as needed\n#SBATCH --ntasks-per-node=1      # do not change, leave as 1 task per node\n#SBATCH --cpus-per-task=36       # cpu-cores per task, change as needed\n#SBATCH --mem 180G               # memory per node, change as needed\n#SBATCH --time=1:00:00\n#SBATCH --mail-type=NONE\n\n# These modules are required. You may need to customize the module version\n# depending on which cluster you are on.\nmodule load Spark/3.5.1-foss-2022b-Scala-2.13\n\n# Start the Spark instance.\nspark-start\n\n# Source spark-env.sh to get useful env variables.\nsource ${HOME}/.spark-local/${SLURM_JOB_ID}/spark/conf/spark-env.sh\n\n# print tunneling instructions\necho -e \"\nMacOS or linux terminal command to create your ssh tunnel:\nssh -N -L ${web_port}:${node}:${web_port} ${user}@${cluster}.ycrc.yale.edu\n\nForwarded port:same as remote port\nRemote server: ${node}\nRemote port: ${port}\nSSH server: ${cluster}.hpc.yale.edu\nSSH login: $user\nSSH port: 22\n\nUse a Browser on your local machine to go to:\nlocalhost:${port} \n\"\n\n# Customize the executor resources below to match resources requested above\n# with an allowance for spark driver overhead. Also change the path to your spark job.\nspark-submit --master ${SPARK_MASTER_URL} \\\n  --executor-cores 1 \\\n  --executor-memory 5G \\\n  --total-executor-cores $((SLURM_CPUS_ON_NODE - 1)) \\\n  /path/to/custom/analysis.py\n</code></pre> The Spark web-interface is accessible after setting up the ssh tunnel as described above. </p>"},{"location":"clusters-at-yale/guides/tensorflow/","title":"Tensorflow","text":"<p>Tensorflow and tensorflow-gpu are now the same package since tensorflow 2 was released. We do have some modules of tensorflow available for use with existing programs. You can find tensorflow versions using the command:</p> <pre><code>module avail tensorflow\n</code></pre>"},{"location":"clusters-at-yale/guides/tensorflow/#installing-tensorflow","title":"Installing Tensorflow","text":"<p>If you need a specific version of tensorflow or are working with specific python packages in a miniconda environment, then you will likely need to install your own version of tensorflow. Each version of tensorflow requires a specific version of CUDA and cudnn to be installed. You can refer to this website.</p> <p>This table outlines how to install each version of tensorflow from 2.15-2.11:</p> tensorflow 2.16 <pre><code>module load miniconda\nconda create -n tf16 python=3.11.*\npip install tensorflow[and-cuda]==2.16.*\n\n#tensorflow can't find cuda libraries, need to tell it\nmkdir -p $CONDA_PREFIX/etc/conda/activate.d\necho 'NVIDIA_DIR=$(dirname $(dirname $(python -c \"import nvidia.cudnn;print(nvidia.cudnn.__file__)\")))' &gt;&gt; $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\necho 'for dir in $NVIDIA_DIR/*; do     if [ -d \"$dir/lib\" ]; then         export LD_LIBRARY_PATH=\"$dir/lib:$LD_LIBRARY_PATH\";     fi; done' \n\n####make changes permanent\nconda deactivate \n\nconda activate tf16\n</code></pre> tensorflow 2.15 <pre><code>module load miniconda\nconda create -n tf15 python=3.11.*\npip install tensorflow[and-cuda]==2.15.*\n</code></pre> tensorflow 2.14 <pre><code>module load miniconda\nconda create -n tf14 python=3.11.*\nconda activate tf14\npip install tensorflow==2.14.0 nvidia-cuda-runtime-cu11==11.8.89 nvidia-cublas-cu11==11.11.3.6 nvidia-cufft-cu11==10.9.0.58 nvidia-cudnn-cu11==8.7.0.84 nvidia-curand-cu11==10.3.0.86 nvidia-cusolver-cu11==11.4.1.48 nvidia-cusparse-cu11==11.7.5.86 nvidia-nccl-cu11==2.16.5 nvidia-cuda-cupti-cu11==11.8.87 nvidia-cuda-nvcc-cu11==11.8.89\n\n# Store system paths to cuda libraries for gpu communication\nmkdir -p $CONDA_PREFIX/etc/conda/activate.d\necho 'CUDNN_PATH=$(dirname $(python -c \"import nvidia.cudnn;print(nvidia.cudnn.__file__)\"))' &gt;&gt; $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\necho 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/:$CUDNN_PATH/lib' &gt;&gt; $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\n\n#deactivate and reactivate environment to permanently keep cuda libraries\nconda deactivate\nconda activate tf14    \n</code></pre> tensorflow 2.13/2.12 <pre><code>module load miniconda\nconda create --name tf-condacuda python=3.11.* numpy pandas matplotlib jupyter cudatoolkit=11.8.0 \nconda activate tf-condacuda\npip install nvidia-cudnn-cu11==8.6.0.163\n\n# Store system paths to cuda libraries for gpu communication\nmkdir -p $CONDA_PREFIX/etc/conda/activate.d\necho 'CUDNN_PATH=$(dirname $(python -c \"import nvidia.cudnn;print(nvidia.cudnn.__file__)\"))' &gt;&gt; $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\necho 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/:$CUDNN_PATH/lib' &gt;&gt; $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\n\n#install tensorflow\npip install tensorflow==2.12.*\n\n#deactivate and reactivate environment to permanently keep cuda libraries\nconda deactivate\nconda activate tf-condacuda\n</code></pre> tensorflow 2.11 <pre><code>module load miniconda\nconda create --name tf-condacuda python=3.10.* numpy pandas matplotlib jupyter cudatoolkit=11.3.1 cudnn=8.2.1\nconda activate tf-condacuda\n\n# Store system paths to cuda libraries for gpu communication\nmkdir -p $CONDA_PREFIX/etc/conda/activate.d\necho 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/' &gt; $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\n\n#install tensorflow\npip install tensorflow==2.11.*\n\n#deactivate and reactivate environment to permanently keep cuda libraries\nconda deactivate\nconda activate tf-condacuda\n</code></pre>"},{"location":"clusters-at-yale/guides/tensorflow/#test-tensorflow-gpu-detection","title":"Test tensorflow gpu detection","text":"<p>If tensorflow is installed correctly, then it should be able to detect any GPUs that are allocated. You can test your tensorflow installation using these steps:</p> <pre><code>#####request compute allocation with a gpu node\nsalloc --partition=gpu_devel --cpus-per-task=1 --gpus=1 -t 4:00:00\n\n#####load tensorflow miniconda environment\n\nmodule load miniconda\nconda activate YOUR_TF_ENVIRONMENT\n\n#####run tensorflow validation test\npython3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n</code></pre> <p>This should print out a line of text that lists the recognized GPU. If this fails, please reach out to YCRC support for further assistance.</p>"},{"location":"clusters-at-yale/guides/tensorflow/#additional-tensorflow-packages","title":"Additional Tensorflow packages","text":""},{"location":"clusters-at-yale/guides/tensorflow/#ptxas-or-nvvm","title":"ptxas or nvvm","text":"<p>Use case: Tensorflow missing ptxas or complaining about can't find $CUDA/nvvm/libdevice:</p> <pre><code>module load miniconda\n\nconda activate YOUR_TF_ENVIRONMENT\n\nconda install -c nvidia cuda-nvcc\n\nmkdir -p $CONDA_PREFIX/etc/conda/activate.d\n\necho 'export XLA_FLAGS=--xla_gpu_cuda_data_dir=$CONDA_PREFIX' &gt;&gt; $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\n\nconda deactivate\n\nconda activate YOUR_TF_ENVIRONMENT\n</code></pre>"},{"location":"clusters-at-yale/guides/tensorflow/#tensorboard","title":"Tensorboard","text":"<p>Tensorboard comes installed with any tensorflow installation. Tensorboard is a visual software package for tensorflow that allows for graphical analysis of tensorflow processes. It is built to work in google colab and jupyter notebooks.</p> <p>However, it requires a web browser to function and must therefore be launched using remote desktop on OOD rather than OOD jupyter apps.</p>"},{"location":"clusters-at-yale/guides/tmux/","title":"tmux","text":"<p><code>tmux</code> is a \"terminal multiplexer\", it enables a number of terminals (or windows) to be accessed and controlled from a single terminal. <code>tmux</code> is a great way to save an interactive session between connections you make to the clusters. You can reconnect to the session from a workstation in your lab or from your laptop from home!</p>"},{"location":"clusters-at-yale/guides/tmux/#get-started","title":"Get Started","text":"<p>To begin a <code>tmux</code> session named myproject, type</p> <pre><code>tmux new -s myproject\n</code></pre> <p>You should see a bar across the bottom of your terminal window now that gives you some information about your session. If you are disconnected or detached from this session, anything you were doing will still be there waiting when you reattach</p> <p>The most important shortcut to remember is Ctrl+b (hold the ctrl or control key, then type \"b\"). This is how you signal to <code>tmux</code> that the following keystroke is meant for it and not the session you are working in. For example: if you want to gracefully detach from your session, you can type Ctrl+b, then d for detach. To reattach to our sample <code>tmux</code> session after detatching, type:</p> <pre><code>tmux attach -t myproject\n#If you are lazy and have only one session running,\n#This works too:\ntmux a\n</code></pre> <p>Lines starting with a \"#\" denote a commented line, which aren't read as code</p> <p>Finally, to exit, you can type <code>exit</code> or Ctrl+d</p>"},{"location":"clusters-at-yale/guides/tmux/#tmux-on-the-clusters","title":"tmux on the Clusters","text":"<p>Using tmux on the cluster allows you to create interactive allocations that you can detach from. Normally, if you get an interactive allocation (e.g. <code>salloc</code>) then disconnect from the cluster, for example by putting your laptop to sleep, your allocation will be terminated and your job killed. Using tmux, you can detach gracefully and tmux will maintain your allocation. Here is how to do this correctly:</p> <ol> <li>ssh to your cluster of choice</li> <li>Start tmux</li> <li>Inside your tmux session, submit an interactive job with <code>salloc</code>. See the Slurm documentation for more details</li> <li>Inside your job allocation (on a compute node), start your application (e.g. matlab)</li> <li>Detach from tmux by typing Ctrl+b then d</li> <li>Later, on the same login node, reattach by running <code>tmux attach</code></li> </ol> <p>Make sure to:</p> <ul> <li>run tmux on the login node, NOT on compute nodes</li> <li>run <code>salloc</code> inside tmux, not the reverse.</li> </ul> <p>Warning</p> <p>Every cluster has two login nodes.  If you cannot find your tmux session, it might be running on the other node.  Check the hostname of your current login node (from either your command prompt or from running <code>hostname -s</code>), then use ssh to login to the other one. For example, if you are logged in to grace1, use <code>ssh -Y grace2</code> to reach the other login node.</p>"},{"location":"clusters-at-yale/guides/tmux/#windows-and-panes","title":"Windows and Panes","text":"<p><code>tmux</code> allows you to create, toggle between and manipulate panes and windows in your session. A window is the whole screen that <code>tmux</code> displays to you. Panes are subdivisions in the curent window, where each runs an independent terminal. Especially at first, you probably won't need more than one pane at a time. Multiple windows can be created and run off-screen. Here is an example where this may be useful.</p> <p>Say you just submitted an interactive job that is running on a compute node inside your <code>tmux</code> session.</p> <pre><code>[ms725@grace1 ~]$ tmux new -s analysis\n# I am in my tmux session now\n[ms725@grace1 ~]$ salloc\n[ms725@c14n02 ~]$ ./my_fancy_analysis.sh\n</code></pre> <p>Now you can easily monitor its CPU and memory utilization without ever taking your eyes off of it by creating a new pane and running <code>top</code> there. Split your window by typing:</p> <p>Ctrl+b then %</p> <p><code>ssh</code> into the compute node you are working on, then run top to watch your work as it runs all from the same window.</p> <pre><code># I'm in a new pane now.\n[ms725@grace1 ~]$ ssh c14n02\n[ms725@c14n02 ~]$ top\n</code></pre> <p>Your view will look something like this:</p> <p></p> <p>To switch back and forth between panes, type Ctrl+b then o</p>"},{"location":"clusters-at-yale/guides/vasp/","title":"VASP","text":"<p>Note</p> <p>VASP requires a paid license. If you wish to use VASP on the cluster and your research group has purchased a license, please contact us to gain access to the cluster installation. Thank you for your cooperation.</p>"},{"location":"clusters-at-yale/guides/vasp/#vasp-and-slurm","title":"VASP and Slurm","text":"<p>In Slurm, there is big difference between <code>--ntasks</code> and <code>--cpus-per-task</code> which is explained in our Requesting Resources documentation.</p> <p>For the purposes of VASP, <code>--ntasks-per-node</code> should always equal <code>NCORE</code> (in your INCAR file). Then <code>--nodes</code> should be equal to the total number of cores you want, divided by <code>--ntasks-per-node</code>.</p> <p>VASP has two parameters for controlling processor layouts, <code>NCORE</code> and <code>NPAR</code>, but you only need to set one of them. If you set <code>NCORE</code>, you don\u2019t need to set <code>NPAR</code>. Instead VASP will automatically set <code>NPAR</code>.</p> <p>In your mpirun line, you should specify the number of MPI tasks as:</p> <pre><code>mpirun -n $SLURM_NTASKS vasp_std\n</code></pre>"},{"location":"clusters-at-yale/guides/vasp/#cores-layout-examples","title":"Cores Layout Examples","text":"<p>If you want 40 cores (2 nodes and 20 cpus per node):</p> <p>in your submission script:</p> <pre><code>#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=20\n</code></pre> <pre><code>mpirun -n 2 vasp_std\n</code></pre> <p>in <code>INCAR</code>:</p> <pre><code>NCORE=20\n</code></pre> <p>You may however find that the wait time to get 20 cores on two nodes can be very long since cores request via <code>--cpus-per-task</code> can\u2019t span multiple nodes. Instead you might want to try breaking it up into smaller chunks. Therefore, try:</p> <p>in your submission script:</p> <pre><code>#SBATCH --nodes=4\n#SBATCH --ntasks-per-node=10\n</code></pre> <pre><code>mpirun -n 4 vasp_std\n</code></pre> <p>in <code>INCAR</code>:</p> <pre><code>NCORE=10\n</code></pre> <p>which would likely spread over 4 nodes using 10 cores each and spend less time in the queue.</p>"},{"location":"clusters-at-yale/guides/vasp/#grace-mpi-partition","title":"Grace mpi partition","text":"<p>On Grace's <code>mpi</code> parttion, since cores are assigned as whole 24-core nodes, <code>NCORE</code> should always be equal to 24 and then you can just request <code>ntasks</code> in multiples of 24.</p> <p>in your submission script:</p> <pre><code>#SBATCH --ntasks=48 # some multiple of 24\n</code></pre> <pre><code>mpirun -n $SLURM_NTASKS vasp_std\n</code></pre> <p>in <code>INCAR</code>:</p> <pre><code>NCORE=24\n</code></pre>"},{"location":"clusters-at-yale/guides/vasp/#additional-performance","title":"Additional Performance","text":"<p>Some users have found that if they actually assign 2 MPI tasks per node (rather than 1), they see even better performance because the MPI tasks doesn't span the two sockets on the node. To try this, set <code>NCORE</code> to half of your nodes' core count and increase <code>mpirun -n</code> to twice the number of nodes you requested.</p>"},{"location":"clusters-at-yale/guides/vasp/#additional-reading","title":"Additional Reading","text":"<p>Here is some documentation on how to optimally configure NCORE and NPAR:</p> <ul> <li>https://www.vasp.at/wiki/index.php/NCORE</li> <li>https://www.vasp.at/wiki/index.php/NPAR</li> <li>https://www.nsc.liu.se/~pla/blog/2015/01/12/vasp-how-many-cores/</li> </ul>"},{"location":"clusters-at-yale/guides/virtualgl/","title":"VirtualGL","text":""},{"location":"clusters-at-yale/guides/virtualgl/#why-virtualgl","title":"Why VirtualGL","text":"<p>To display a 3D application running remotely on a cluster, you could use X11 forwarding  to display the application on your local machine. This is usually very slow and often unusable. </p> <p>An alternative approach is to use VNC - also called Remote Desktop - to run GUI applications remotely on the cluster.  This approach only works well with applications that only need moderate 3D rendering where software rendering is good enough. For applications that need to render large complicated models, hardware accelerated 3D rendering must be used.  </p> <p>However, VNC cannot directly utilize the graphic devices on the cluster for rendering.  VirtualGL, in conjunction with VNC, provides a commonly used  solution for remote 3D rendering with hardware acceleration.</p>"},{"location":"clusters-at-yale/guides/virtualgl/#how-to-use-virtualgl","title":"How to use VirtualGL","text":"<p>VirtualGL 3.0+ supports the traditional GLX back end and the new EGL back end for 3D rendering. </p> <p>The EGL back end uses a DRI (Direct Rendering Infrastructure) device to access a graphics device,  while the GLX back end uses an X server to access a graphics device.  The EGL back end allows simultaneous jobs on the same node, each using their own dedicated GPU device for rendering. Although it can render many applications properly, the EGL back end may fail to render some applications. The GLX back end supports a wider range of OpenGL applications than the EGL back end, however, only one X server can work properly with the graphics devices on the node. This means only one job can use the GLX back end on any GPU node, no matter how many GPU devices the node has.</p> <p>We suggest you use the EGL back end first. If it does not render your application properly, then switch to the GLX back end. </p> <p>We have provided a wrapper script <code>ycrc_vglrun</code> to make it easy for you to choose which back end to use for 3D rendering. In the following examples, we will use ParaView (unless mentioned otherwise) to demonstrate how to use <code>ycrc_vglrun</code>.</p> <p>Note</p> <p>If you need to run a hardware accelerated GUI application, you should first start a Remote Desktop on a GPU node, and then run the application from the shell in the Remote Desktop as shown below. We have not incorporated VirtualGL into the standalone interactive Apps on OOD that could benefit from VirtualGL. However, this could change in the future.</p>"},{"location":"clusters-at-yale/guides/virtualgl/#use-virtualgl-with-the-egl-back-end","title":"Use VirtualGL with the EGL back end","text":"<p>EGL is the default back end which <code>ycrc_vglrun</code> will choose to use if no option is provided. You can also add the <code>-e</code>  option to choose the EGL back end explicitly. </p> <pre><code>module load ParaView\nycrc_vglrun paraview\n</code></pre> <pre><code>module load ParaView\nycrc_vglrun -e paraview\n</code></pre>"},{"location":"clusters-at-yale/guides/virtualgl/#use-virtualgl-with-the-glx-back-end","title":"Use VirtualGL with the GLX back end","text":"<p>If your application cannot be rendered properly with the EGL back end, your next step is to try the GLX back end.  You should choose it explicitly with the <code>-g</code> option. </p> <pre><code>module load ParaView\nycrc_vglrun -g paraview\n</code></pre>"},{"location":"clusters-at-yale/guides/virtualgl/#run-matlab-with-hardware-opengl-rendering","title":"Run MATLAB with hardware OpenGL rendering","text":"<p>By default, MATLAB will use software OpenGL rendering. To run MATLAB with hardware OpenGL rendering,  add <code>-nosoftwareopengl</code>. <pre><code>module load MATLAB\nycrc_vglrun matlab -nosoftwareopengl\n</code></pre></p>"},{"location":"clusters-at-yale/guides/virtualgl/#troubleshoot","title":"Troubleshoot","text":""},{"location":"clusters-at-yale/guides/virtualgl/#nvidia-smi-or-vglrun-cannot-be-found","title":"<code>nvidia-smi</code> or <code>vglrun</code> cannot be found","text":"<p>You must submit your job to a GPU node. If you are using the Remote Desktop from OOD,  make sure you have specified <code>gpu</code> as <code>1</code> and <code>partition</code> as <code>gpu</code> or any other partition with GPU nodes.</p>"},{"location":"clusters-at-yale/guides/virtualgl/#glx-back-end-is-used-by-another-application","title":"GLX back end is used by another application","text":"<p>If you get the following message when running your application with the GLX back end, you need to add <code>--exclude=nodename</code> to <code>Advanced options</code> in the Remote Desktop OOD user interface and resubmit Remote Desktop. Replace <code>nodename</code> with the actual node name from the message. </p> <pre><code>    VirtualGL with the GLX back end is currently used by another application.\n    Please resubmit your job with\n\n              --exclude=c22n01\n</code></pre>"},{"location":"clusters-at-yale/guides/xvfb/","title":"Virtual Frame Buffer for Batch Mode","text":"<p>Often there is a need to run a program with a graphical interface in batch mode. This can be either due to extended run-time or the desire to run many instances of the process at once.  In either case the lack of a display can prevent the program from running.</p> <p>A solution has been developed to create a virtual display that only lives in memory. This allows the program to happily launch its graphical interface while in batch mode.</p> <p>Note</p> <p>It is common for R to require a display session to save certain types of figures.  You may see a warning like \"unable to start device PNG\" or \"unable to open connection to X11 display\". <code>xvfb</code> can help avoid these issues.</p> <p>This tool is called the <code>X Virtual Frame Buffer</code> or <code>xvfb</code>. It can act as a wrapper to your script which creates a virtual display session. For example, to run an R script (e.g. <code>make_jpeg.R</code>)  which needs a display session in order to save a JPEG file:</p> <pre><code>xvfb-run Rscript make_jpeg.R\n</code></pre> <p>For more details and other examples see the <code>xvfb-run</code> man page by running <code>man xvfb-run</code> on any compute node.</p>"},{"location":"clusters-at-yale/job-scheduling/","title":"Run Jobs with Slurm","text":"<p>Performing computational work at scale in a shared environment involves organizing everyone's work into jobs and scheduling them. We use Slurm to schedule and manage jobs on the YCRC clusters. </p> <p>Submitting a job involves specifying a resource request then running one or more commands or applications. These requests take the form of options to the command-line programs <code>salloc</code> and <code>sbatch</code> or those same options as directives inside submission scripts. Requests are made of groups of compute nodes (servers) called partitions. Partitions, their defaults, limits, and purposes are listed on each cluster page. Once submitted, jobs wait in a queue and are subject to several factors affecting scheduling priority. When your scheduled job begins, the commands or applications you specify are run on compute nodes the scheduler found to satisfy your resource request. If the job was submitted as a batch job, output normally printed to the screen will be saved to file.</p> <p>Please be a good cluster citizen.</p> <ul> <li>Do not run heavy computation on login nodes (e.g. <code>grace1</code>, <code>login1.mccleary</code>). Doing so negatively impacts everyone's ability to interact with the cluster.</li> <li>Make resource requests for your jobs that reflect what they will use. Wasteful job allocations slow down everyone's work on the clusters. See our documentation on Monitoring CPU and Memory Usage for how to measure job resource usage.</li> <li>If you plan to run many similar jobs, use our Dead Simple Queue tool or job arrays - we enforce limits on job submission rates on all clusters.</li> </ul> <p>If you find yourself wondering how best to schedule a job, please contact us for some help.</p>"},{"location":"clusters-at-yale/job-scheduling/#common-slurm-commands","title":"Common Slurm Commands","text":"<p>For an exhaustive list of commands and their official manuals, see the SchedMD Man Pages. Below are some of the most common commands used to interact with the scheduler.</p> <p>Submit a script called <code>my_job.sh</code> as a job (see below for details):</p> <pre><code>sbatch my_job.sh\n</code></pre> <p>List your queued and running jobs:</p> <pre><code>squeue --me\n</code></pre> <p>Cancel a queued job or kill a running job, e.g. a job with ID 12345:</p> <pre><code>scancel 12345\n</code></pre> <p>Check status of a job, e.g. a job with ID 12345:</p> <pre><code>sacct -j 12345\n</code></pre> <p>Check how efficiently a job ran, e.g. a job with ID 12345:</p> <p><pre><code>seff 12345\n</code></pre> See our Monitor CPU and Memory page for more on tracking the resources your job actually uses.</p> <p></p>"},{"location":"clusters-at-yale/job-scheduling/#common-job-request-options","title":"Common Job Request Options","text":"<p>These options modify the size, length and behavior of jobs you submit. They can be specified when calling <code>salloc</code> or <code>sbatch</code>, or saved to a batch script. Options specified on the command line to <code>sbatch</code> will override those in a batch script. See our Request Compute Resources page for discussion on the differences between <code>--ntasks</code> and <code>--cpus-per-task</code>, constraints, GPUs, etc. If options are left unspecified defaults are used.</p> Long Option Short Option Default Description <code>--job-name</code> <code>-J</code> Name of script Custom job name. <code>--output</code> <code>-o</code> <code>\"slurm-%j.out\"</code> Where to save <code>stdout</code> and <code>stderr</code> from the job. See filename patterns for more formatting options. <code>--partition</code> <code>-p</code> Varies by cluster Partition to run on. See individual cluster pages for details. <code>--account</code> <code>-A</code> Your group name Specify if you have access to multiple private partitions. <code>--time</code> <code>-t</code> Varies by partition Time limit for the job in D-HH:MM:SS, e.g. <code>-t 1-</code> is one day, <code>-t 4:00:00</code> is 4 hours. <code>--nodes</code> <code>-N</code> <code>1</code> Total number of nodes. <code>--ntasks</code> <code>-n</code> <code>1</code> Number of tasks (MPI workers). <code>--ntasks-per-node</code> Scheduler decides Number of tasks per node. <code>--cpus-per-task</code> <code>-c</code> <code>1</code> Number of CPUs for each task. Use this for threads/cores in single-node jobs. <code>--mem-per-cpu</code> <code>5G</code> Memory requested per CPU in MiB. Add <code>G</code> to specify GiB (e.g. <code>10G</code>). <code>--mem</code> Memory requested per node in MiB. Add <code>G</code> to specify GiB (e.g. <code>10G</code>). <code>--gpus</code> <code>-G</code> Used to request GPUs <code>--constraint</code> <code>-C</code> Constraints on node features. To limit kinds of nodes to run on. <code>--mail-user</code> Your Yale email Mail address (alternatively, put your email address in ~/.forward). <code>--mail-type</code> None Send email when jobs change state. Use <code>ALL</code> to receive email notifications at the beginning and end of the job."},{"location":"clusters-at-yale/job-scheduling/#interactive-jobs","title":"Interactive Jobs","text":"<p>Interactive jobs can be used for testing and troubleshooting code. Requesting an interactive job will allocate resources and log you into a shell on a compute node.</p> <p>You can start an interactive job using the <code>salloc</code> command. Unless specified otherwise using the <code>-p</code> flag (see above), all <code>salloc</code> requests will go to the <code>devel</code> partition on the cluster.</p> <p>For example, to request an interactive job with 8GB of RAM for 2 hours:</p> <pre><code>salloc -t 2:00:00 --mem=8G\n</code></pre> <p>This will assign one CPU and 8GiB of RAM to you for two hours. You can run commands in this shell as needed. To exit, you can type <code>exit</code> or Ctrl+d </p> <p>Use <code>tmux</code> with Interactive Sessions</p> <p>Remote sessions are vulnerable to being killed if you lose your network connection. We recommend using <code>tmux</code> alleviate this. When using <code>tmux</code> with interactive jobs, please take extra care to stop jobs that are no longer needed.</p>"},{"location":"clusters-at-yale/job-scheduling/#graphical-applications","title":"Graphical applications","text":"<p>Many graphical applications are well served with the Open OnDemand Remote Desktop app. If you would like to use X11 forwarding, first make sure it is installed and configured. Then, add the <code>--x11</code> flag to an interactive job request:</p> <pre><code>salloc --x11\n</code></pre>"},{"location":"clusters-at-yale/job-scheduling/#batch-jobs","title":"Batch Jobs","text":"<p>You can submit a script as a batch job, i.e. one that can be run non-interactively in batches. These submission scripts are comprised of three parts:</p> <ol> <li>A hashbang line specifying the program that runs the script. This is normally <code>#!/bin/bash</code>.</li> <li>Directives that list job request options. These lines must appear before any other commands or definitions, otherwise they will be ignored.</li> <li>The commands or applications you want executed during your job.</li> </ol> <p>See our page of Submission Script Examples for a few more, or the example scripts repo for more in-depth examples. Here is an example submission script that prints some job information and exits:</p> <pre><code>#!/bin/bash\n\n#SBATCH --job-name=example_job\n#SBATCH --time=2:00:00\n#SBATCH --mail-type=ALL\n\nmodule purge\nmodule load MATLAB/2021a\n\nmatlab -batch \"your_script\"\n</code></pre> <p>Save this file as <code>example_job.sh</code>, then submit it with:</p> <pre><code>sbatch example_job.sh\n</code></pre> <p>When the job finishes the output should be stored in a file called <code>slurm-jobid.out</code>, where <code>jobid</code> is the submitted job's ID. If you find yourself writing loops to submit jobs, instead use our Dead Simple Queue tool or job arrays.</p>"},{"location":"clusters-at-yale/job-scheduling/common-job-failures/","title":"Common Job Failures","text":"<p>Your jobs haven't failed, you have just found ways to run them that won't work. Here are some common error messages and steps to correct them.</p>"},{"location":"clusters-at-yale/job-scheduling/common-job-failures/#memory-limits","title":"Memory Limits","text":"<p>Jobs can fail due to an insufficient memory being requested. Depending on the job, this failure might present as a Slurm error:</p> <pre><code>slurmstepd: error: Detected 1 oom-kill event(s).\nSome of your processes may have been killed by the cgroup out-of-memory handler.\n</code></pre> <p>This means Slurm detected the job hitting the maximum requested memory and then the job was killed.</p> <p>When process inside a job tries to access memory outside what was allocated to that job (more than what you requested) the operating system tells your program that address is invalid with the fault <code>Bus Error</code>. A similar fault you might be more familiar with is a <code>Segmentation Fault</code>, which usually results from a program incorrectly trying to access a valid memory address.</p> <p>These errors can be fixed in two ways.</p>"},{"location":"clusters-at-yale/job-scheduling/common-job-failures/#request-more-memory","title":"Request More Memory","text":"<p>The default is almost always <code>--mem-per-cpu=5G</code></p> <p>In a batch script:</p> <pre><code>#SBATCH --mem-per-cpu=8G\n</code></pre> <p>In an interactive job:</p> <pre><code>salloc --mem-per-cpu=8G\n</code></pre>"},{"location":"clusters-at-yale/job-scheduling/common-job-failures/#use-less-memory","title":"Use Less Memory","text":"<p>This method is usually a little more involved, and is easier if you can inspect the code you are using. Watching your job's resource usage, attending a workshop, or getting in touch with us are good places to start.</p>"},{"location":"clusters-at-yale/job-scheduling/common-job-failures/#disk-quotas","title":"Disk Quotas","text":"<p>Since the clusters are shared resources, we have quotas in place to enforce fair use of storage. When you or your group reach a quota, you can't write to existing files or create new ones. Any jobs that depend on creating or writing files that count toward the affected quota will fail. To inspect your current usage, run the command <code>getquota</code>. Remember, your home quota is yours but your project, scratch60, and any purchased storage quotas are shared across your group.</p>"},{"location":"clusters-at-yale/job-scheduling/common-job-failures/#archive-files","title":"Archive Files","text":"<p>You may find that some files or direcories for previous projects are no longer needed on the cluster. We recommend you archive these to recover space.</p>"},{"location":"clusters-at-yale/job-scheduling/common-job-failures/#delete-files","title":"Delete Files","text":"<p>If you are sure you no longer need some files or direcories, you can delete them. Unless files are in your home directory (not <code>project</code> or <code>scratch60</code>) they are not backed up and may be unrecoverable. Use the <code>rm -rf</code> command very carefully.</p>"},{"location":"clusters-at-yale/job-scheduling/common-job-failures/#buy-more-space","title":"Buy More Space","text":"<p>If you would like to purchase more than the default quotas, we can help you buy space on the clusters.</p>"},{"location":"clusters-at-yale/job-scheduling/common-job-failures/#rate-limits","title":"Rate Limits","text":"<p>We rate-limit job submissions to 200 jobs per hour on each cluster. This limit helps even out load on the scheduler and encourages good practice. When you hit this limit, you will get an error when submitting new jobs that looks like this:</p> <pre><code>sbatch: error: Reached jobs per hour limit\nsbatch: error: Batch job submission failed: Job violates accounting/QOS policy (job submit limit, user's size and/or time limits)\n</code></pre> <p>You will then need to wait until your submission rate drops. </p>"},{"location":"clusters-at-yale/job-scheduling/common-job-failures/#use-job-arrays","title":"Use Job Arrays","text":"<p>To avoid hitting this limit and make large numbers of jobs more manageable, you should use Dead Simple Queue or job arrays. If you need help adapting your workflow to <code>dsq</code> or job arrays contact us.</p>"},{"location":"clusters-at-yale/job-scheduling/common-job-failures/#software-modules","title":"Software Modules","text":"<p>We build and organize software modules on the cluster using toolchains. The major toolchains we use produce modules that end in foss-yearletter or intel-yearletter, e.g. <code>foss-2018b</code> or <code>intel-2018a</code>. If modules from different toolchains are loaded at the same time, the conflicts that arise often lead to errors or strange application behavior. Seeing either of the following messages is a sign that you are loading incompatible modules. </p> <pre><code>The following have been reloaded with a version change:\n  1) FFTW/3.3.7-gompi-2018a =&gt; FFTW/3.3.8-gompi-2018b\n  2) GCC/6.4.0-2.28 =&gt; GCC/7.3.0-2.3.0\n  3) GCCcore/6.4.0 =&gt; GCCcore/7.3.0\n...\n</code></pre> <p>or</p> <pre><code>GCCcore/7.3.0 exists but could not be loaded as requested.\n</code></pre>"},{"location":"clusters-at-yale/job-scheduling/common-job-failures/#match-or-purge-your-toolchains","title":"Match or Purge Your Toolchains","text":"<p>Where possible, only use one toolchain at a time. When you want to use software from muliple toolchains run <code>module purge</code> between running new <code>module load</code> commands. If your work requires a version of software that is not installed, contact us.</p>"},{"location":"clusters-at-yale/job-scheduling/common-job-failures/#conda-environments","title":"Conda Environments","text":"<p>Conda environments provide a nice way to manage <code>python</code> and <code>R</code> packages and modules. Conda acieves this by setting functions and environment variables that point to your environment files when you run <code>conda activate</code>. Unlike modules, conda environments are not completely forwarded into a job; having a conda environment loaded when you submit a job doesn't forward it well into your job. You will likely see messages about missing packages and libraries you definitely installed into the environment you want to use in your job.</p>"},{"location":"clusters-at-yale/job-scheduling/common-job-failures/#load-conda-environments-right-before-use","title":"Load Conda Environments Right Before Use","text":"<p>To make sure that your environment is set up properly for interactive use, wait until you are on the host you plan to use your environment on. Then run <code>conda activate my_env</code>.</p> <p>To make sure batch jobs function properly, only submit jobs without an environment loaded (<code>conda deactivate</code> before <code>sbatch</code>). Make sure you load miniconda and your environment in the body of your batch submission script.</p>"},{"location":"clusters-at-yale/job-scheduling/dependency/","title":"Jobs with Dependencies","text":"<p>SLURM offers a tool which can help string jobs together via dependencies.  When submitting a job, you can specify that it should wait to run until a specified job has finished.  This provides a mechanism to create simple pipelines for managing complicated workflows. </p>"},{"location":"clusters-at-yale/job-scheduling/dependency/#simple-pipeline","title":"Simple Pipeline","text":"<p>As a toy example, consider a two-step pipeline, first a data transfer followed by an analysis step.  Here we will use the <code>--dependency</code> flag for sbatch and the <code>afterok</code> type that requires a job to finish successfully before starting the second step:</p> <p>The first step is controlled by a <code>sbatch</code> submission script called <code>step1.sh</code>:</p> <p><pre><code>#!/bin/bash\n#SBATCH --job-name=DataTransfer\n#SBATCH -t 30:00\n\nrsync -avP remote_host:/path/to/data.csv $HOME/project/\n</code></pre> The second step is controlled by <code>step2.sh</code>:</p> <p><pre><code>#!/bin/bash\n#SBATCH --job-name=DataProcess\n#SBATCH -t 5:00:00\n\nmodule load miniconda\nsource activate my_env\npython my_script.py $HOME/project/data.csv\n</code></pre> When we submit the first step (using the command <code>sbatch step1.sh</code>) we obtain the jobid number for that job. We then submit the second step adding in the <code>--dependency</code> flag to tell Slurm that this job requires the first job to finish before it can start:</p> <pre><code>sbatch --dependency=afterok:56761133 step2.sh\n</code></pre> <p>When the 'transfer' job finishes successfully (without an error exit code) the 'processing' step will begin. While this is a simple dependency structure, it is possible to have multiple dependencies or more complicated structure.</p>"},{"location":"clusters-at-yale/job-scheduling/dependency/#job-clean-up","title":"Job Clean-up","text":"<p>One frequent use-case is a clean-up job that runs after all other jobs have finished.  This is a common way to collect results from processing multiple files into a single output file.  This can be done using the <code>--dependency=singleton:&lt;job_id&gt;</code> flag that will wait until all previously launched jobs with the same name and user have finished.</p> <pre><code>[tl397@grace1 ~]$ squeue -u tl397\n             JOBID PARTITION     NAME     USER    ST      SUBMIT_TIME       NODELIST(REASON)\n          12345670       day   JobName    tl397   R    2020-05-27T11:54     c01n08\n          12345671       day   JobName    tl397   R    2020-05-27T11:54     c01n08\n          ...\n          12345678       day   JobName    tl397   R    2020-05-27T11:54     c01n08\n          12345679       day   JobName    tl397   R    2020-05-27T11:54     c01n08\n\n[tl397@grace1 ~]$ sbatch --dependency=singleton --job-name=JobName cleanup.sh\n[tl397@grace1 ~]$ squeue -u tl397\n             JOBID PARTITION     NAME     USER    ST      SUBMIT_TIME       NODELIST(REASON)\n          12345670       day   JobName    tl397   R    2020-05-27T11:54     c01n08\n          12345671       day   JobName    tl397   R    2020-05-27T11:54     c01n08\n          ...\n          12345678       day   JobName    tl397   R    2020-05-27T11:54     c01n08\n          12345679       day   JobName    tl397   R    2020-05-27T11:54     c01n08\n          12345680       day   JobName    tl397   R    2020-05-27T11:54     (Dependency)\n</code></pre> <p>This last job will wait to run until all previous jobs with name <code>JobName</code> finish. </p>"},{"location":"clusters-at-yale/job-scheduling/dependency/#further-reading","title":"Further Reading","text":"<p>SLURM provides a number of options for logic controlling dependencies.  Most common are the two discussed above, but <code>--dependency=afternotok:&lt;job_id&gt;</code> can be useful to control behavior if a job fails.  Full discussion of the options can be found on the SLURM manual page for <code>sbatch</code> (https://slurm.schedmd.com/sbatch.html). A very detailed overview, with examples in both bash and python, can also be found at the NIH computing reference: https://hpc.nih.gov/docs/job_dependencies.html.</p>"},{"location":"clusters-at-yale/job-scheduling/dsq/","title":"Job Arrays with dSQ","text":"<p>Dead Simple Queue is a light-weight tool to help submit large batches of homogenous jobs to a Slurm-based HPC cluster. It wraps around slurm's sbatch to help you submit independent jobs as job arrays. Job arrays have several advantages over submitting your jobs in a loop:</p> <ul> <li>Your job array will grow during the run to use available resources, up to a limit you can set. Even if the cluster is busy, you probably get work done because each job from your array can be run independently.</li> <li>Your job will only use the resources needed to complete remaining jobs. It will shrink as your jobs finish, giving you and your peers better access to compute resources.</li> <li>If you run your array on a pre-emptable partition (scavenge on YCRC clusters), only individual jobs are preempted. Your whole array will continue.</li> </ul> <p>dSQ adds a few nice features on top of job arrays:</p> <ul> <li>Your jobs don't need to know they're running in an array; your job file is a great way to document what was done in a way that you can move to other systems relatively easily.</li> <li>You get a simple report of which job ran where and for how long</li> <li>dSQAutopsy can create a new job file that has only the jobs that didn't complete from your last run.</li> <li>All you need is Python 2.7+, or Python 3.</li> </ul> <p>dSQ is not recommended for situations where the initialization of the job takes most of its execution time and it is re-usable.  These situations are much better handled by a worker-based job handler.</p>"},{"location":"clusters-at-yale/job-scheduling/dsq/#step-1-create-your-job-file","title":"Step 1: Create Your Job File","text":"<p>First, you'll need to generate a job file.  Each line of this job file needs to specify exactly what you want run for each job, including any modules that need to be loaded or modifications to your environment variables.  Empty lines or lines that begin with <code>#</code> will be ignored when submitting your job array.  Note: slurm jobs start in the directory from which your job was submitted.</p> <p>For example, imagine that you have 1000 image files that correspond to individual samples you want to process with a python script (<code>my_script.py</code>) inside a conda environment (<code>env_name</code>).  Given some initial testing, you think that each job needs 4 GiB of RAM, and will run in less than 20 minutes.</p> <p>Create a file with the jobs you want to run, one per line.  A simple loop that prints your jobs should usually suffice.  A job can be a simple command invocation, or a sequence of commands.  You can call the job file anything, but for this example assume it's called \"joblist.txt\" and contains:</p> <pre><code>module load miniconda; conda activate env_name; python my_script.py image_000.jpg\nmodule load miniconda; conda activate env_name; python my_script.py image_001.jpg\n...\nmodule load miniconda; conda activate env_name; python my_script.py image_999.jpg\n</code></pre> <p>Avoid Very Short Jobs</p> <p>When building your job file, please bundle very short jobs (less than a minute) such that each element of the job array will run for at least 10 minutes. You can do this by putting multiple tasks on a single line, separated by a <code>;</code>.  In the same vein, avoid jobs that simply check for a previous successful completion and then exit.  See dSQAutopsy below for a way to completely avoid submitting these types of jobs.</p> <p>Our clusters are not tuned for extremely high throughput jobs.  Therefore, large numbers of very short jobs put a lot of strain on both the scheduler, resulting in delays in scheduling other users' jobs, and the storage, due to large numbers of I/O operations.</p>"},{"location":"clusters-at-yale/job-scheduling/dsq/#step-2-generate-batch-script-with-dsq","title":"Step 2: Generate Batch Script with <code>dsq</code>","text":"<p>On YCRC clusters you can load Dead Simple Queue onto your path with:</p> <pre><code>module load dSQ\n</code></pre> <p>You can also download or clone this repo and use the scripts directly.</p> <p><code>dsq</code> takes a few arguments, then writes a job submission script (default) or can directly submit a job for you. The resources you request will be given to each job in the array (each line in your job file), e.g. requesting 2 GiB of RAM with dSQ will run each individual job with a separate 2 GiB of RAM available. Run <code>sbatch --help</code> or see the official Slurm documentation for more info on sbatch options. dSQ will set a default job name of dsq-jobfile (your job file name without the file extension). dSQ will also set the job output file name pattern to dsq-jobfile-%A_%a-%N.out, which will capture each of your jobs' output to a file with the job's ID(%A), its array index or zero-based line number(%a), and the host name of the node it ran on (%N). If you are handling output in each of your jobs, set this to <code>/dev/null</code>, which will stop these files from being created.</p> <pre><code>Required Arguments:\n  --job-file jobs.txt   Job file, one self-contained job per line.\n\nOptional Arguments:\n  -h, --help            Show this help message and exit.\n  --version             show program's version number and exit\n  --batch-file sub_script.sh\n                        Name for batch script file. Defaults to dsq-jobfile-YYYY-MM-DD.sh\n  -J jobname, --job-name jobname\n                        Name of your job array. Defaults to dsq-jobfile\n  --max-jobs number     Maximum number of simultaneously running jobs from the job array.\n  -o fmt_string, --output fmt_string\n                        Slurm output file pattern. There will be one file per line in your job file. To suppress slurm out files, set this to /dev/null. Defaults to dsq-jobfile-%A_%a-%N.out\n  --status-dir dir      Directory to save the job_jobid_status.tsv file to. Defaults to working directory.\n  --suppress-stats-file  Don't save job stats to job_jobid_status.tsv\n  --submit              Submit the job array on the fly instead of creating a submission script.\n</code></pre> <p>In the example above, we want walltime of 20 minutes and memory=4GiB per job. Our invocation would be:</p> <pre><code>dsq --job-file joblist.txt --mem-per-cpu 4g -t 20:00 --mail-type ALL\n</code></pre> <p>The <code>dsq</code> command will create a file called <code>dsq-joblist-yyyy-mm-dd.sh</code>, where the y, m, and d are today's date. After creating the batch script, take a look at its contents. You can further modify the Slurm directives in this file before submitting.</p> <pre><code>#!/bin/bash\n#SBATCH --array 0-999\n#SBATCH --output dsq-joblist-%A_%3a-%N.out\n#SBATCH --job-name dsq-joblist\n#SBATCH --mem-per-cpu 4g -t 10:00 --mail-type ALL\n\n# DO NOT EDIT LINE BELOW\n/path/to/dSQBatch.py --job-file /path/to/joblist.txt --status-dir /path/to/here\n</code></pre>"},{"location":"clusters-at-yale/job-scheduling/dsq/#step-3-submit-batch-script","title":"Step 3: Submit Batch Script","text":"<pre><code>sbatch dsq-joblist-yyyy-mm-dd.sh\n</code></pre>"},{"location":"clusters-at-yale/job-scheduling/dsq/#manage-your-dsq-job","title":"Manage Your dSQ Job","text":"<p>You can refer to any portion of your job with <code>jobid_index</code> syntax, or the entire array with its jobid. The index Dead Simple Queue uses starts at zero, so the 3rd line in your job file will have an index of 2. You can also specify ranges.</p> <pre><code># to cancel job 4 for array job 14567\nscancel 14567_4\n\n# to cancel jobs 10-20 for job 14567:\nscancel 14567_[10-20]\n</code></pre>"},{"location":"clusters-at-yale/job-scheduling/dsq/#dsq-output","title":"dSQ Output","text":"<p>You can monitor the status of your jobs in Slurm by using <code>squeue -u &lt;netid&gt;</code>, <code>squeue -j &lt;jobid&gt;</code>, or <code>dsqa -j &lt;jobid&gt;</code>.</p> <p>dSQ creates a file named <code>job_jobid_status.tsv</code>, unless you suppress this output with <code>--supress-stats-file</code>. This file will report the success or failure of each job as it finishes. Note this file will not contain information for any jobs that were canceled (e.g. by the user with scancel) before they began. This file contains details about the completed jobs in the following tab-separated columns:</p> <ul> <li>Job_ID: the zero-based line number from your job file.</li> <li>Exit_Code: exit code returned from your job (non-zero number generally indicates a failed job).</li> <li>Hostname: The hostname of the compute node that this job ran on.</li> <li>Time_Started: time started, formatted as year-month-day hour:minute:second.</li> <li>Time_Ended: time started, formatted as year-month-day hour:minute:second.</li> <li>Time_Elapsed: in seconds.</li> <li>Job: the line from your job file.</li> </ul>"},{"location":"clusters-at-yale/job-scheduling/dsq/#dsqautopsy","title":"dSQAutopsy","text":"<p>You can use dSQAutopsy or <code>dsqa</code> to create a simple report of the array of jobs, and a new jobsfile that contains just the jobs you want to re-run if you specify the original jobsfile. Options listed below</p> <pre><code>  -j JOB_ID, --job-id JOB_ID\n                        The Job ID of a running or completed dSQ Array\n  -f JOB_FILE, --job-file JOB_FILE\n                        Job file, one job per line (not your job submission script).\n  -s STATES, --states STATES\n                        Comma separated list of states to use for re-writing job file. Default: CANCELLED,NODE_FAIL,PREEMPTED\n</code></pre> <p>Asking for a simple report:</p> <pre><code>dsqa -j 13233846\n</code></pre> <p>Produces one</p> <pre><code>State Summary for Array 13233846\nState       Num_Jobs Indices  \n-----       -------- -------  \nCOMPLETED      12    4,7-17   \nRUNNING        5     1-3,5-6  \nPREEMPTED      1     0 \n</code></pre> <p>You can redirect the report and the failed jobs to separate files:</p> <pre><code>dsqa -j 2629186 -f jobsfile.txt &gt; re-run_jobs.txt 2&gt; 2629186_report.txt\n</code></pre>"},{"location":"clusters-at-yale/job-scheduling/fairshare/","title":"Priority &amp; Wait Time","text":""},{"location":"clusters-at-yale/job-scheduling/fairshare/#job-priority-score","title":"Job Priority Score","text":""},{"location":"clusters-at-yale/job-scheduling/fairshare/#fairshare","title":"Fairshare","text":"<p>To ensure well-balanced access to cluster resources, we institute a fairshare system on our clusters. In practice this means jobs have a priority score that dictates when it can be run in relation to other jobs. This score is affected by the amount of CPU-equivalent hours used by a group in the past few weeks. The number of CPU-equivalents allocated to a job is defined as the larger of (a) the number of requested cores and (b) the total amount of requested memory divided by the default memory per core (usually 5G/core). If a group has used a large amount of CPU-equivalent hours, their jobs are given a lower priority score and therefore will take longer to start if the cluster is busy. Regardless of a job's prority, the scheduler still considers all jobs for backfill (see below).</p> <p>To see all pending jobs sorted by priority (jobs with higher priority at the top), use the following <code>squeue</code> command:</p> <pre><code>squeue --sort=-p -t PD -p &lt;partition_name&gt;\n</code></pre> <p>To monitor usage of members of your group, run the <code>sshare</code> command:</p> <pre><code>sshare -a -A &lt;group&gt;\n</code></pre> <p>Note: Resources used on private partitions do not count affect fairshare.</p> <p>Similarly, resources used in the scavenge partition cost 10% of comparable resources in the other partitions.</p>"},{"location":"clusters-at-yale/job-scheduling/fairshare/#length-of-time-in-queue","title":"Length of Time in Queue","text":"<p>In addition to fairshare, any pending job will accrue priority over time, which can help overcome small fairshare penalties. To see the factors affecting your job's priority, run the following <code>sprio</code> command:</p> <pre><code>sprio -j &lt;job_id&gt;\n</code></pre>"},{"location":"clusters-at-yale/job-scheduling/fairshare/#backfill","title":"Backfill","text":"<p>In addition to the main scheduling cycle, where jobs are run in the order of priority and availability of resources, all jobs are also considered for \"backfill\". Backfill is a mechanism which will let jobs with lower priority score start before high priority jobs if they can fit in around them. For example, if a higher priority job needs 4 nodes with 20 cores on each node and it will have to wait 30 hours for those resources to be available, if a lower priority job only needs a couple cores for an hour, Slurm will run that job in the meantime.</p> <p>For this reason, it is important to request accurate walltime limits for your jobs. If your job only requires 2 hours to run, but you request 24 hours, the likelihood that your job will be backfilled is greatly lowered. Moreover, for performance reasons, the backfill scheduler on Grace only looks at the top 10 jobs by each user. Therefore, if you bundle similar jobs into job arrays (see dSQ), the backfill cycle will consider more of your jobs since entire job arrays only count as one job for the limit accounting.</p>"},{"location":"clusters-at-yale/job-scheduling/getusage/","title":"Monitor Overall Slurm Usage","text":"<p>To enable research groups to monitor their combined utilization of cluster resources, we have developed a suite of <code>getusage</code> tools.  We perform nightly queries of Slurm's database to aggregate usage (in ServiceUnit-hours <code>su_hours</code>) broken down by user, account, and partition.  Service Units are a weighted combination of CPUs, memory, and GPUs allocated for each job.  The relative weights are derived from the approximate cost of these different resources. </p> Type Subtype Service Units Compute Hour* - 1 GPU Hour A5000 15 GPU Hour A100 100 <p>* Number of SUs per non-GPU compute job is the maximum of the CPU core count and the total RAM allocation/15GB. </p> <p>Usage data are available through each cluster's Open OnDemand and as a command-line utility.</p>"},{"location":"clusters-at-yale/job-scheduling/getusage/#open-ondemand-web-app","title":"Open OnDemand Web-app","text":"<p>The Open OnDemand User Portals host an interactive data dashboard that provide tables and visualization of Slurm utilization.</p> Cluster OOD site Grace ood-grace.ycrc.yale.edu/pun/sys/ycrc_userportal/clusterusage McCleary ood-mccleary.ycrc.yale.edu/pun/sys/ycrc_userportal/clusterusage Milgram ood-milgram.ycrc.yale.edu/pun/sys/ycrc_userportal/clusterusage <p>An example of such a view is shown below.</p> <p>Multiple Accounts</p> <p>If you belong to multiple Slurm Accounts, including priority tier accounts, these will be populated in the pull-down <code>Account</code> menu. </p> <p></p>"},{"location":"clusters-at-yale/job-scheduling/getusage/#command-line-getusage","title":"Command-line <code>getusage</code>","text":"<p>These aggrigates are collected from all clusters and made accessible to researchers by running <code>getusage</code>:</p> <pre><code>[testuser@login1.grace ~]$ getusage --help\n\n Usage: getusage [OPTIONS]\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --user       -u      TEXT  User name [default: Current user]               \u2502\n\u2502 --group      -g      TEXT  Slurm Account [default: Default Account]        \u2502\n\u2502 --cluster    -c      TEXT  Filter usage by cluster CLUSTER [default: All]  \u2502\n\u2502 --partition  -p            Break usage down by partition                   \u2502\n\u2502 --summary    -s            Only report monthly summary                     \u2502\n\u2502 --help                     Show this message and exit.                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>Multiple Accounts</p> <p>If you belong to multiple accounts, you can specify them with the <code>-g</code> flag.  By default, <code>getusage</code> displays information about your \"default\" Slurm Account. If you wish to view your secondary account's usage (or for priority tier accounts, specify them like: <code>getusage -g prio_account</code></p> <p>Running without any arguments produces a report for the full fiscal year (starting in July):</p> <pre><code>[testuser@login1.grace ~]$ getusage\nMonthly usage (in su_hours) for testuser\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                                 Standard         PI\n               Cluster  User\n2024 July      grace    user1      456.19       0.00\n               mccleary user2       33.67       0.00\n2024 August    grace    user1      366.15       0.00\n               mccleary user2       46.40       0.00\n2024 September grace    user1      360.24       0.00\n                        user3        2.02       7.28\n                        user5        0.01       0.00\n               mccleary user2       39.35       0.00\n2024 October   grace    user3      544.93   5,659.68\n               mccleary user2        5.28       0.00\n2024 November  grace    user4      526.84      14.27\n                        user3    4,442.07     169.76\n               mccleary user2       32.54       0.00\nMonthly Summary\nLatest month is in-progress (data updated daily at midnight)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                Standard            PI\n2024-07-31        489.87          0.00\n2024-08-31        412.55          0.00\n2024-09-30        401.62          7.28\n2024-10-31        550.22      5,659.68\n2024-11-30      5,001.44        184.03\n\nTotal usage:          12,706.68 Service-Unit Hours\n</code></pre> <p>Please reach out to hpc@yale.edu with any comments or suggestions about how we can improve <code>getusage</code>. </p>"},{"location":"clusters-at-yale/job-scheduling/jobstats/","title":"Job Performance Monitoring","text":"<p>We have recently deployed a new tool for measuring and monitoring job performance called <code>jobstats</code>.  Available on all clusters, <code>jobstats</code> provides a report of the utilization of CPU, Memory, and GPU resources for in-progress and recently completed jobs.  To generate the report simply run (replacing the ID number of the job in question):</p> <pre><code>[ab123@grace ~]$ jobstats 123456789\n================================================================================\n                              Slurm Job Statistics\n================================================================================\n         Job ID: 123456789\n  NetID/Account: ab123/group\n       Job Name: gpu_job\n          State: RUNNING\n          Nodes: 1\n      CPU Cores: 1\n     CPU Memory: 5GB\n           GPUs: 1\n  QOS/Partition: normal/gpu\n        Cluster: grace\n     Start Time: Tue Nov 26, 2024 at 2:10 PM\n       Run Time: 20:09:56 (in progress)\n     Time Limit: 2-00:00:00\n\n                              Overall Utilization\n================================================================================\n  CPU utilization  [||||||||||||||||||||||||||||||||||||||||||||||100%]\n  CPU memory usage [                                                1%]\n  GPU utilization  [|||||||||||||||||||||||||||||||||||||||||||||||98%]\n  GPU memory usage [|                                               3%]\n\n                              Detailed Utilization\n================================================================================\n  CPU utilization per node (CPU time used/run time)\n      r808u11n01: 20:07:01/20:09:56 (efficiency=99.8%)\n\n  CPU memory usage per node - used/allocated\n      r808u11n01: 32.4MB/5.0GB (32.4MB/5.0GB per core of 1)\n\n  GPU utilization per node\n      r808u11n01 (GPU 1): 98.3%\n\n  GPU memory usage per node - maximum used/total\n      r808u11n01 (GPU 1): 689.6MB/24.0GB (2.8%)\n\n                                     Notes\n================================================================================\n  * Have a nice day!\n</code></pre> <p>When viewed from a web-browser, these statistics are enhanced with plots of performance over time.</p> <p></p> <p>This is a great way to monitor your job's behavior and resource utilization over time. </p>"},{"location":"clusters-at-yale/job-scheduling/mpi/","title":"MPI Partition","text":"<p>Grace and Bouchet have special common partitions called <code>mpi</code>. The <code>mpi</code> partitions are a bit different from other partitions on YCRC clusters-- jobs submitted to the partition are always allocated full nodes.  Each node in the <code>mpi</code> partition are identical.  On Grace, these nodes are 24 core, 2x Skylake Gold 6136, 96GiB RAM (88GiB usable) nodes.  On Bouchet, these nodes are 64 core, 2x Emerald Rapids Platinum 8562Y+, 500GiB RAM (487GiB usable) nodes. While these partitions are available to all Grace/Bouchet users, only certain types of jobs are allowed on them (similar to the restrictions on our GPU partitions).</p> <p>In addition the the common partition <code>mpi</code>, there is a <code>scavenge_mpi</code> partition.  This partition is has the same purpose and limitations as the regular <code>mpi</code> partition, but allows users to run a lower priority (e.g. subject to preemption if nodes are requested in the <code>mpi</code> partition ) without incurring cpu charges.</p>"},{"location":"clusters-at-yale/job-scheduling/mpi/#appropriate-jobs","title":"Appropriate Jobs","text":"<p>This partition is specifically designed to support jobs that use tightly-coupled MPI-enabled applications that will run across multiple nodes and are sensitive to sharing their nodes with other jobs.  Since every node on the <code>mpi</code> partition is identical, it can support workloads that are sensitive to hardware difference across a single job. </p> <p>We expect most of jobs submitted to <code>mpi</code> to use all cores on each node.  There are occasionally instances where a tightly coupled application will use multiple nodes but less than all cores due to load balancing or memory limitations.  For example, some applications require power of 2 cores in the job, but 24 cores doesn't always divide evenly into those configurations.  So we occasionally see jobs that use multiple nodes but only 16 of the 24 cores per node and are also acceptable submissions to the <code>mpi</code> partition. </p> <p>Jobs that do not require exclusive nodes, even if they use <code>mpirun</code> to launch, will run fine and experience normal wait times in the day and week (and scavenge) partitions.  As such, we ask you to protect the special <code>mpi</code> partition nodes for the more resource sensitive jobs listed above and, therefore, submit any jobs that will not be using whole node(s) to the other partitions.  If smaller or single core jobs are submitted to the <code>mpi</code> partition, they may be cancelled without warning.  As with our GPU partitions, if you would like to make use of available cores on any <code>mpi</code> nodes for small jobs, the scavenge partition is the correct way to do that.</p> <p>If you have any questions about whether your workload is appropriate for the <code>mpi</code> partition, please contact us.</p>"},{"location":"clusters-at-yale/job-scheduling/mpi/#compilation","title":"Compilation","text":"<p>The <code>devel</code> partitions on Grace and Bouchet each have a node that is identical to the <code>mpi</code> partition nodes.  If you choose to compile your code with advanced optimization flags specific to the new generation of compute nodes, you can request that node in the <code>devel</code> partition:</p> <pre><code># Grace \n--partition devel --constraint skylake \n# Bouchet\n--partition devel --constraint cpugen:emeraldrapid\ns\n</code></pre>"},{"location":"clusters-at-yale/job-scheduling/mpi/#core-layouts","title":"Core Layouts","text":"<p>Please review the Request Compute Resources documentation for the appropriate Slurm flags for different types of core and node layouts.  If you have any questions, feel free to contact us.</p>"},{"location":"clusters-at-yale/job-scheduling/priority-tier/","title":"Priority Tier","text":""},{"location":"clusters-at-yale/job-scheduling/priority-tier/#overview","title":"Overview","text":"<p>Effective December 1st 2024, the current YCRC CPU-Hour based service charges has been replaced with new Priority Tier service charges. The YCRC has added a new Priority Tier of partitions that is an opt-in, fast lane for computational jobs.  All computation on the \u201cstandard\u201d tier of partitions (e.g. day, week, mpi, gpu)  no longer incur charges. Private nodes and scavenge partitions continue to not incur charges.</p> <p>The new compute charging model was developed in close collaboration with faculty, YCRC staff and university administrators to ensure the YCRC service charging models support the researchers who rely on our systems and the needs of the University.</p>"},{"location":"clusters-at-yale/job-scheduling/priority-tier/#access","title":"Access","text":"<p>Access to Priority Tier partitions is granted upon request through the Priority Tier Access Request Form. This form must be submitted by the group\u2019s PI (or delegate).</p> <p>During the Priority Tier onboarding process, the YCRC will require certain information before access can be granted.</p> <ul> <li>Charging instructions (COA)</li> <li>A list of members in your group who should have access (and therefore the privileges to incur charges). Additional group members can be added to Priority Tier at any time by submitted a request to hpc@yale.edu (all group members should already have cluster accounts requested through the Account Request Form).</li> <li>We also strongly recommend providing an annual usage limit, beyond which no additional computation on Priority Tier will occur (computation in Standard Tier will still be available at no cost). Note this limit can be changed at any time upon request.</li> </ul>"},{"location":"clusters-at-yale/job-scheduling/priority-tier/#how-to-use-priority-tier-partitions","title":"How to Use Priority Tier Partitions","text":""},{"location":"clusters-at-yale/job-scheduling/priority-tier/#job-submission","title":"Job Submission","text":"<p>Starting on December 1st, there is a new tier of partitions on Grace, McCleary and Milgram. Priority Tier partitions will be added to Bouchet when it enters production. Jobs submitted to a Priority Tier partition precede all pending jobs in the corresponding standard tier partitions in the scheduling queue to provide a \u201cfast-lane\u201d.  The Priority Tier partitions are composed of the YCRC\u2019s newest nodes and GPUs. Any compute resources not in use by a Priority Tier partition are available for use by the Standard Tier partitions.</p> Partition Description Grace McCleary Milgram <code>priority</code> similar to <code>day</code> Intel Ice Lake Nodes Intel Ice Lake Nodes Intel Cascade Lake Nodes <code>priority_gpu</code> similar to <code>gpu</code> A100, A5000 GPU-enabled Nodes A100, A5000 GPU-enabled Nodes NA <code>priority_mpi</code> similar to <code>mpi</code> Intel Skylake Nodes NA NA <p>At launch all Priority Tier partitions has a 7-day maximum wall time limit. Interactive jobs are permitted on Priority Tier partitions. Priority Tier jobs are still bound by YCRC policies and best practices, so users are expected to use interactive jobs mindfully and terminate their session when they are pausing their work.</p> <p>The expectation for a job submitted to Priority Tier partition is not necessarily that it will run immediately (as one experiences in <code>devel</code> or jobs preempting <code>scavenge</code> jobs) but rather that it will start before any Standard Tier jobs, when resources are available and it reaches the top of the Priority Tier queue relative to other Priority Tier jobs.</p>"},{"location":"clusters-at-yale/job-scheduling/priority-tier/#account-selection","title":"Account Selection","text":"<p>When you are granted access to Priority Tier, you will be added to one or more <code>prio_</code> Slurm group accounts. These group account names take the form <code>prio_groupname</code>, where <code>groupname</code> is the name of the Slurm group account used in the existing Standard Tier partitions. PIs can elect to have multiple Slurm group accounts for different projects, each with their own COA, for direct connection between certain computation and the associated grant or other source of funds. In these instances the additional Slurm group accounts will take the form <code>prio_groupname_projectid</code>.</p> <p>In either instance, the Priority Tier Slurm group account must be specified in the job submission script using the <code>-A</code> flag</p> <pre><code>#SBATCH -A prio_groupname\n### or\n#SBATCH -A prio_groupname_projectid\n</code></pre> <p>Only <code>prio_</code> groups can access the Priority Tier partitions and they cannot be used in the Standard Tier partitions (see below section on Fairshare for more information on why this is). </p>"},{"location":"clusters-at-yale/job-scheduling/priority-tier/#priority_gpu-partitions","title":"<code>priority_gpu</code> Partitions","text":"<p>To avoid unexpected costs due to the Service Unit differences between A100 GPUs and A5000 GPUs, we strongly recommend being specific about the GPU model if any job submissions.</p> <pre><code>#SBATCH --gpus=a100:1\n</code></pre>"},{"location":"clusters-at-yale/job-scheduling/priority-tier/#fairshare-and-concurrent-utilization-limits","title":"Fairshare and Concurrent Utilization Limits","text":"<p>All YCRC clusters are governed by a set of \"fairness\" control.  \"Fairshare\u201d is an algorithm that controls moment-to-moment priority of a job based on recent use of the cluster.  For example, jobs from heavy recent users/groups start at the end of the queue and work their way forward over time and jobs from new or low-usage users/groups start at the front of the queue.  CPU core hours, memory consumption and GPU hours all contribute at proportional levels to the usage incurred by running jobs.  The cluster scheduler also has \"concurrent utilization limits\" (QOSs) that prevent a single user or group from taking over a whole cluster, regardless of recent use and fairshare status. </p> <p>All accounts in the Priority Tier share a distinct fairshare pool from the one shared by Standard Tier accounts. Computations in Standard Tier will not affect your priority in Priority Tier and vice versa.</p> <p>At launch there are no concurrent utilization limits on Priority Tier but they may be instated at a later date based on demand and user behavior. Communications will be sent if and when this is being considered.</p>"},{"location":"clusters-at-yale/job-scheduling/priority-tier/#rate-structure","title":"Rate Structure","text":"<p>The new charging model for computations run on a Priority Tier partition is Service Unit (SU) based at a rate of $0.004/SU/hour. This rate is derived to closely match the prorated cost of a similar dedicated node over a 5 year expected lifetime. The SUs of a compute job are calculated as follows:</p> Type Subtype Service Units Cost per Hour Compute Hour* - 1 $0.004 GPU Hour A5000 15 $0.060 GPU Hour A100 100 $0.400 <p>* Number of SUs per non-GPU compute job is the maximum of the CPU core count and the total RAM allocation/15GB</p> <p>Usage is billed for actual runtime, not requested walltime of a job.  However, all compute resources (CPUs, memory, GPUs) allocated to a job are billed, regardless of whether a job makes use of those resources.</p> <p>Usage is billed monthly, with the bills expected the first week of the following month. To assist with cost estimates and budgeting, see below for tools for calculating charges.</p>"},{"location":"clusters-at-yale/job-scheduling/priority-tier/#annual-usage-limit","title":"Annual Usage Limit","text":"<p>We strongly encourage every group to set an annual usage limit for Priority Tier accounts to ensure Priority Tier expenses stay within expected bounds. This limit can be changed at any time but during the onboarding process YCRC can assist with setting a reasonable starting limit.</p> <p>As the annual usage limit is approached, you will no longer be able to submit any jobs that would (if they ran for their full requested walltime) over run the limit. If they choose, the PI (or delegate) of the group can request to have the limit increased. In the meantime, you can continue to run any computations in the Standard Tier of partitions which are always free of charge.</p>"},{"location":"clusters-at-yale/job-scheduling/priority-tier/#estimate-charges-and-review-usage","title":"Estimate Charges and Review Usage","text":"<p>To assist with cost estimates and budgeting, we provide a Cost Calculator. </p> <p>Usage to date can be monitored in the User Portal and on the cluster using the <code>getusage -g prio_groupname</code> command.</p>"},{"location":"clusters-at-yale/job-scheduling/priority-tier/#other-upcoming-improvements","title":"Other Upcoming Improvements","text":"<p>In conjunction with the new charging model, the YCRC is committed to making improvements to the ongoing tuning of fairshare and concurrent utilization algorithms and additional practices and tooling to enable users to make efficient use of the systems. One such improvement available today is the User Portal where researchers can view information about their activity on our clusters. Keep an eye on the YCRC Bulldog User News in coming months for information about these improvements as we roll them out.</p>"},{"location":"clusters-at-yale/job-scheduling/resource-requests/","title":"Request Compute Resources","text":""},{"location":"clusters-at-yale/job-scheduling/resource-requests/#request-cores-and-nodes","title":"Request Cores and Nodes","text":"<p>When running jobs with Slurm, you must be explicit about requesting CPU cores and nodes. See our page on monitoring usage for tips on verifying your jobs are using the resources you expect. The three options <code>--nodes</code> or <code>-N</code>, <code>--ntasks</code> or <code>-n</code>, and <code>--cpus-per-task</code> or <code>-c</code> can be a bit confusing at first but are necessary to understand for applications that use more than one CPU.</p> <p>Tip</p> <p>If your application references threads or cores but makes no mention of MPI, only use <code>--cpus-per-task</code> to request CPUs. You cannot request more cores than there are on a single compute node where your job runs.</p>"},{"location":"clusters-at-yale/job-scheduling/resource-requests/#multi-thread-multi-process-and-mpi","title":"Multi-thread, Multi-process, and MPI","text":"<p>The majority of applications in the world were written to use one or more cores on a single computer.  Most can only use one core, and do not benefit from being given more cores. The best way to speed these applications up is to run many separate jobs at once, using Dead Simple Queue or job arrays. </p> <p>If an application is able to use multiple cores, it usually achieves this by either spawning threads and sharing memory (multi-threaded) or starting entire new processes (multi-process). Some applications are written to use the Message Passing Interface (MPI) standard to run across many compute nodes. This allows such applications to scale computation in a way not limited by the number of cores on a single node. MPI translates what Slurm calls tasks to separate workers or processes. Because each of these processes can communicate across compute nodes, Slurm does not constrain them to the same node by default. Though tasks can be distributed across nodes, Slurm will not split the CPUs allocated to individual tasks. For this reason a single task that has multiple CPUs allocated will always be on a single node. In some cases using <code>--ntasks=4</code> (or <code>-n 4</code>) and <code>--cpus-per-task=4</code> (or <code>-c 4</code>) achieves the same job allocation by luck, but you should only use <code>--cpus-per-task</code> when using non-MPI applications to guarantee that the CPUs you expect your program to use are all accessable.</p> <p>Some MPI programs are also multi-threaded, so each process can use multiple CPUs. Only these applications can use <code>--ntasks</code> and <code>--cpus-per-task</code> to run faster.</p>"},{"location":"clusters-at-yale/job-scheduling/resource-requests/#mpi-applications","title":"MPI Applications","text":"<p>For more control over how Slurm lays out your job, you can add the <code>--nodes</code> and <code>--ntasks-per-node</code> flags. <code>--nodes</code> specifies how many nodes to allocate to your job. Slurm will allocate your requested number of cores to a minimal number of nodes on the cluster, so it is likely if you request a small number of tasks that they will all be allocated on the same node. However, to ensure they are on the same node, set <code>--nodes=1</code> (obviously this is contingent on the number of CPUs on your cluster's nodes and requesting too many may result in a job that will never run). Conversely, if you would like to ensure a specific layout, such as one task per node for memory, I/O or other reasons, you can also set <code>--ntasks-per-node=1</code>. Note that the following must be true:</p> <pre><code>ntasks-per-node * nodes &gt;= ntasks\n</code></pre>"},{"location":"clusters-at-yale/job-scheduling/resource-requests/#hybrid-mpiopenmp-applications","title":"Hybrid (MPI+OpenMP) Applications","text":"<p>For the most predictable performance for hybrid applications, you will need to use all three of the <code>--ntasks</code>, <code>--cpus-per-task</code>, and <code>--nodes</code> flags, where <code>--ntasks</code> equals the number of MPI tasks, <code>--cpus-per-task</code> equals the number of <code>OMP_NUM_THREADS</code> and <code>--nodes</code> is the number of nodes required to fit <code>--ntasks * --cpus-per-task</code>.</p>"},{"location":"clusters-at-yale/job-scheduling/resource-requests/#request-memory-ram","title":"Request Memory (RAM)","text":"<p>Slurm strictly enforces the memory your job can use. If you request 5GiB of memory for your job and the total used by all processes you launch hits that limit, some of your processes may die and you will get errors. Make sure you either request the right amount of memory per core on each node in your job with <code>--mem-per-cpu</code> or memory per node in your job with <code>--mem</code>. You can request more memory than you think you might need for an example job, then make note of its actual usage to better tune future requests for similar jobs.</p>"},{"location":"clusters-at-yale/job-scheduling/resource-requests/#request-gpus","title":"Request GPUs","text":"<p>Some of our clusters have nodes that contain GPU co-processors. Please refer to the individual cluster pages regarding node configurations that include GPUs. There are several <code>salloc</code>/<code>sbatch</code> options that allow you to request GPUs and specify your job layout relative to the GPUs requested.</p> Long Option Short Option Description <code>--cpus-per-gpu</code> Use instead of <code>--cpus-per-task</code> to specify number of CPUs per allocated GPU. <code>--gpus</code> <code>-G</code> Specify the total number of GPUs required for the job either with number or type:number. <code>--gpus-per-node</code> Specify the number of GPUs per node, either with number or type:number. New option similar to <code>--gres=gpu</code>. <code>--gpus-per-task</code> Specify the number of GPUs per task, either with number or type:number. <code>--mem-per-gpu</code><sup>*</sup> Request system memory that scales per GPU. The <code>--mem</code>, <code>--mem-per-cpu</code> and <code>--mem-per-gpu</code> options are mutually exclusive <code>--constraint</code> <code>-C</code> Request a selection of GPU types (separate types with <code>|</code>). This option requires the <code>--gpus</code> option for GPU selection. <p><sup>* The <code>--mem-per-gpu</code> flag does not currently work as intended, please do not use. Request memory using <code>--mem</code> or <code>--mem-per-cpu</code> in the meantime.</sup></p> <p>In order for your job to be able to access gpus, you must submit your job to a partition that contains nodes with GPUs and request them - the default GPU request for jobs is to not request any. Some applications require double-precision capable GPUs. If yours does, see the next section for using \"features\" to request any node with compatible GPUs. The Slurm options <code>--mem</code>, <code>--mem-per-gpu</code> and <code>--mem-per-cpu</code> do not request memory on GPUs, sometimes called vRAM. Instead you are allocated the GPU(s) requested and all attached GPU memory for your jobs. Memory accessible on GPUs is limited by their model, and is also listed on each cluster page. </p>"},{"location":"clusters-at-yale/job-scheduling/resource-requests/#request-specific-gpu-types","title":"Request Specific GPU Types","text":"<p>If your job can only run on a subset of the GPU types available in the partition, you can request one or more specific types of GPUs. </p> <p>To request a specific type of GPU, use <code>type:number</code> notation. For example, to request an NVIDIA P100.</p> <pre><code>sbatch --cpus-per-gpu=2 --gpus=p100:1 --time=6:00:00 --partition gpu my_gpu_job.sh\n</code></pre> <p>To submit your job to a number of GPU options (such as NVIDIA P100, V100 or A100), use a combination of the constraint flag (<code>-C</code>) and the <code>--gpus</code> flag (with just a number). For the constraint flag, separate the different GPU type names with the pipe character (<code>|</code>). Your job will then start on a node with any of those GPU types. This is not guaranteed to work as expected if you are requesting multiple nodes. GPU type names can be found in the partition tables on each respective cluster page.</p> <pre><code>sbatch -C \"p100|v100|a100\" --gpus=1 --time=6:00:00 --partition gpu my_gpu_job.sh\n</code></pre> <p>Tip</p> <p>As with requesting multiple cores or multiple nodes, we strongly recommend that you test your jobs using the <code>gpu_devel</code> partition to make sure they can well utilize multiple GPUs before requesting them; allocating more GPUs does not speed up code that can only use one at a time. Here is an example interactive request that would allocate two GPUs and four CPUs for thirty minutes:</p> <pre><code>salloc --cpus-per-gpu=2 --gpus=2 --time=30:00 --partition gpu_devel\n</code></pre> <p>For more documentation on using GPUs on our clusters, please see GPUs and CUDA.</p>"},{"location":"clusters-at-yale/job-scheduling/resource-requests/#features-and-constraints","title":"Features and Constraints","text":"<p>You may want to run programs that require specific hardware. To ensure your job runs on specific types of nodes, use the <code>--constraint</code> flag.</p> <p>You can use the processor codename (e.g. <code>haswell</code>) or processor type (e.g. <code>E5-2660_v3</code>) to limit your job to specific node types. You can also specify an instruction set (e.g. <code>avx512</code>) to require that no matter what CPU your job runs on, it must understand at least these instructions. See the individual cluster pages for the exact tags for the different node types. Multiple requirements (\"AND\") are separated by a comma (<code>,</code>) and multiple options (\"OR\") should be separated by the pipe character (<code>|</code>).</p> <pre><code># run on a node with a haswell codenamed CPU (e.g. a E5-2660 v3)\nsbatch --constraint=haswell submit.sh\n\n# only run on nodes with E5-2660 v4 CPUs\nsbatch --constraint=E5-2660_v4 submit.sh\n</code></pre> <p>We also have keyword features to help you constrain your jobs to certain categories of nodes.  </p> <ul> <li><code>oldest</code>: the oldest generation of node on the cluster. Use this constraint when compiling code if you wish to ensure it can run on any standard node on the cluster.</li> <li><code>nogpu</code>: nodes without GPUs.</li> <li><code>standard</code>: nodes without GPUs or extra memory. Useful for protecting special nodes in a private partition for jobs that can use the extra capabilities.</li> <li><code>singleprecision</code>: nodes with single-precision only capable GPUs (e.g. GTX 1080s, RTX 2080s).</li> <li><code>doubleprecision</code>: nodes with double-precision capable GPUs (e.g. K80s, P100s and V100s).</li> <li>GPU type (e.g. <code>v100</code>): nodes with a specific type of GPU. </li> <li><code>bigtmp</code>: nodes with at least 1.5T of local storage in <code>/tmp</code>. Useful to ensure that your code will have sufficient space if it uses local storage (e.g. Gaussian's <code>$GAUSS_SCRDIR</code>).  </li> </ul> <p>Tip</p> <p>Use the command <code>scontrol show node &lt;hostname&gt;</code>, replacing <code>&lt;hostname&gt;</code> with the node's name you're interested in, to see more information about the node including its features.</p>"},{"location":"clusters-at-yale/job-scheduling/resource-usage/","title":"Monitor CPU and Memory","text":""},{"location":"clusters-at-yale/job-scheduling/resource-usage/#general-note","title":"General Note","text":"<p>Making sure your jobs use the right amount of RAM and the right number of CPUs helps you and others using the clusters use these resources more effeciently, and in turn get work done more quickly. Below are some examples of how to measure your CPU and RAM (aka memory) usage so you can make this happen. Be sure to check the Slurm documentation and the clusters page (especially the partitions and hardware sections) to make sure you are submitting the right jobs to the right hardware.</p>"},{"location":"clusters-at-yale/job-scheduling/resource-usage/#future-jobs","title":"Future Jobs","text":"<p>If you launch a program by putting <code>/usr/bin/time</code> in front of it, <code>time</code> will watch your program and provide statistics about the resources it used. For example:</p> <pre><code>[netid@node ~]$ /usr/bin/time -v stress-ng --cpu 8 --timeout 10s\nstress-ng: info:  [32574] dispatching hogs: 8 cpu\nstress-ng: info:  [32574] successful run completed in 10.08s\n    Command being timed: \"stress-ng --cpu 8 --timeout 10s\"\n    User time (seconds): 80.22\n    System time (seconds): 0.04\n    Percent of CPU this job got: 795%\n    Elapsed (wall clock) time (h:mm:ss or m:ss): 0:10.09\n    Average shared text size (kbytes): 0\n    Average unshared data size (kbytes): 0\n    Average stack size (kbytes): 0\n    Average total size (kbytes): 0\n    Maximum resident set size (kbytes): 6328\n    Average resident set size (kbytes): 0\n    Major (requiring I/O) page faults: 0\n    Minor (reclaiming a frame) page faults: 30799\n    Voluntary context switches: 1380\n    Involuntary context switches: 68\n    Swaps: 0\n    File system inputs: 0\n    File system outputs: 0\n    Socket messages sent: 0\n    Socket messages received: 0\n    Signals delivered: 0\n</code></pre> <p>To know how much RAM your job used (and what jobs like it will need in the future), look at the \"Maximum resident set size\"</p>"},{"location":"clusters-at-yale/job-scheduling/resource-usage/#running-jobs","title":"Running Jobs","text":"<p>If your job is already running, you can check on its usage, but will have to wait until it has finished to find the maximum memory and CPU used. The easiest way to check the instantaneous memory and CPU usage of a job is to ssh to a compute node your job is running on. To find the node you should <code>ssh</code> to, run:</p> <pre><code>[netid@node ~]$ squeue --me\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n          21252409   general    12345    netid   R      32:17     17 c13n[02-04],c14n[05-10],c16n[03-10]\n</code></pre> <p>Then use ssh to connect to a node your job is running on from the <code>NODELIST</code> column:</p> <pre><code>[netid@node ~]$ ssh c13n03\n[netid@c13n03 ~]$\n</code></pre> <p>Once you are on the compute node, run either <code>ps</code> or <code>top</code>.</p>"},{"location":"clusters-at-yale/job-scheduling/resource-usage/#ps","title":"<code>ps</code>","text":"<p><code>ps</code> will give you instantaneous usage every time you run it. Here is some sample <code>ps</code> output:</p> <pre><code>[netid@bigmem01 ~]$  ps -u$USER -o %cpu,rss,args\n%CPU   RSS COMMAND\n92.6 79446140 /gpfs/ysm/apps/hpc/Apps/Matlab/R2016b/bin/glnxa64/MATLAB -dmlworker -nodisplay -r distcomp_evaluate_filetask\n94.5 80758040 /gpfs/ysm/apps/hpc/Apps/Matlab/R2016b/bin/glnxa64/MATLAB -dmlworker -nodisplay -r distcomp_evaluate_filetask\n92.6 79676460 /gpfs/ysm/apps/hpc/Apps/Matlab/R2016b/bin/glnxa64/MATLAB -dmlworker -nodisplay -r distcomp_evaluate_filetask\n92.5 81243364 /gpfs/ysm/apps/hpc/Apps/Matlab/R2016b/bin/glnxa64/MATLAB -dmlworker -nodisplay -r distcomp_evaluate_filetask\n93.8 80799668 /gpfs/ysm/apps/hpc/Apps/Matlab/R2016b/bin/glnxa64/MATLAB -dmlworker -nodisplay -r distcomp_evaluate_filetask\n</code></pre> <p><code>ps</code> reports memory used in kilobytes, so each of the 5 matlab processes is using ~77GiB of RAM. They are also using most of 5 cores, so future jobs like this should request 5 CPUs.</p>"},{"location":"clusters-at-yale/job-scheduling/resource-usage/#top","title":"<code>top</code>","text":"<p><code>top</code> runs interactively and shows you live usage statistics. You can press u, enter your netid, then enter to filter just your processes. For Memory usage, the number you are interested in is RES. In the case below, the YEPNEE.exe programs are each consuming ~600MB of memory and each fully utilizing one CPU. You can press ? for help and q to quit.</p> <p></p>"},{"location":"clusters-at-yale/job-scheduling/resource-usage/#clustershell","title":"ClusterShell","text":"<p>For multi-node jobs <code>clush</code> can be very useful. Please see our guide on how to set up and use ClusterShell.</p>"},{"location":"clusters-at-yale/job-scheduling/resource-usage/#completed-jobs","title":"Completed Jobs","text":"<p>Slurm records statistics for every job, including how much memory and CPU was used.</p>"},{"location":"clusters-at-yale/job-scheduling/resource-usage/#seff","title":"<code>seff</code>","text":"<p>After the job completes, you can run <code>seff &lt;jobid&gt;</code> to get some useful information about your job, including the memory used and what percent of your allocated memory that amounts to.</p> <pre><code>[netid@node ~]$ seff 21294645\nJob ID: 21294645\nCluster: mccleary\nUser/Group: rdb9/support\nState: COMPLETED (exit code 0)\nCores: 1\nCPU Utilized: 00:15:55\nCPU Efficiency: 17.04% of 01:33:23 core-walltime\nJob Wall-clock time: 01:33:23\nMemory Utilized: 446.20 MB\nMemory Efficiency: 8.71% of 5.00 GiB\n</code></pre>"},{"location":"clusters-at-yale/job-scheduling/resource-usage/#seff-array","title":"<code>seff-array</code>","text":"<p>For job arrays (see here for details) it is helpful to  look at statistics for how resources are used by each element of the array. The <code>seff-array</code> tool takes the job ID of the array and then calculates the distribution and average CPU and memory usage:</p> <pre><code>[netid@node ~]$ seff-array 43283382\n========== Max Memory Usage ==========\n# NumSamples = 90; Min = 896.29 MB; Max = 900.48 MB\n# Mean = 897.77 MB; Variance = 0.40 MB;                   SD = 0.63 MB; Median 897.78 MB\n# each \u220e represents a count of 1\n  806.6628 -   896.7108 MB [   2]: \u220e\u220e\n  896.7108 -   897.1296 MB [   9]: \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\n  897.1296 -   897.5484 MB [  21]: \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\n  897.5484 -   897.9672 MB [  34]: \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\n  897.9672 -   898.3860 MB [  15]: \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\n  898.3860 -   898.8048 MB [   4]: \u220e\u220e\u220e\u220e\n  898.8048 -   899.2236 MB [   1]: \u220e\n  899.2236 -   899.6424 MB [   3]: \u220e\u220e\u220e\n  899.6424 -   900.0612 MB [   0]:\n  900.0612 -   900.4800 MB [   1]: \u220e\nThe requested memory was 2000MB.\n\n========== Elapsed Time ==========\n# NumSamples = 90; Min = 00:03:25.0; Max = 00:07:24.0\n# Mean = 00:05:45.0; SD = 00:01:39.0; Median 00:06:44.0\n# each \u220e represents a count of 1\n00:03:5.0  - 00:03:48.0 [  30]: \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\n00:03:48.0 - 00:04:11.0 [   0]:\n00:04:11.0 - 00:04:34.0 [   0]:\n00:04:34.0 - 00:04:57.0 [   0]:\n00:04:57.0 - 00:05:20.0 [   0]:\n00:05:20.0 - 00:05:43.0 [   0]:\n00:05:43.0 - 00:06:6.0  [   0]:\n00:06:6.0  - 00:06:29.0 [   0]:\n00:06:29.0 - 00:06:52.0 [  30]: \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\n00:06:52.0 - 00:07:15.0 [  28]: \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\n********************************************************************************\nThe requested runtime was 01:00:00.\nThe average runtime was 00:05:45.0.\nRequesting less time would allow jobs to run more quickly.\n********************************************************************************\n</code></pre> <p>This shows how efficiently the resource request was for all the jobs in an array. In this example, we see that the average memory usage was just under 1GiB, which is reasonable for the 2GiB requested. However, the requested runtime was for an hour, while the jobs only ran for six minutes. These jobs could have been scheduled more quickly if a more accurate runtime was specified.</p>"},{"location":"clusters-at-yale/job-scheduling/resource-usage/#sacct","title":"<code>sacct</code>","text":"<p>You can also use the more flexible <code>sacct</code> to get that info, along with other more advanced job queries. Unfortunately, the default output from <code>sacct</code> is not as useful. We recommend setting an environment variable to customize the output.</p> <pre><code>[netid@node ~]$ export SACCT_FORMAT=\"JobID%20,JobName,User,Partition,NodeList,Elapsed,State,ExitCode,MaxRSS,AllocTRES%32\"\n[netid@node ~]$ sacct -j 21294645\n               JobID    JobName      User  Partition        NodeList    Elapsed      State ExitCode     MaxRSS                        AllocTRES\n-------------------- ---------- --------- ---------- --------------- ---------- ---------- -------- ---------- --------------------------------\n            21294645       bash      rdb9 interacti+          c06n09   01:33:23  COMPLETED      0:0               cpu=1,mem=5G,node=1,billing=1\n     21294645.extern     extern                               c06n09   01:33:23  COMPLETED      0:0       716K    cpu=1,mem=5G,node=1,billing=1\n          21294645.0       bash                               c06n09   01:33:23  COMPLETED      0:0    456908K              cpu=1,mem=5G,node=1\n</code></pre> <p>You should look at the MaxRSS value to see your memory usage.</p>"},{"location":"clusters-at-yale/job-scheduling/scavenge/","title":"Scavenge Partition","text":"<p>A scavenge partition is available on all of our clusters. It allows you to (a) run jobs outside of your normal limits (e.g. <code>QOSMaxCpuPerUserLimit</code>) and (b) use unutilized cores, if available, in any private partition on the cluster. You can also use the scavenge partition to get access to unused cores in special purpose partitions, such as the \"gpu\" or \"mpi\" partitions, and unused GPUs in private partitions.</p> <p>However, any job running in the scavenge partition is subject to preemption if any node in use by the job is required for a job in the node's normal partition. This means that your job may be killed without advance notice, so you should only run jobs in the scavenge partition that either have checkpoint capabilities or that can otherwise be restarted with minimal loss of progress.</p> <p>Warning</p> <p>Not all jobs are a good fit for the scavenge partition, such as jobs with long startup times or jobs that run a long time between checkpoint operations.</p>"},{"location":"clusters-at-yale/job-scheduling/scavenge/#automatically-requeue-preempted-jobs","title":"Automatically Requeue Preempted Jobs","text":"<p>If you would like your job to be automatically added back to the queue if preempted, you can add the <code>--requeue</code> flag to your submission script.</p> <pre><code>#SBATCH --requeue\n</code></pre> <p>Be aware that your job, when started from a requeue, will still re-run the entire original submission script. It will only resume progress if your program has the its own ability to checkpoint and restart from previous progress.</p>"},{"location":"clusters-at-yale/job-scheduling/scavenge/#track-history-of-a-requeued-job","title":"Track History of a Requeued Job","text":"<p>When a scavenge job is requeued after preemption, it retains the same job id. However, this can make it difficult to track the history of the job (how many times it was requeued, how long it ran for each time). To view the full history of your job use the <code>--duplicates</code> flag for the <code>sacct</code> command.</p> <pre><code>sacct -j &lt;jobid&gt; --duplicates\n</code></pre>"},{"location":"clusters-at-yale/job-scheduling/scavenge/#scavenge-gpus","title":"Scavenge GPUs","text":"<p>On Grace and McCleary, we also have a <code>scavenge_gpu</code> partition, that contains all scavenge-able GPU enabled nodes and has higher priority for those node than normal scavenge. In all other ways (e.g. preemption, time limit), <code>scavenge_gpu</code> behaves the same as the normal scavenge partition. You can see the full count of GPU nodes in the Partition tables on the respective cluster pages.</p>"},{"location":"clusters-at-yale/job-scheduling/scavenge/#scavenge-mpi-nodes","title":"Scavenge MPI Nodes","text":"<p>On Grace, we have a <code>scavenge_mpi</code> partition, that contains all scavenge-able nodes similar to the <code>mpi</code> partition and has higher priority for those node than normal scavenge. <code>scavenge_mpi</code> is subject to the same preemption model as <code>scavenge</code> and the same use case restrictions as the regular <code>mpi</code> partition (multi-node, tightly couple parallel codes). You can see the full count of MPI nodes in the Partition tables on the respective cluster pages.</p>"},{"location":"clusters-at-yale/job-scheduling/scavenge/#research-available-nodes","title":"Research Available Nodes","text":"<p>If you are interested in specific hardware and its availability, you can use the <code>sinfo</code> command to query how many of each type of node is available and what features it lists. For example:</p> <pre><code>sinfo -e -o \"%.6D|%c|%G|%b\" | column -ts \"|\"\n</code></pre> <p>will show you the kinds of nodes available, and</p> <pre><code>sinfo -e -o \"%.6D|%T|%c|%G|%b\" | column -ts \"|\"\n</code></pre> <p>will break out how many nodes in each state (e.g. allocated, mixed, idle) there are. For more options see the official <code>sinfo</code> documentation.</p>"},{"location":"clusters-at-yale/job-scheduling/scrontab/","title":"Recurring Jobs","text":"<p>You can use <code>scrontab</code> to schedule recurring jobs. It uses a syntax similar to <code>crontab</code>, a standard Unix/Linux utility for running programs at specified intervals. </p> <p><code>scrontab</code> vs <code>crontab</code></p> <p>If you are familiar with <code>crontab</code>, there are some important differences to note:</p> <ul> <li>The scheduled times for <code>scrontab</code> indicate when your job is eligible to start. They are not start times like a traditional Cron jobs.</li> <li>Jobs managed with <code>scrontab</code> won't start if an earlier iteration of the same job is still running. Cron will happily run multiple copies of a job at the same time.</li> <li>You have one scrontab file for the entire cluster, unlike crontabs which are stored locally on each computer.</li> </ul>"},{"location":"clusters-at-yale/job-scheduling/scrontab/#set-up-your-scrontab","title":"Set Up Your <code>scrontab</code>","text":""},{"location":"clusters-at-yale/job-scheduling/scrontab/#edit-your-scrontab","title":"Edit Your <code>scrontab</code>","text":"<p>Run <code>scrontab -e</code> to edit your <code>scrontab</code> file. If you prefer to use <code>nano</code> to edit files, run</p> <p><pre><code>EDITOR=nano scrontab -e\n</code></pre> Lines that start with <code>#SCRON</code> are treated like the beginning of a new batch job, and work like <code>#SBATCH</code> directives for batch jobs. Slurm will ignore <code>#SBATCH</code> directives in scripts you run as <code>scrontab</code> jobs. You can use most common <code>sbatch</code> options just as you would using sbatch on the command line. The first line after your <code>SCRON</code> directives specifies the schedule for your job and the command to run. </p> <p>Note</p> <p>All of your <code>scrontab</code> jobs will start with your home directory as the working directory. You can change this with the <code>--chdir</code> slurm option.</p>"},{"location":"clusters-at-yale/job-scheduling/scrontab/#cron-syntax","title":"Cron syntax","text":"<p>Crontab syntax is specified in five columns, to specify minutes, hours, days of the month, months, and days of the week. Especially at first you may find it easiest to use a helper application to generate your cron date fields, such as crontab-generator or cronhub.io. You can also use the short-hand syntax <code>@hourly</code>, <code>@daily</code>, <code>@weekly</code>, <code>@monthly</code>, and <code>@yearly</code> instead of the five separate columns.</p>"},{"location":"clusters-at-yale/job-scheduling/scrontab/#what-to-run","title":"What to Run","text":"<p>If you're running a script it must be marked as executable. Jobs handled by scrontab do not run in a full login shell, so if you have customized your <code>.bashrc</code> file you need to add:</p> <p><pre><code>source ~/.bashrc\n</code></pre> To your script to ensure that your environment is set up correctly.</p> <p>Note</p> <p>The command you specify in the scrontab is executed via bash, NOT sbatch. You can list multiple commands separated by ;, and use other shell features, such as redirects.  Also, any #SBATCH directives in executed scripts will be ignored.  You must use #SCRON in the scrontab file instead.</p> <p>Note</p> <p>Your <code>scrontab</code> jobs will appear to have the same JobID every time they run until the next time you edit your <code>scrontab</code> file (they are being requeued). This means that only the most recent job will be logged to the default output file. If you want deeper history, you should redirect output in your scripts to filenames with something more unique in their names, like a date or timestamp, e.g.</p> <pre><code>python my_script.py &gt; $(date +\"%Y-%m-%d\")_myjob_scrontab.out\n</code></pre> <p>If you want to see slurm accounting of a job handled by scrontab, for example job <code>12345</code> run:</p> <pre><code>sacct --duplicates --jobs 12345\n# or with short options\nsacct -Dj 12345\n</code></pre>"},{"location":"clusters-at-yale/job-scheduling/scrontab/#examples","title":"Examples","text":""},{"location":"clusters-at-yale/job-scheduling/scrontab/#run-a-daily-simulation","title":"Run a Daily Simulation","text":"<p>This example submits a 6-hour simulation eligible to start every day at 12:00 AM.</p> <pre><code>#SCRON --time 6:00:00\n#SCRON --cpus-per-task 4\n#SCRON --name \"daily_sim\"\n#SCRON --chdir /home/netid/project\n#SCRON -o my_simulations/%j-out.txt\n@daily ./simulation_v2_final.sh\n</code></pre>"},{"location":"clusters-at-yale/job-scheduling/scrontab/#run-a-weekly-transfer-job","title":"Run a Weekly Transfer Job","text":"<p>This example submits a transfer script eligible to start every Wednesday at 8:00 PM.</p> <pre><code>#SCRON --time 1:00:00\n#SCRON --partition transfer\n#SCRON --chdir /home/netid/project/to_transfer\n#SCRON -o transfer_log_%j.txt\n0 20 * * 3 ./rclone_commands.sh\n</code></pre>"},{"location":"clusters-at-yale/job-scheduling/scrontab/#capture-output-from-each-run-in-a-separate-file","title":"Capture output from each run in a separate file","text":"<p>Normally scrontab will clobber the output file from the previous run on each execution, since each execution uses the same jobid.  This can be avoided using a redirect to a date-stamped file.</p> <pre><code>0 20 * * 3 ./commands.sh &gt; myjob_$(date +%Y%m%d%H%M).out\n</code></pre>"},{"location":"clusters-at-yale/job-scheduling/simplequeue/","title":"SimpleQueue","text":"<p>SimpleQueue is a tool written here to streamline submission of a large number of jobs using a task file. It has a number of advantages:</p> <ul> <li>You can run more of your sequential jobs concurrently, since there is a limit on the number of individual qsubs you can run simultaneously.</li> <li>You only have one job to keep track of.</li> <li>If you need to shut everything down, you only need to kill one job.</li> <li>SimpleQueue keeps track of the status of individual jobs.</li> </ul> <p>Note that version 3.0+ of SimpleQueue differs from earlier versions in important ways, in particular the meaning of -n. If you have been using an earlier version, please read the following carefully!</p> <p>SimpleQueue is available as a module on our clusters. Run:</p> <pre><code>module avail simplequeue\n</code></pre> <p>to locate the simplequeue module on your cluster of choice.</p>"},{"location":"clusters-at-yale/job-scheduling/simplequeue/#example-simplequeue-job","title":"Example SimpleQueue Job","text":"<p>For example, imagine that you have 1000 fastq files that correspond to individual samples you want to map to a genome with <code>bowtie2</code> and convert to bam files with <code>samtools</code>. Given some initial testing, you think that 80 cpus working together will be enough to finish the job in a reasonable time.</p>"},{"location":"clusters-at-yale/job-scheduling/simplequeue/#step-1-create-task-list","title":"Step 1: Create Task List","text":"<p>The first step is to create a file with a list of the \"tasks\" you want to run. Each task corresponds to what you might otherwise have run as a single job. A task can be a simple command invocation, or a sequence of commands. You can call the task file anything, but for this example assume it's called \"tasklist.txt\" and contains:</p> <pre><code>module load bowtie2 samtools; bowtie2 -p 8 --local --rg-id sample1 --rg SM:sample1 --rg LB:sci_seq --rg PL:ILLUMINA -x my_genome -U sample1.fastq - | samtools view -Shu - | samtools sort  - sample1\nmodule load bowtie2 samtools; bowtie2 -p 8 --local --rg-id sample2 --rg SM:sample2 --rg LB:sci_seq --rg PL:ILLUMINA -x my_genome -U sample2.fastq - | samtools view -Shu - | samtools sort  - sample2\n...\nmodule load bowtie2 samtools; bowtie2 -p 8 --local --rg-id sample1000 --rg SM:sample1000 --rg LB:sci_seq --rg PL:ILLUMINA -x my_genome -U sample1000.fastq - | samtools view -Shu - | samtools sort  - sample1000\n</code></pre> <p>For simplicity, we'll assume that tasklist, input fastq files, and indexed genome are in a directory called <code>~/genome_proj/mapping</code>.</p>"},{"location":"clusters-at-yale/job-scheduling/simplequeue/#step-2-create-submission-script","title":"Step 2: Create Submission Script","text":"<p>Load the SimpleQueue module, then create the launch script using:</p> <pre><code>sqCreateScript -q general -N genome_map -n 80 tasklist.txt &gt; run.sh\n</code></pre> <p>These parameters specify that the job, named genome_map, will be submitted to the general queue/partition. This job will find 80 free cores, start 80 workers on them, and begin processing tasks from the taskfile <code>tasklist.txt</code>.</p> <p>sqCreateScript takes a number of options. They differ somewhat from cluster to cluster, particularly the default values for queue, walltime, and memory. You can run sqCreateScript without any arguments to see the exact options on your cluster.</p> <pre><code>Usage:\n\n  -h, --help            show this help message and exit\n  -n WORKERS, --workers=WORKERS\n                        Number of workers to use. Not required. Defaults to 1.\n  -c CORES, --cores=CORES\n                        Number of cores to request per worker. Defaults to 1.\n  -m MEM, --mem=MEM     Memory per worker. Not required. Defaults to 1G\n  -w WALLTIME, --walltime=WALLTIME\n                        Walltime to request for the Slurm Job in form\n                        [[D-]HH:]MM:SS. Not required. Defaults to 1:00:00.\n  -q QUEUE, --queue=QUEUE\n                        Name of queue to use. Not required. Defaults to\n                        general\n  -N NAME, --name=NAME  Base job name to use. Not required. Defaults to\n                        SimpleQueue.\n  --logdir=LOGDIR       Name of logging directory. Defaults to\n                        SQ_Files_${SLURM_JOB_ID}.\n</code></pre>"},{"location":"clusters-at-yale/job-scheduling/simplequeue/#step-3-submit-your-job","title":"Step 3: Submit Your Job","text":"<p>Now you can simply submit <code>run.sh</code> to the scheduler. All of the important scheduler options (queue, number of tasks, number of cpus per task) will have been set in the script so you needn't worry about them.</p> <p>Shortly after run.sh begins running, you should see a directory appear called <code>SQ_Files_jobid</code> where jobid is the jobid the scheduler assigned your job. This directory contains logs from all the tasks that are run during your job.</p> <p>In addition, there are a few other files that record information about the job as a whole. Of these, the most important one is <code>SQ.log</code>. It should be reviewed if you encounter a problem with a run.</p> <p>Assuming that all goes well, tasks from the tasklist file will be scheduled automatically onto the cpus you acquired until all the tasks have completed. At that time, the job will terminate, and you'll see several summary files:</p> <ul> <li><code>scheduler_jobid_out.txt</code>: this is the stdout from simple queue proper (it is generally empty).</li> <li><code>scheduler_jobid_err.txt</code>: this is the stderr from simple queue proper (it is generally a copy of <code>SQ.log</code>).</li> <li><code>tasklist.txt.STATUS</code>: this contains a list of all the tasks that were run, including exit status, start time, end time, pid, node run on, and the command run.</li> <li><code>tasklist.txt.REMAINING</code>: Failed or uncompleted tasks will be listed in this file in the same format as tasklist, so that those tasks can be easily rerun. You should review the status files related to these tasks to understand why they did not complete. This list is provided for convenience. It is always a good idea to scan tasklist.STATUS to double check which tasks did in fact complete with a normal exit status.</li> <li><code>tasklist.txt.ROGUES</code>: The simple queue system attempts to ensure that all tasks launched eventually exit (normally or abnormally). If it fails to get confirmation that a task has exited, information about the command will be written to this file. This information can be used to hunt down and kill run away processes.</li> </ul>"},{"location":"clusters-at-yale/job-scheduling/simplequeue/#other-important-options","title":"Other Important Options","text":"<p>If your individual tasks need more than the default memory allocated on your cluster, you can specify a different value using -m. For example: <code>sqCreateScript -m 10g -n 4 ... tasklist &gt; run.sh</code> would request 10GiB of RAM for each of your workers.</p> <p>If your jobs are themselves multithreaded, you can request that your workers have multiple cores using the -c option: <code>sqCreateScript -c 20 -n 4 ... tasklist &gt; run.sh</code> This would create 4 workers, each having access to 20 cores.</p>"},{"location":"clusters-at-yale/job-scheduling/slurm-account/","title":"Slurm Account Coordinator","text":"<p>On the clusters the YCRC maintains, we map your linux user and group to your Slurm user and account, which is what actually gives you permission to submit to the various partitions available on the clusters. By changing the Slurm accounts associated with your user, you can modify access to partitions. As a coordinator of an account, you have permission to modify users' association with that account and modify jobs running that are associated with that account. Below are some useful example commands where we use an example user with the name \"be59\" where you are the coordinator of the slurm account \"cryoem\".</p>"},{"location":"clusters-at-yale/job-scheduling/slurm-account/#addremove-users-from-an-account","title":"Add/Remove Users From an Account","text":"<pre><code>sacctmgr add user be59 account=cryoem # add user\nsacctmgr remove user where user=be59 and account=cryoem # remove user\n</code></pre>"},{"location":"clusters-at-yale/job-scheduling/slurm-account/#show-account-info","title":"Show Account Info","text":"<pre><code>sacctmgr show assoc user=be59 # show user associations\nsacctmgr show assoc account=cryoem # show assocations for account\n</code></pre>"},{"location":"clusters-at-yale/job-scheduling/slurm-account/#submit-jobs","title":"Submit Jobs","text":"<pre><code>salloc -A cryoem ...\nsbatch -A cryoem my_script.sh\n</code></pre>"},{"location":"clusters-at-yale/job-scheduling/slurm-account/#list-jobs","title":"List Jobs","text":"<pre><code>squeue -A cryoem  # by account\nsqueue -u be59 # by user\n</code></pre>"},{"location":"clusters-at-yale/job-scheduling/slurm-account/#cancel-jobs","title":"Cancel Jobs","text":"<pre><code>scancel 1234  # by job ID\nscancel -u be59   # kill all jobs by user\nscancel -u be59 --state=running  # kill running jobs by user\nscancel -u be59 --state=pending  # kill pending jobs by user\nscancel -A cryoem  # kill all jobs in the account\n</code></pre>"},{"location":"clusters-at-yale/job-scheduling/slurm-account/#hold-and-release-jobs","title":"Hold and Release Jobs","text":"<pre><code>scontrol hold 1234   # by job ID\nscontrol release 1234   # remove the hold\nscontrol uhold 1234   # hold job 1234 but allow the job's owner to release it\n</code></pre>"},{"location":"clusters-at-yale/job-scheduling/slurm-examples/","title":"Submission Script Examples","text":"<p>In addition to those below, we have additional example submission scripts for Parallel R, Matlab and Python.</p>"},{"location":"clusters-at-yale/job-scheduling/slurm-examples/#single-threaded-programs-basic","title":"Single threaded programs (basic)","text":"<pre><code>#!/bin/bash\n\n#SBATCH --job-name=my_job\n#SBATCH --time=10:00\n\n./hello.omp\n</code></pre>"},{"location":"clusters-at-yale/job-scheduling/slurm-examples/#multi-threaded-programs","title":"Multi-threaded programs","text":"<pre><code>#!/bin/bash\n\n#SBATCH --job-name=omp_job\n#SBATCH --output=omp_job.txt\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=4\n#SBATCH --time=10:00\n\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n./hello.omp\n</code></pre>"},{"location":"clusters-at-yale/job-scheduling/slurm-examples/#multi-process-programs","title":"Multi-process programs","text":"<pre><code>#!/bin/bash\n\n#SBATCH --job-name=mpi\n#SBATCH --output=mpi_job.txt\n#SBATCH --ntasks=4\n#SBATCH --time=10:00\n\nmpirun hello.mpi\n</code></pre> <p>Tip</p> <p>On Grace's mpi partition, try to make ntasks equal to a multiple of 24.</p>"},{"location":"clusters-at-yale/job-scheduling/slurm-examples/#hybrid-mpiopenmp-programs","title":"Hybrid (MPI+OpenMP) programs","text":"<pre><code>#!/bin/bash\n\n#SBATCH --job-name=hybrid\n#SBATCH --output=hydrid_job.txt\n#SBATCH --ntasks=8\n#SBATCH --cpus-per-task=5\n#SBATCH --nodes=2\n#SBATCH --time=10:00\n\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n\nmpirun hello_hybrid.mpi\n</code></pre>"},{"location":"clusters-at-yale/job-scheduling/slurm-examples/#gpu-job","title":"GPU job","text":"<pre><code>#!/bin/bash\n\n#SBATCH --job-name=deep_learn\n#SBATCH --output=gpu_job.txt\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=2\n#SBATCH --gpus=p100:2\n#SBATCH --partition=gpu\n#SBATCH --time=10:00\n\nmodule load CUDA\nmodule load cuDNN\n# using your anaconda environment\nsource activate deep-learn\npython my_tensorflow.py\n</code></pre>"},{"location":"data/","title":"Data Storage","text":"<p>Below we highlight some data storage option at Yale that are appropriate for research data. For a more complete list of data storage options, see the Storage Finder. If you have questions about selecting an appropriate home for your data, contact us for assistance.</p>"},{"location":"data/#hpc-cluster-storage","title":"HPC Cluster Storage","text":"<ul> <li>Capacity: Varies. Cost: Varies</li> <li>Sensitive data is only allowed on the Milgram cluster</li> <li>Only available on YCRC HPC clusters</li> </ul> <p>Along with access to the compute clusters we provide each research group with cluster storage space for research data. The storage is separated into three quotas: Home, Project, and 60-day Scratch. Each of these quotas limit both the amount in bytes and number of files you can store. Details can be found on our Cluster Storage page.</p> <p>Additional project-style storage allocations can be purchased. See here for more information.</p>"},{"location":"data/#google-drive-via-eliapps","title":"Google Drive via EliApps","text":"<p>Warning</p> <p>** Changes to Google Drive pricing **</p> <p>ITS has informed us of a number of changes to the EliApps Google Drive quotas, including shared drives.</p> <ul> <li>As of 8/15/23, all new EliApps accounts will have a free quota of 5GB.</li> <li>As of 7/1/24, all existing EliApps accounts will have a free quota of 5GB.</li> <li>Quotas beyond 5GB will be available for $145/TB/yr</li> </ul> <p>Therefore, you should probably not consider Google Drive on EliApps for storage large amounts of data.  ITS suggested alternatives are Storage@Yale, Teams/SharePoint, or DropBox.</p> <ul> <li>Capacity: 400,000 file count quota, 5TiB max file size. Cost: Free</li> <li>No sensitive data (e.g. ePHI, HIPAA)</li> <li>Can be mounted on your local machine and transferred to via Globus Google Drive Connector</li> </ul> <p>Google Drive is a cloud service for file storage, document editing and sharing. All members of the Yale community with an EliApps (Google Workspace for Education) account have storage at no cost in the associated Google Drive account. Moreover, EliApps users can request Shared Drives, which are shared spaces where all files are group-owned. For more information on Google Drive through EliApps, see our Google Drive documentation.</p>"},{"location":"data/#storage-yale","title":"Storage @ Yale","text":"<ul> <li>Capacity: As requested. Cost: See below</li> <li>No sensitive data (e.g. ePHI, HIPAA) for cluster mounts</li> <li>Can be mounted on the cluster or computers on campus (but not both)</li> </ul> <p>Storage @ Yale (S@Y) is a central storage service provided by ITS. S@Y shares can either be accessible on campus computers or the clusters, but not both. </p> Type Use Object Tier Good for staging data between cloud and clusters Active Tier Daily use, still copy to cluster before using in jobs Archive Tier Long term storage, low access. Make sure to properly archive Backup Tier Low-access remote object backup. Make sure to properly archive <p>For pricing information, see the ITS Data Rates. All prices are charged monthly for storage used at that time.</p> <p>To request a share, press the \u201cRequest this Service\u201d button in the right sidebar on the Storage@Yale website. If you would like to request a share that is mounted on the clusters, specify in your request that the share be mounted from the HPC clusters. If you elect to use archive tier storage, be cognizant of its performance characteristics.</p> <p>Cluster I/O Performance</p> <p>Since cluster-mounted S@Y shares do not provide sufficient performance for use in jobs, they are not mounted on our compute or login nodes. To access S@Y on the clusters, connect to one of the transfer nodes to stage the data to Project or Scratch60 before running jobs.</p>"},{"location":"data/#microsoft-teamssharepoint","title":"Microsoft Teams/SharePoint","text":"<ul> <li>Capacity: 25 TB, 250 GB per file. Cost: Free</li> </ul> <p>You can request a Team and 25TiB of underlying SharePoint storage space from ITS Email And Collaboration Services. For more information on The relationship between Teams, SharePoint, and OneDrive, see the official Microsoft post on the subject.</p>"},{"location":"data/#dropbox-at-yale","title":"Dropbox at Yale","text":"<p>ITS offers departmental subscriptions to DropBox for a low cost (currently $23.66/user/year).</p> <ul> <li>Unlimited storage (take this with a grain of salt)</li> <li>Low risk data only</li> </ul> <p>For more information about DropBox at Yale, see the ITS website.</p>"},{"location":"data/#box-at-yale","title":"Box at Yale","text":"<ul> <li>Capacity: 50GiB per user. Cost: Free. 15 GiB max file size.</li> <li>Sensitive data (e.g. ePHI, HIPAA) only in Secure Box</li> <li>Can be mounted on your local machine and transferred with <code>rclone</code></li> </ul> <p>All members of the Yale community have access to a share at Box at Yale. Box is another cloud-based file sharing and storage service. You can upload and access your data using the web portal and sync data with your local machines via Box Sync.</p> <p>To access, navigate to yale.box.com and login with your yale.edu account.</p> <p>For sync with your local machine, install Box Sync and authenticate with your yale.edu account.</p> <p>For more information about Box at Yale, see the ITS website.</p> <p>To learn more about these options, see the Yale Collaboration Counts page available through Yale ITS for details.</p>"},{"location":"data/archive/","title":"Archive Your Data","text":""},{"location":"data/archive/#clean-out-unnecessary-files","title":"Clean Out Unnecessary Files","text":"<p>Not every file created during a project needs to be archived. If you proactively reduce the number of extraneous files in your archive, you will both reduce storage costs and increase the usefulness of that data upon retrieval. Common files that can be deleted when archiving data include:</p> <ul> <li>Compiled codes, such as <code>.o</code> or <code>.pyc</code> files. These files will likely not even work on the next system you may restore these data to and they can contribute significantly to your file count limit. Just keep the source code and clean installation instructions.</li> <li>Some log files. Many log created by the system are not necessary to store indefinitely. Any Slurm logs from failed runs (prior to a successful run) or outputs from Matlab (e.g. <code>hs_error_pid*.log</code>, <code>java.log.*</code>) can often safely be ignored.</li> <li>Crash files such are core dumps (e.g. <code>core.*</code>, <code>matlab_crash_dump.</code>).</li> </ul>"},{"location":"data/archive/#compress-your-data","title":"Compress Your Data","text":"<p>Most archive locations (S@Y Archive Tier, Google Drive) perform much better with a smaller number of larger files. In fact, Google Shared Drives have a file count limit of 400,000 files. Therefore, it is highly recommended that your compress, using zip or tar, portions of your data for ease of storage and retrieval. For example, to create a compressed archive of a directory you can do the following:</p> <p><pre><code>tar -cvzf archive-2021-04-26.tar.gz ./data_for_archival\n</code></pre> This will create a new file (<code>archive-2021-04-26.tar.gz</code>) which contains all the data from within <code>data_for_archival</code> and is compressed to minimize storage requirements. This file can then be transferred to any off-site backup or archive location.</p>"},{"location":"data/archive/#list-and-extract-data-from-existing-archive","title":"List and Extract Data From Existing Archive","text":"<p>You can list the contents of an archive file like this:</p> <p><pre><code>tar -ztvf archive-2021-04-26.tar.gz\n</code></pre> which will print the full list of every file within the archive. The clusters also have the <code>lz</code> tool installed that provides a shorter way to list the contents:</p> <pre><code>lz archive-2021-04-26.tar.gz\n</code></pre> <p>You can then extract a single file from a large tar-file without decompressing the full thing:</p> <p><pre><code>tar -zxvf archive-2021-04-26.tar.gz path/to/file.txt\n</code></pre> There is an alternative syntax that is more legible:</p> <p><pre><code>tar --extract --file=archive-2021-04-26.tar.gz file.txt\n</code></pre> Either should work fine on the clusters.</p>"},{"location":"data/archive/#tips-for-sy-archive-tier","title":"Tips for S@Y Archive Tier","text":"<p>The archive tier of Storage@Yale is a cloud-based system. It provides an archive location for long-term data, featuring professional systems management, security, and protection from data loss via redundant, enterprise-grade hardware.  Data is dual-written to two locations.  The cost per TB is subtantially lower than for the active-access S@Y tier.  For current pricing, see ITS Data Rates.</p> <p>To use S@Y (Archive) effectively, you need to be aware of how it works and follow some best practices.</p> <p>Note</p> <p>Just as for the S@Y Active Tier, direct access from the cluster should be specified when requesting the share.  Direct access from the cluster is only authorized for Low and Moderate risk data.</p> <p>When you write to the archive, you are actually copying to a large hard disk-based cache, so writes are normally fast. Your copy will appear to complete as soon as the file is in the disk cache. It is NOT yet in the cloud. In the background, the system will flush files to the cloud and delete them from the cache. If you read a file very soon after you write it, it is probably still in the cache, and your read will be quick.</p> <p>However, once some time has elapsed and the file has been moved to the cloud, read speed will be somewhat slower.</p> <p>Note</p> <p>S@Y Archive has a single-filesize limit of 5 TB, so plan your data compressions accordingly.</p> <p>Some key takeaways:</p> <ul> <li>Operations that only read the metadata of files will be fast (ls, find, etc) even if the file is in the cloud, since metadata is kept in the disk cache.</li> <li>Operations that actually read the file (cp, wc -l, tar, etc) will require recovering the entire file to disk cache first, and can take several minutes or longer depending on how busy the system is.</li> <li>If many files will need to be recovered together, it is much better to store them as a single file first with tar or zip, then write that file to the archive.</li> <li>Please do NOT write huge numbers of small files. They will be difficult or impossible to restore in large numbers.</li> <li>Please do NOT do repetitive operations like rsyncs to the archive, since they overload the system.</li> </ul>"},{"location":"data/archive/#sy-backup-tier","title":"S@Y Backup Tier","text":"<p>Yale ITS offers dedicated offsite \"S3\"-style object storage for data backup and archive to the cloud.  Clients are responsible for the data transfers and recovery via the S3 protocol, such as by using RClone.</p> <p>The Backup Tier is authorized for Low, Moderate, and High Risk data.</p> <p>As with the Archive Tier, the Backup Tier is low-speed and not meant for daily use.</p> <p>For current pricing, see ITS Data Rates.</p>"},{"location":"data/archived-sequencing/","title":"YCGA Sequence Data Archive","text":""},{"location":"data/archived-sequencing/#retrieve-data-from-the-archive","title":"Retrieve Data from the Archive","text":"<p>In the sequencing archive on McCleary, a directory exists for each run, holding one or more tar files. There is a main tar file, plus a tar file for each project directory. Most users only need the project tar file corresponding to their data.</p> <p>Although the archive actually exists on tape or in cloud storage, you can treat it as a regular directory tree. Many operations such as <code>ls</code>, <code>cd</code>, etc. are very fast, since directory structures and file metadata are on a disk cache. However, when you actually read the contents of files the file is retrieved and read into a disk cache.  This can take some time.</p> <p>Archived runs are stored in the following locations.</p> Original location Archive location <code>/panfs/sequencers</code> <code>/SAY/archive/YCGA-729009-YCGA-A2/archive/panfs/sequencers</code> <code>/ycga-ba/ba_sequencers</code> <code>/SAY/archive/YCGA-729009-YCGA-A2/archive/ycga-ba/ba_sequencers</code> <code>/gpfs/ycga/sequencers/illumina/sequencers</code> <code>/SAY/archive/YCGA-729009-YCGA-A2/archive/ycga-gpfs/sequencers/illumina/sequencers</code> <p>You can directly copy or untar the project tarfile into a scratch directory.</p> <p>Info</p> <p>Very large tar files over 500GB, sometimes fail to download.  If you run into problems, contact us at hpc@yale.edu and we can manually download it.</p> <pre><code>cd ~/scratch60/somedir\ntar \u2013xvf /SAY/archive/YCGA-729009-YCGA-A2/archive/path/to/file.tar\n</code></pre> <p>Inside the project tar files are the fastq files, which have been compressed using <code>quip</code>. If your pipeline cannot read quip files directly, you will need to uncompress them before using them.</p> <pre><code>module load Quip\nquip \u2013d M20_ACAGTG_L008_R1_009.fastq.qp\n</code></pre> <p>For your convenience, we have a tool, <code>restore</code>, that will download a tar file, untar it, and uncompress all quip files.</p> <pre><code>module load ycga-public\nrestore \u2013t /SAY/archive/YCGA-729009-YCGA/archive/path/to/file.tar\n</code></pre> <p>If you have trouble locating your files, you can use the utility <code>locateRun</code>, using any substring of the original run name. <code>locateRun</code> is in the same module as restore.</p> <pre><code>locateRun C9374AN\n</code></pre> <p>Restore spends most of the time running quip. You can parallelize and thereby speed up that process using the <code>-n</code> flag.</p> <pre><code>restore \u2013n 20 ...\n</code></pre> <p>Tip</p> <p>When retrieving data, run untar/unquip as a job on a compute node, not a login node and make sure to allocate sufficient resources to your job, e.g. <code>\u2013c 20 --mem=100G</code>.</p> <p>Tip</p> <p>The ycgaFastq tool can also be used to recover archived data.  See here. </p>"},{"location":"data/archived-sequencing/#example","title":"Example:","text":"<p>Imagine that user rdb9 wants to restore data from run BHJWZZBCX3</p>"},{"location":"data/archived-sequencing/#step-1","title":"step 1","text":"<p>Initialize compute node with 20 cores <pre><code>salloc -c 20\nmodule load ycga-public\n</code></pre></p>"},{"location":"data/archived-sequencing/#step-2","title":"step 2","text":"<p>Find the run location <pre><code>$ locateRun BHJWZZBCX3\n/ycga-gpfs/sequencers/illumina/sequencerV/runs/210305_D00306_1337_BHJWZZBCX3.deleted\n/SAY/archive/YCGA-729009-YCGA-A2/archive/ycga-gpfs/sequencers/illumina/sequencerV/runs/210305_D00306_1337_BHJWZZBCX3\n</code></pre></p> <p>Note that the original run location has been deleted, but the archive location is listed.</p>"},{"location":"data/archived-sequencing/#step-3","title":"step 3","text":"<p>List the contents of the archived run, and locate the desired project tarball: <pre><code>$ ls -1 /SAY/archive/YCGA-729009-YCGA-A2/archive/ycga-gpfs/sequencers/illumina/sequencerV/runs/210305_D00306_1337_BHJWZZBCX3\n210305_D00306_1337_BHJWZZBCX3_0.tar\n210305_D00306_1337_BHJWZZBCX3_0_Unaligned_Project_Jdm222.tar\n210305_D00306_1337_BHJWZZBCX3_1_Unaligned-1_Project_Rdb9.tar\n210305_D00306_1337_BHJWZZBCX3_2021_05_09_04:00:36_archive.log\n</code></pre></p> <p>We want 210305_D00306_1337_BHJWZZBCX3_1_Unaligned-1_Project_Rdb9.tar, matching our netid.</p>"},{"location":"data/archived-sequencing/#step-4","title":"step 4","text":"<p>Use the restore utility to copy and uncompress the fastq files from the tar file.  By default, restore will start 20 threads, which matches our srun above.  The restore will likely take several minutes. To see progress, you can use the -v flag. <pre><code>restore -v -t /SAY/archive/YCGA-729009-YCGA-A2/archive/ycga-gpfs/sequencers/illumina/sequencerV/runs/210305_D00306_1337_BHJWZZBCX3/210305_D00306_1337_BHJWKHBCX3_1_Unaligned-1_Project_Rdb9.tar\n</code></pre></p> <p>The restored fastq files will written to a directory like this:  <pre><code>210305_D00306_1337_BHJWZZBCX3/Data/Intensities/BaseCalls/Unaligned*/Project_*\n</code></pre></p>"},{"location":"data/backups/","title":"Backups and Snapshots","text":"<p>The only storage backed up on every cluster is Home. We do provide local snapshots, covering at least the last 2 days, on Home and Project directories (see below for details). See the individual cluster documentation for more details about which storage is backed up or has snapshots. </p> <p>Please see our HPC Policies page for additional information about backups. </p>"},{"location":"data/backups/#retrieve-data-from-home-backups","title":"Retrieve Data from Home Backups","text":"<p>Contact us with your netid and the list of files/directories you would like restored. For any data deleted in the last couple days, first try the self-service snapshots described below.</p>"},{"location":"data/backups/#retrieve-data-from-snapshots","title":"Retrieve Data from Snapshots","text":"<p>Our clusters create snapshots nightly on portions of the filesystem so that you can retrieve mistakenly modified or deleted files for yourself. We do not currently provide snapshots of scratch storage.</p> <p>As long as your files existed in the form you want them in before the most recent midnight and the deletion was in the last few days, they can probably be recovered. Snapshot directory structure mirrors the files that are being tracked with a prefix, listed in the table below. Contact us if you need assistance finding the appropriate snapshot location for your files.</p> Storage space File set Snapshot Prefix Project (Grace, McCleary) <code>/gpfs/gibbs/project</code> <code>/gpfs/gibbs/project/.snapshots</code> Gibbs PI (Grace, McCleary) <code>/gpfs/gibbs/pi/group</code> <code>/gpfs/gibbs/pi/group/.snapshots</code> Palmer PI (Grace, McCleary) <code>/vast/palmer/pi/group</code> <code>/vast/palmer/pi/group/.snapshot</code> Home (Grace) <code>/vast/palmer/home.grace</code> <code>/vast/palmer/home.grace/.snapshot</code> Home (McCleary) <code>/vast/palmer/home.mccleary</code> <code>/vast/palmer/home.mccleary/.snapshot</code> YCGA (McCleary) <code>/gpfs/ycga</code> <code>/gpfs/ycga/.snapshots</code> Home (Milgram) <code>/gpfs/milgram/home</code> <code>/gpfs/milgram/home/.snapshots</code> Project (Milgram) <code>/gpfs/milgram/project</code> <code>/gpfs/milgram/project/.snapshots</code> PI (Milgram) <code>/gpfs/milgram/pi/group</code> <code>/gpfs/milgram/pi/group/.snapshots</code> <p>Within the snapshot directory, you will find multiple directories with names that indicate specific dates. For example, if you wanted to recover the file <code>/gpfs/gibbs/project/bjornson/rdb9/doit.sh</code> (a file in the bjornson group's project directory owned by rdb9) it would be found at <code>/gpfs/gibbs/.snapshots/date/project/bjornson/rdb9/doit.sh</code> .</p> <p>Snapshot Sizes</p> <p>Because of the way snapshots are stored, sizes will not be correctly reported until you copy your files/directories back out of the <code>.snapshots</code> directory.</p>"},{"location":"data/external/","title":"Share Data Outside Yale","text":""},{"location":"data/external/#share-data-using-microsoft-onedrive","title":"Share data using Microsoft OneDrive","text":"<p>Yale ITS's recommended way to send other people large files is by using Microsoft OneDrive.  See details.</p>"},{"location":"data/external/#public-website","title":"Public Website","text":"<p>Researchers frequently ask how they can set up a public website to share data or provide a web-based application.  The easiest way to do this is by using Yale ITS's spinup service.  </p> <p>First get an account on Spinup.</p> <p>Info</p> <p>When getting your account on Spinup, you will need to provide a charging account (aka COA). </p>"},{"location":"data/external/#static-website","title":"Static website","text":"<p>You can use a static website with a public address to serve data publicly to collaborators or services that need to see the data via http. A common example of this is hosting tracks for the UCSC Genome Browser.  Note that this only serves static files.  If you wish to host a dynamic web application, see below.</p> <p>ITS's spinup service makes creating a static website easy and inexpensive.  </p> <p>Follow their instructions on creating a static website, giving it an appropriate website name.  Make sure to save the access key and secret key, since you'll need them to connect to the website.  The static website will incur a small charge per month of a few cents per GB stored or downloaded.</p> <p>Then use an S3 transfer tool like Cyberduck, AWS CLI, or CrossFTP to connect to the website and transfer your files.  The spinup page for your static website provides a link to a Cyberduck config file.  That is the probably the easiest way to connect. </p>"},{"location":"data/external/#ucsc-hub","title":"UCSC Hub","text":"<p>To set up the UCSC Hub, follow their directions to set up the appropriate file heirarchy on your static website, using the transfer tool.</p>"},{"location":"data/external/#web-based-application","title":"Web-based application","text":"<p>If your web application goes beyond simply serving static data, the best solution is to create a spinup virtual machine (VM), set up your web application on the VM, then  follow the spinup instructions on requesting public access to a web server</p> <p>Info</p> <p>Running a VM 24x7 can incur significant costs on spinup, depending on the size of the VM.  </p>"},{"location":"data/external/#private-share-using-globus","title":"Private Share Using Globus","text":"<p>Globus can be used to shared data hosts on one of the clusters privately with a specific person or group of people.</p> <ol> <li>From the file manager interface enter the name of the endpoint you would like to share from in the collection field (e.g. \"Yale CRC Grace\")</li> <li>Click the Share button on the right</li> <li>Click on \"Add a Shared Endpoint\"</li> <li>Next to Path, click \"Browse\" to find and select the directory you want to share</li> <li>Add other details as desired and click on \"Create Share\"</li> <li>Click on \"Add Permissions -- Share With\"</li> <li>Under \"Username or Email\" enter the e-mail address of the person that you want to share the data with, then click on \"Save\", then click on \"Add Permission\"</li> <li>Do not select \"write\" unless you want the person you are sharing the data with to be able to write to your storage on the cluster.</li> </ol> <p>For more information, please see the official Globus Documentation.</p>"},{"location":"data/globus/","title":"Large Transfers with Globus","text":"<p>For large data transfers both within Yale and to external collaborators, we recommend using Globus. Globus is a file transfer service that is efficient and easy to use. It has several advantages:</p> <ul> <li>Robust and fast transfers of large files and/or large collections of files.</li> <li>Files can be transferred between your computer and the clusters.</li> <li>Files can be transferred between Yale and other sites.</li> <li>A web and command-line interface for starting and monitoring transfers.</li> <li>Access to specific files or directories granted to external collaborators in a secure way.</li> </ul> <p>Globus transfers data between computers set up as \"endpoints\". The official YCRC endpoints are listed below. Transfers can be to and from these endpoints or those you have defined for yourself with Globus Connect.</p> <p>Course Accounts</p> <p>Globus does not work for course accounts (<code>&lt;course_id&gt;_&lt;netid&gt;</code>). Please try the other transfer methods listed in our Transfer documentation instead.</p>"},{"location":"data/globus/#cluster-endpoints","title":"Cluster Endpoints","text":"<p>We currently support endpoints for the following clusters.</p> Cluster Globus Endpoint Grace <code>Yale CRC Grace</code> McCleary <code>Yale CRC McCleary</code> Milgram <code>Yale CRC Milgram</code> <p>For Grace and McCleary, these endpoints provide access to all files you normally have access to.</p> <p>For security reasons, Milgram Globus uses a staging area (<code>/gpfs/milgram/globus/$NETID</code>).  Once uploaded, data should be moved from this staging area to its final location within Milgram. Files in the staging area are purged after 21 days.</p>"},{"location":"data/globus/#get-started-with-globus","title":"Get Started with Globus","text":"<ol> <li>In a browser, go to app.globus.org.</li> <li>Use the pull-down menu to select Yale and click \"Continue\".</li> <li>If you are not already logged into CAS, you will be prompted to log in.<ol> <li>[First login only] Do not associate with another account yet unless you are familiar with doing this</li> <li>[First login only] Select \"non-profit research or educational purposes\"</li> <li>[First login only] Click on \"Allow\" for allowing Globus Web App</li> </ol> </li> <li>From the file manager interface enter the name of the endpoint you would like to browse in the collection field (e.g. Yale CRC Grace)</li> <li>Click on the right-hand side menu option \"Transfer or Sync to...\"</li> <li>Enter the second endpoint name in the right search box (e.g. another cluster or your personal endpoint)</li> <li>Select one or more files you would like to transfer and click the appropriate start button on the bottom.</li> <li>To complete a partial transfer, you can click the \"sync\" checkbox in the Transfer Setting window on the Globus page, and then Globus should resume the transfer where it left off.</li> </ol>"},{"location":"data/globus/#manage-your-endpoints","title":"Manage Your Endpoints","text":"<p>To manage your endpoints, such as delete an endpoint, rename it, or share it with additional people (be aware, they will be able to access your storage), go to Manage Endpoint on the Globus website.</p>"},{"location":"data/globus/#set-up-an-endpoint-on-your-computer","title":"Set Up an Endpoint on Your Computer","text":"<p>You can set up your own endpoint for transferring data to and from your own computer with Globus Connect Personal. </p> <p>To transfer or share data between two personal endpoints, you will need to request access to the YCRC's Globus Plus subscription on this page.</p>"},{"location":"data/globus/#set-up-a-microsoft-onedrive-endpoint","title":"Set Up a Microsoft OneDrive Endpoint","text":"<ol> <li>Click on the following link: Globus OneDrive Endpoint</li> <li>Log into Globus, if needed.</li> <li>The first time you log into the endpoint, you will be asked ot grant access to your OneDrive account.  Click to allow access and be taken through the approval process.</li> <li>After granting approval, you will be able to access the top level of your Yale OneDrive via the Globus Collection \"Yale OneDrive\".</li> </ol>"},{"location":"data/globus/#set-up-a-dropbox-endpoint","title":"Set Up a Dropbox Endpoint","text":"<ol> <li>Click on the following link: Globus Dropbox Endpoint</li> <li>Log into Globus, if needed.</li> <li>The first time you log into the endpoint, you will be asked to grant access to your DropBox account.  Click to allow access and be taken through the approval process.</li> <li>After granting approval, you will be able to access the top level of your DropBox storage via the Globus Collection \"Yale Dropbox\".</li> </ol>"},{"location":"data/globus/#set-up-a-google-drive-endpoint","title":"Set Up a Google Drive Endpoint","text":"<p>The Globus connector is configured to only allow data to be uploaded into EliApps (Yale's GSuite for Education) Google Drive accounts. If you don't have an EliApps account, request one as described above.</p> <ol> <li>Click on the following link: Globus Google Drive Endpoint</li> <li>Log into Globus, if needed.</li> <li>The first time you login to the Globus Google Drive endpoint, you will be asked to grant access to your Google Drive. Click to allow access and be taken through the approval process.</li> <li>You may see your Yale EliApps account expressed in an uncommon format, such as netid@yale.edu@accounts.google.com. This is normal, and expected.</li> <li>After granting approval, you will be able to access your Google Drive via the Globus Collection \"YCRC Globus Google Drive Collection\". The default view is \"/My Drive\". To see \"/Team Drives\" and other Google Drive features use the \"up one folder\" arrow icon in the File Manager.</li> </ol> <p>Note</p> <p>There are \"rate limits\" to how much data and how many files you can transfer in any 24 hours period. If you have hit your rate limit, Globus should automatically resume the transfer during the next 24 hour period. You see a \"Endpoint Busy\" error during this time.</p> <p>Google has a 400,000 file limit per Shared Drive, so if you are archiving data to Google Drive, it is better to compress folders that contain lots of small files (e.g. using tar) before transferring. </p> <p>In our testing, we have seen up to 10MB/s upload and 100MB/s download speeds.</p>"},{"location":"data/globus/#setup-a-s3-endpoint","title":"Setup a S3 Endpoint","text":"<p>We support creating Globus S3 endpoints. To request a Globus S3 Endpoint, please contact YCRC. Please include in your request:</p> <ul> <li>S3 bucket name</li> <li>The Amazon Region for that bucket</li> <li>An initial list of Yale NetIDs who should be able to access the bucket</li> </ul> <p>Warning</p> <p>Please DO NOT send us the Amazon login credentials through an insecure method such as email or our ticketing system.</p> <p>After we have created your Globus S3 endpoint, you will be able to further self-serve you own access controls with the Globus portal.</p>"},{"location":"data/glossary/","title":"Glossary","text":"<p>To help clarify the way we refer to certain terms in our user documentation, here is a brief list of some of the words that regularly come up in our documents. Please reach out to us at hpc@yale.edu if there are any other words that need to be added.</p> <p>Account - used to authenticate and grant permission to access resources</p> <p>Account (Slurm) - an accounting mechanism to keep track of a group's computing usage</p> <p>Activate - making something operational</p> <p>Array - a data structure across a series of memory locations consisting of elements organized in an index</p> <p>Array (job) - a series of jobs that all request the same resources and run the same batch script</p> <p>Array Task ID - a unique sequential number with an appended number that refers to an individual task within the set of submitted jobs</p> <p>Channel - Community-led collections of packages created by a group or lab installed with conda to allow for a homogenous environment across systems</p> <p>CLI - Command Line Interface processes commands to a computer program in the form of lines of text in a window</p> <p>Cluster - a set of computers, called nodes) networked together so nodes can perform the tasks facilitated by a scheduling software</p> <p>Command - a specific order from a computer to execute a service with either an application or the operating system</p> <p>Compute Node - the nodes that work runs on to perform computational work</p> <p>Container - A stack of software, libraries and operating system that is independent of the host computer and can be accessed on other computers</p> <p>Container Image - Self-contained read-only files used to run applications</p> <p>CPU - Central Processing Units are the components of a system that perform basic operations and exchange data with the system\u2019s memory (also known as a processor)</p> <p>Data - items of information collected together for reference or analysis</p> <p>Database - a collection of structured data held within a computer</p> <p>Deactivate - making something de-operational</p> <p>Environment - a collection of hardware, software, data storage and networks that work together in facilitating the processing and exchange of information</p> <p>Extension - Suffix at the end of a filename to indicate the file type</p> <p>Fileset - a section of a storage device that is given a designated purpose</p> <p>Filesystem - a process that manages how and where data is stored</p> <p>Flag - (see Options)</p> <p>GPU - Graphics Processing Units are specialized circuits designed to rapidly manipulate memory and create images in a frame buffer for a displayed output</p> <p>GridFTP - an extension of the Fire Transfer Protocol for grid computing that allows users to transfer and save data on a different account such as Google Drive or other off network memory</p> <p>Group - a collection of users who can all be given the same permissions on a system</p> <p>GUI - Graphical User Interface allows users to interact with devices and applications through a visual window that can commonly display icons and predetermined fields</p> <p>Hardware - the physical parts of a computer</p> <p>Host - (ie. Host Computer) A device connected to a computer network that offers resources, services and applications to users on the network</p> <p>Image - (See Container Image)</p> <p>Index - a method of sorting data by creating keywords or a listing of data</p> <p>Interface - a boundary across which two or more computer system components can exchange information</p> <p>Job - a unit of work given to an operating system by a scheduler</p> <p>Job Array - a way to submit multiple similar jobs by associating each subjob with an index value based on an array task id</p> <p>Key - a variable value applied using an algorithm to a block of unencrypted text to produce encrypted text</p> <p>Load - transfer a program or data into memory or into the CPU</p> <p>Login Node - a node that users log in on to access the cluster</p> <p>Memory - (see RAM)</p> <p>Metadata - A set of data that describes and gives basic information about other data</p> <p>Module - a number of distinct but interrelated units that build up or into a program</p> <p>MPI - Message Passing Interface is a standardized and portable message-passing standard designed to function on parallel computing architectures</p> <p>Multiprocessing - the ability to operate more than one task simultaneously on the same program across two or more processors in a computer</p> <p>Node - a server in the cluster</p> <p>Option - a single letter or full word that modifies the behavior of a command in a predetermined way (also known as a flag or switch)</p> <p>Package - a collection of hardware and software needed to create a working system</p> <p>Parallel - (ex. Computing/Programming) Architecture in which several processes are carried out simultaneously across smaller, independent parts</p> <p>Partition - a section of a storage device that is given a designated purpose</p> <p>Partition (Slurm) - a collection of compute nodes available via the scheduler</p> <p>Path - A string of characters used to identify locations throughout a directory structure</p> <p>Pane - (Associate with window) A subdivision within a window where an independent terminal can run simultaneously alongside another terminal</p> <p>Processor - (see CPU)</p> <p>Queue - a sequence of objects arranged according to priority waiting to be processed</p> <p>RAM - Random Access Memory, also known as \"Memory\" can be read and changed in any order and is typically used to to store working data</p> <p>Reproducibility - the ability to execute the same results across multiple systems by different individuals using the same data</p> <p>Scheduler - the software used to assign resources to a job for tasks</p> <p>Scheduling - the act of assigning resources to a task through a software product</p> <p>Session - a temporary information exchange between two or more devices</p> <p>SSH - secure shell is a cryptographic network protocol for operating network services securely over an unsecured network</p> <p>Software - a collection of data and instructions that tell a computer how to operate</p> <p>Switch - (see Options)</p> <p>System - a set of integrated hardware and software that input, output, process, and store data and information</p> <p>Task ID - a unique sequential number used to refer to a task</p> <p>Terminal - Referring to a terminal program, a text-based interface for typing commands</p> <p>Toolchain - a set of tools performing individual actions used in delivering an operation</p> <p>Unload - remove a program or data from memory or out of the CPU</p> <p>User - a person interacting and utilizing a computing service</p> <p>Variable - assigned and referenced data values that can be called within a program and changed depending on how the program runs</p> <p>Window - (Associate with pane) the whole screen being displayed, containing subdivisions, or panes, that can run independent terminals alongside each other</p>"},{"location":"data/google-drive/","title":"Google Drive","text":"<p>Through Yale Google Apps for Education (EliApps), researchers have access to 5GB of storage with the option to purchase additional storage as needed. The Globus Google Drive connector allows you to create a Globus endpoint that allows you to use the Globus infrastructure to transfer data into your Google Drive account. As always, no sensitive data (e.g. ePHI, HIPAA) is allowed in Google Drive storage.</p>"},{"location":"data/google-drive/#eliapps","title":"EliApps","text":"<p>If your Yale email account is already an EliApps account (Gmail), then you are all set. If your Yale email is in Microsoft Office365, send an email to the ITS helpdesk requesting a \"no-email EliApps account\". Once it is created you can login to Google Drive using your EliApps account name, which will be of the form <code>netid@yale.edu</code>. The Globus connector is configured to only allow data to be uploaded into EliApps Google Drive accounts.</p>"},{"location":"data/google-drive/#google-shared-drives-formerly-team-drive","title":"Google Shared Drives (formerly Team Drive)","text":"<p>Shared Drives is an additional feature for EliApps that is available by request only (at the moment). A Shared Drive is a Google Drive space that solves a lot of ownership and permissions issues present with traditional shared Google Drive folder. Once you create a Shared Drive, e.g. for a project or research group, any data placed in that Drive are owned by the drive and the permission (which accounts can own or access the data) can be easily managed from the Shared Drive interface by drive owners. With Shared Drive, you can be sure the data will stay with research group as students and postdocs come and go. If your group already uses Google Drive, contact us if you need additional Shared Drives. Although group members are limited to a default of 5GB of EliApps Storage, this can be increased as needed by reaching out through the Yale ITS Google Shared page. Aside from these quota limits, there are also limits for Google Shared Drives put in place by Google directly. Some are listed below.</p> <p>Warning</p> <p>To keep file counts low (and for easier data retrieval) we highly recommended that you archive your data using zip or tar.</p> Limit type Limit Number of files and folders 400,000 Daily upload cap 750 GiB Max individual file size 5 TiB Max number of nested folders 20"},{"location":"data/google-drive/#local-file-access","title":"Local File Access","text":"<p>You can upload and access your data using the web portal and sync data with your local machines via the Google File Stream software. For sync with your local machine, install Drive for desktop. Authenticate with your EliApps account and you will see Google Drive mounted as an additional drive on your machine.</p>"},{"location":"data/google-drive/#rclone","title":"Rclone","text":"<p>You can also transfer data using the command line utility Rclone. Rclone can be used to transfer data to any Google Drive account.</p>"},{"location":"data/google-drive/#globus-google-drive-connector","title":"Globus Google Drive Connector","text":"<p>You can use Globus to transfer data to/from any EliApps Google Drive as well. See our Globus documentation for more information.</p>"},{"location":"data/group-change/","title":"Group Change","text":"<p>When your PI is changed, the primary group of your account on the cluster will also be changed.  As a result, you will have a new storage space on the cluster which belongs to the new group, including Home, Project, Scratch, etc.</p> <p>We will change the primary group of your cluster account to the new group and will move all the files stored in your  old storage space into the new storage space. However, some local installations most likely will not be able to work  properly after being moved. In particular, Conda environments and R packages will fail. You need to rebuild them in your new space under the new group.</p> <p>For R packages, you just need to reinstall them with <code>install.packages()</code>. </p>"},{"location":"data/group-change/#rebuild-a-conda-environment-after-group-change","title":"Rebuild a Conda Environment after Group Change","text":"<p>We will use an example to illustrate how to rebuild a conda env after group change. Assume the conda env is  originally installed in <code>/gpfs/gibbs/project/oldgrp/user123</code>, and we want to move it to the project directory of the new group. </p> <p>First, find the paths of the conda env stored in your old space that you want to rebuild in the new space. Set two environment variables CONDA_ENVS_PATH and CONDA_PKGS_DIRS to the paths. <pre><code>module load miniconda\nexport CONDA_ENVS_PATH=/gpfs/gibbs/project/oldgrp/user123/conda_envs\nexport CONDA_PKGS_DIRS=/gpfs/gibbs/project/oldgrp/user123/conda_pkgs\nconda activate myenv\nconda env export &gt; myenv.yml\nconda deactivate\n</code></pre></p> <p>Now, start a new login session, submit an interactive job, and rebuild the conda env in your new storage space. When a new session starts, CONDA_ENVS_PATH and CONDA_PKGS_DIRS will be set to the right locations by the system,  so you don't have to set them explicitly.  <pre><code>ssh grace\nsalloc\nmodule load miniconda\nconda env create -f myenv.yml\n</code></pre></p>"},{"location":"data/hpc-storage/","title":"HPC Storage","text":"<p>Along with access to the compute clusters we provide each research group with cluster storage space for research data. The storage is separated into three quotas: Home, Project, and 60-day Scratch. Each of these quotas limit both the amount in bytes and number of files you can store. Hitting your quota stops you from being able to write data, and can cause jobs to fail. You can monitor your storage usage by running the <code>getquota</code> command on a cluster. No sensitive data can be stored on any cluster storage, except for Milgram.</p> <p>Backups</p> <p>The only storage backed up on every cluster is Home. We do provide local snapshots, covering at least the last 2 days, on Home and Project directories (see below for details). Please see our HPC Policies page for additional information about backups.</p>"},{"location":"data/hpc-storage/#storage-spaces","title":"Storage Spaces","text":"<p>For an overview of which filesystems are mounted on each cluster, see the HPC Resources documentation.</p>"},{"location":"data/hpc-storage/#home","title":"Home","text":"<p>Quota: 125 GiB and 500,000 files per person</p> <p>Your home directory is where your sessions begin by default. Its intended use is for storing scripts, notes, final products (e.g. figures), etc.  Its path is <code>/home/netid</code> (where <code>netid</code> is your Yale netid) on every cluster. Home storage is backed up daily. If you would like to restore files, please contact us with your netid and the list of files/directories you would like restored.</p>"},{"location":"data/hpc-storage/#project","title":"Project","text":"<p>Quota: 1 TiB and 5,000,000 files per group, expanded to 4 TiB on request</p> <p>Project storage is shared among all members of a specific group. Project storage is not backed up, so we strongly recommend that you have a second copy somewhere off-cluster of any valuable data you have stored in project. </p> <p>You can access this space through a symlink, or shortcut, in your home directory called <code>project</code>. See our Sharing Data documentation for instructions on sharing data in your project space with other users.</p> <p>Project quotas are global to the whole project space, so if the group ownership on a file is your group, it will count towards your quota, regardless of its location within <code>project</code>. This can occasionally create confusion for users who belong to multiple groups and they need to be mindful of which files are owned by which of their group affiliations to ensure proper accounting.</p>"},{"location":"data/hpc-storage/#purchased-storage","title":"Purchased Storage","text":"<p>Quota: varies</p> <p>Storage purchased for the dedicated use by a single group or collection of groups provides similar functionality as <code>project</code> storage and is also not backed up. See below for details on purchasing storage.  Purchased storage, if applicable, is assigned on one of our two storage systems:</p> <ul> <li>on the Gibbs filesystem in a <code>/gpfs/gibbs/pi/</code> directory under the group's name</li> <li>on the Palmer filesystem in a <code>/vast/palmer/pi/</code> directory under the group's name  </li> </ul> <p>Unlike project space described above, all files in your purchased storage count towards your quotas, regardless of file ownership.</p>"},{"location":"data/hpc-storage/#60-day-scratch","title":"60-Day Scratch","text":"<p>Quota: 10 TiB and 15,000,000 files per group</p> <p>60-day scratch is intended to be used for storing temporary data. Any file in this space older than 60 days will automatically be deleted. We send out a weekly warning about files we expect to delete the following week. Like project, scratch quota is shared by your entire research group. If we begin to run low on storage, you may be asked to delete files younger than 60 days old. Artificial extension of scratch file expiration is forbidden without explicit approval from the YCRC. Please purchase storage if you need additional longer term storage.</p> <p>You can access this space through a symlink, or shortcut, in your home directory called <code>palmer_scratch</code> (or <code>scratch60</code> on Milgram). See our Sharing Data documentation for instructions on sharing data in your scratch space with other users.</p>"},{"location":"data/hpc-storage/#check-your-usage-and-quotas","title":"Check Your Usage and Quotas","text":"<p>To inspect your current usage, run the command <code>getquota</code>. Here is an example output of the command:</p> <pre><code>This script shows information about your quotas on grace.\nIf you plan to poll this sort of information extensively,\nplease contact us for help at hpc@yale.edu\n\n## Usage Details for support (as of Jan 25 2023 12:00)\nFileset                User  Usage (GiB) File Count   \n---------------------- ----- ---------- -------------\ngibbs:project          ahs3         568       121,786\ngibbs:project          kln26        435       423,219\ngibbs:project          ms725        233       456,736\ngibbs:project          pl543        427     1,551,959\ngibbs:project          rdb9        1952     1,049,346\ngibbs:project          tl397        605     2,573,824\n----\ngibbs:pi_support       ahs3           0             1\ngibbs:pi_support       kln26       5886    14,514,143\ngibbs:pi_support       ms725      19651     2,692,158\ngibbs:pi_support       pl543        328       142,936\ngibbs:pi_support       rdb9        1047       165,553\ngibbs:pi_support       tl397        175       118,038\n\n## Quota Summary for support (as of right now [*palmer stats are gathered once a day])\nFileset                Type    Usage (GiB)  Quota (GiB) File Count    File Limit    Backup    Purged   \n---------------------- ------- ------------ ----------- ------------- ------------- --------- ---------\npalmer:home.grace      USR               63         125       216,046       500,000 Yes       No        \ngibbs:project          GRP             3832       10240     3,350,198    10,000,000 No        No        \npalmer:scratch         GRP                0       10240           903    15,000,000 No        60 days        \ngibbs:pi_support       FILESET        27240       30720    17,647,694    22,000,000 No        No\n</code></pre> <p>The per-user breakdown is only generated periodically, and the summary at the bottom is close to real-time. Purchased storage allocations will only appear in the <code>getquota</code> output for users who have data in that directory.</p>"},{"location":"data/hpc-storage/#purchase-additional-storage","title":"Purchase Additional Storage","text":"<p>For long-term allocations, additional project storage spaces can be purchased on one of our shared filesystems, which provides similar functionality to the primary project storage. This storage currently costs $280/TiB (minimum of 10 TiB, with exact pricing to be confirmed before a purchase is made). The price covers all costs, including administration, power, cooling, networking, etc. YCRC commits to making the storage available for 5 years from the purchase date, after which the storage allocation will need to be renewed, or the allocation will expire and be removed (see Storage Expiration Policy).</p> <p>For shorter-term or smaller allocations, we have a monthly billing option. More details on this option can be found here (CAS login required).</p> <p>Please note that, as with existing project storage, purchased storage will not be backed up, so you should make arrangements for the safekeeping of critical files off the clusters. Please contact us with your requirements and budget to start the purchasing process.</p> <p>Purchased storage, as with all storage allocations, are subject to corresponding file count limit to preserve the health of the shared storage system. The file count limits for different size allocations are listed above.  If you need additional files beyond your limit, contact us to discuss as increases may be granted on a case-by-case basis and at the YCRC's discretion.</p> Allocation Quota File Count Limit &lt; 50 TiB 10 million 50-99 TiB 20 million 100-499 TiB 40 million 500-999 TiB 50 million &gt;= 1 PiB 75 million"},{"location":"data/hpc-storage/#hpc-storage-best-practices","title":"HPC Storage Best Practices","text":""},{"location":"data/hpc-storage/#stage-data","title":"Stage Data","text":"<p>Large datasets are often stored off-cluster on departmental servers, Storage@Yale, in cloud storage, etc. If these data are too large to fit in your current quotas and you do not plan on purchasing more storage (see above), you must 'stage' your data. Since the permanent copy of the data remains on off-cluster storage, you can transfer a working copy to <code>palmer_scratch</code>, for example. Both Grace and McCleary have dedicated <code>transfer</code> partitions where you can submit long-running transfer jobs. When your computation finishes you can remove the copy and transmit or copy results to a permanent location. Please see the Staging Data documentation for more details and examples.</p>"},{"location":"data/hpc-storage/#prevent-large-numbers-of-small-files","title":"Prevent Large Numbers of Small Files","text":"<p>The parallel filesystems the clusters use perform poorly with very large numbers of small files. This is one reason we enforce file count quotas. If you are running an application that unavoidably make large numbers of files, do what you can to reduce file creation. Additionally you can reduce load on the filesystem by spreading the files across multiple subdirectories. Delete unneeded files between jobs and compress or archive collections of files.</p>"},{"location":"data/loomis-decommission/","title":"Loomis Decommission","text":"<p>After over eight years in service, the primary storage system on Grace, Loomis (/gpfs/loomis), was retired in December 2022. Since its inception, Loomis  doubled in size to host over 2 petabytes of data for more than 600 research groups and almost 4000 individual researchers. The usage and capacity on Loomis has been replaced by two existing YCRC storage systems, Palmer and Gibbs. </p>"},{"location":"data/loomis-decommission/#unified-storage-at-the-ycrc","title":"Unified Storage at the YCRC","text":"<p>2022 saw the introduction of a more unified approach to storage across the YCRC\u2019s clusters. Each group will have one project and one scratch space that is available on all of the HPC clusters (except for Milgram).</p>"},{"location":"data/loomis-decommission/#project","title":"Project","text":"<p>A single project space to host no-cost project-style storage allocations is available on the Gibbs storage system. Purchased allocations are also on Gibbs under the /gpfs/gibbs/pi space of the storage system. Grace users are using this space as of the August 2022 maintenance.</p>"},{"location":"data/loomis-decommission/#scratch","title":"Scratch","text":"<p>A single scratch space on Palmer, available for Grace users at /vast/palmer/scratch, serves both Grace and McCleary cluster (replacement for Farnam and Ruddle). The Loomis scratch space was decommissioned and purged on October 3, 2022.</p>"},{"location":"data/loomis-decommission/#software","title":"Software","text":"<p>In 2023, a new unified software and module tree was created on Palmer, so the same software will be available for use regardless of which YCRC HPC cluster you are using. </p> <p>We have migrated the software located in /gpfs/loomis/apps/avx to Palmer at /vast/palmer/apps/grace.avx. To continue to support this software without interruption, we are maintaining a symlink at /gpfs/loomis/apps/avx to the new location on Palmer, so software will continue to appear as if it is on Loomis even after the maintenance, despite being hosted on Palmer. In August 2023, Grace was upgraded to Red Hat 8 and this old software tree was deprecated and is no longer supported.</p>"},{"location":"data/loomis-decommission/#what-about-existing-data-on-loomis","title":"What about Existing Data on Loomis?","text":"<p>Your Grace home directory was already migrated to Palmer during the January 2022 maintenance.</p> <p>During the Grace Maintenance in August 2022, we migrated all of the Loomis project space (<code>/gpfs/loomis/project</code>) to the Gibbs storage system at <code>/gpfs/gibbs/project</code>. You will need to update your scripts and workflows to point to the new location (<code>/gpfs/gibbs/project/&lt;group&gt;/&lt;netid&gt;</code>). The \"project\" symlink in your home directory has been updated to point to your new space (with a few exceptions described below), so scripts using the symlinked path will not need to be updated.</p> <p>If you had a project space that exceeds the no-cost allocation (4TiB), your data was migrated to a new allocation under <code>/gpfs/gibbs/pi</code>. In these instances, your group has been granted a new, empty \"project\" space with the default no-cost quota. Any scripts will need to be updated accordingly.</p> <p>The Loomis scratch space was decommissioned and purged on October 3, 2022.</p>"},{"location":"data/loomis-decommission/#conda-environments","title":"Conda Environments","text":"<p>By default, all conda environments are installed into your project directory. However, most conda environments do not survive being moved from one location to another, so you may need to regenerate your conda environment(s). To assist with this, we provide conda-export documentation.</p>"},{"location":"data/loomis-decommission/#r-packages","title":"R Packages","text":"<p>Similarly, in 2022 we started redirecting user R packages to your project space to conserve home directory usage. If your R environment is not working as expected, we recommend deleting the old installation (found in <code>~/project/R/&lt;version&gt;</code>) and rerunning install.packages.</p>"},{"location":"data/loomis-decommission/#custom-software-installations","title":"Custom Software Installations","text":"<p>If you or your group had any self-installed software in the project directory, it is possible that the migration will have broken the software and it will need to be recompiled.</p>"},{"location":"data/loomis-decommission/#decommission-of-old-deprecated-software-trees","title":"Decommission of Old, Deprecated Software Trees","text":"<p>As part of the Loomis Decommission, we did not be migrating the old software trees located at /gpfs/loomis/apps/hpc, /gpfs/loomis/apps/hpc.rhel6 and /gpfs/loomis/apps/hpc.rhel7. The deprecated modules can be identified as being prefixed with \"Apps/\", \"GPU/\", \"Libs/\" or \"MPI/\" rather than beginning with the software name. If you are using software modules in one of the old trees, please find an alternative in the current supported tree or reach out to us to install a replacement.</p>"},{"location":"data/loomis-decommission/#researchers-with-purchased-storage-on-loomis","title":"Researchers with Purchased Storage on Loomis","text":"<p>If you had purchased space that is still active (not expired), we created a new area of the same size for you on Gibbs and transferred your data. </p> <p>If you have purchased storage on <code>/gpfs/loomis</code> that has expired or will be expiring in 2022 and you chose not to renew, any data in that allocation is now retired.</p>"},{"location":"data/mccleary-transfer/","title":"Transfer data from Farnam / Ruddle to McCleary","text":"<p>In the process of migrating from Farnam/Ruddle to McCleary, we are requesting researchers migrate their own data. Researchers are encouraged to only transfer data which is actively needed and take this opportunity to archive or delete old data.  Transfers should be initiated on Ruddle's or McCleary's <code>transfer</code> nodes and sync'd to either Gibbs project directories (<code>/gpfs/gibbs/project/GROUP/NETID</code>) or their McCleary home spaces (which are mounted at <code>/vast/palmer/home.mccleary/NETID</code>). All users are able to log into the <code>transfer</code> nodes via ssh:</p> <pre><code>[tl397@ruddle1 ~]$ ssh transfer\n[tl397@transfer-ruddle ~]$\n</code></pre> <p>Warning</p> <p>Do not attempt to transfer conda environments to McCleary.  Environments are not portable and will not work properly if simply copied. Instead, please export and rebuild environments following our guide.</p> <p>The two tools we recommend for this transfer are <code>rsync</code> and <code>Globus</code>. <code>rsync</code> is a command-line utility which copies files, along with their attributes, with protections against file corruption.  <code>Globus</code> is a web app where you can schedule large transfers which occur in the background and provide notifications when complete. Since McCleary mounts Farnam and Ruddle's filesystems, these copies are \"local\" copies and should run at high speed. <code>rsync</code> is best suited for smaller data transfers, while <code>Globus</code> is our recommended tool for larger transfers. </p> <p>In this short note we will detail these two approaches.</p>"},{"location":"data/mccleary-transfer/#rsync","title":"Rsync","text":"<p>While <code>rsync</code> is most commonly used for remote transfers between two systems, it is an excellent tool for local work as well. In particular, it's ability to perform tests to make sure that files are transfered properly and to recover from interrupted transfers make it a good option for data migration. There are many configuration possibilities, but we recommend using the following flags:</p> <pre><code>rsync -avP /path/to/existing/data /path/to/new/home/for/data\n</code></pre> <p>Here the <code>-a</code> will run the transfer in <code>archive</code> mode, which preserves ownership, permissions, and creation/modification times. Additionally, the <code>-v</code> will run in <code>verbose</code> mode where the name of every file is printed out, and <code>-P</code> displays a progress bar.</p> <p>One subtle detail is that <code>rsync</code> changes its behavior based on whether the source path has a trailing <code>/</code>.  If one initiates a sync like this: <pre><code>rsync -avP /path/to/existing/data /path/to/new/home/for/data\n</code></pre> the existing <code>data</code> directory is transferred as a whole entity, including the top-level directory <code>data</code>. However, if the source path includes a trailing <code>/</code>:</p> <p><pre><code>rsync -avP /path/to/existing/data/ /path/to/new/home/for/data\n</code></pre> then the contents of data are transferred, omitting the top-level directory. </p> <p>As an example, to transfer a directory (named <code>my_data</code>) from a YSM project directory on McCleary to your Gibbs project space, you can run:</p> <pre><code>rsync -avP /gpfs/ysm/project/GROUP/NETID/my_data /gpfs/gibbs/project/GROUP/NETID/\n</code></pre> <p>Similarly, to transfer a directory (<code>my_code</code>) from your YCGA homespace to your new McCleary homespace:</p> <pre><code>rsync -avP /home/NETID/my_code /vast/palmer/home.mccleary/NETID/\n</code></pre> <p>where <code>GROUP</code> and <code>NETID</code> are replaced by your specific group/netid.</p> <p>For more detailed information about <code>rsync</code>, please take a look at this nice tutorial (link).</p> <p>For <code>rsync</code> transfers that may take a while, it's best to run the transfer inside a tmux virtual login session. This enables you to \"detach\" from the session while the transfer continues in the background. <code>tmux</code> uses special key-strokes to control the session, with the most important being <code>Ctrl-b d</code> (first pressing the <code>control</code> and <code>b</code> keys, releasing, and then pressing <code>d</code>) which detaches from the current session. To reattach to a detached session, run <code>tmux attach</code> from the same host where <code>tmux</code> was initially started. For more information about <code>tmux</code>, please see their Getting Started Guide.</p>"},{"location":"data/mccleary-transfer/#globus","title":"Globus","text":"<p>Yale provides dedicated Globus connections for each of the clusters.  Transfers can be managed through existing accounts on Ruddle using <code>yale#ruddle</code>, or using McCleary's Globus connection (<code>Yale CRC McCleary</code>). For a general getting started with Globus, please check out their website. We have a stand-alone docs page about Globus here, but here we will detail the process to transfer data from YSM (for example) to the Gibbs file system.</p> <ol> <li>log in to app.globus.org and use your Yale credentials to authenticate. </li> <li>navigate to the File Manager and access Ruddle or McCleary by searching for the \"collection\" <code>yale#ruddle</code> or <code>Yale CRC McCleary</code> in the left-hand panel.</li> <li>find the files you wish to transfer, using the check-boxes to select any and all files needed.</li> <li>click on the \"Transfer or Sync to\" option and in the right-hand panel also search for the same cluster's collection.</li> <li>navigate through the file-browser to find the desired destination for these data (most likely <code>gibbs_project</code> or a subdirectory).</li> <li>start the transfer, click the \"Start\" button on the left-hand side.</li> </ol> <p>This will start a background process to transfer all the selected files and directories to their destination. You will receive an email when the transfer completes detailing the size and average speed of the transferred data.</p>"},{"location":"data/mccleary-transfer/#getting-help","title":"Getting help","text":"<p>If you run into any issues or if you would like help in setting up your data migration, please feel free to reach out to hpc@yale.edu to request one-on-one support.</p>"},{"location":"data/permissions/","title":"Share with Cluster Users","text":""},{"location":"data/permissions/#home-directories","title":"Home Directories","text":"<p>Do not give your home directory group write permissions. This will break your ability to log into the cluster.  If you need to share files currently located in your home directory, either move it your project directory or contact us for assistance finding an appropriate location.</p>"},{"location":"data/permissions/#project-and-scratch60-links-in-home-directories","title":"<code>project</code> and <code>scratch60</code> links in Home Directories","text":"<p>For convenience, we create a symlink, or shortcut, in every home directory called <code>project</code> and <code>palmer_scratch</code> (and <code>~/scratch60</code>on Milgram) that go to your respective storage spaces. However, if another user attempts to access any data via your symlink, they will receive errors related to permissions for your home space.</p> <p>You can run <code>mydirectories</code> or <code>readlink - f dirname</code> (replace <code>dirname</code> with the one you are interested in) to get the \"true\" paths, which is more readily accesible to other users.</p>"},{"location":"data/permissions/#share-data-within-your-group","title":"Share Data within your Group","text":"<p>By default, all project, purchased allocation and scratch directories are readable by other members of your group. As long as they use the true path (not the shortcut your home directory, see above), no permission changes should be needed.</p> <p>If you want to ensure all new files and directories you create have group write permission, add the following line to your <code>~/.bashrc</code> files:</p> <pre><code>umask 002\n</code></pre>"},{"location":"data/permissions/#shared-group-directories","title":"Shared Group Directories","text":"<p>Upon request we can setup directories for sharing scripts or data across your research group. These directories can either have read-only permissions for the group (so no one accidentally modifies something) or read and write permissions for all group members. If interested, contact us to request such a directory.</p>"},{"location":"data/permissions/#share-with-specific-users-or-other-groups","title":"Share With Specific Users or Other Groups","text":"<p>It can be very useful to create shared directories that can be read and written by multiple users, or all members of a group. The linux command <code>setfacl</code> is useful for this, but can be complicated to use. We recommend that you create a shared directory somewhere in your <code>project</code> or <code>scratch</code> directories, rather than <code>home</code>. When sharing a sub-directory in your <code>project</code> or <code>scratch</code>, you need first share your <code>project</code> or <code>scratch</code>, and then share the sub-directory. Here are some simple scenarios.</p>"},{"location":"data/permissions/#share-a-directory-with-all-members-of-a-group","title":"Share a Directory with All Members of a Group","text":"<p>To share a new directory called <code>shared</code> in your project directory with group <code>othergroup</code>:</p> <pre><code>setfacl -m g:othergroup:rx $(readlink -f ~/project)\ncd ~/project\nmkdir shared\nsetfacl -m g:othergroup:rwX shared\nsetfacl -d -m g:othergroup:rwX shared\n</code></pre>"},{"location":"data/permissions/#share-a-directory-with-a-particular-person","title":"Share a Directory with a Particular Person","text":"<p>To share a new directory called <code>shared</code> with a person with netid <code>aa111</code>:</p> <pre><code>setfacl -m u:aa111:rx $(readlink -f ~/project)\ncd ~/project\nmkdir shared\nsetfacl -m u:aa111:rwX shared\nsetfacl -d -m u:aa111:rwX shared\n</code></pre> <p>If the shared directory already exists and contains files and directories, you should run the setfacl commands recursively, using -R:</p> <pre><code>setfacl -R -m u:aa111:rwX shared\nsetfacl -R -d -m u:aa111:rwX shared\n</code></pre> <p>Note that only the owner of a file or directory can run setfacl on it.</p>"},{"location":"data/permissions/#remove-sharing-of-a-directory","title":"Remove Sharing of a Directory","text":"<p>To remove a group <code>othergroup</code> from sharing of a directory called <code>shared</code>:</p> <pre><code>setfacl -R -x g:othergroup shared\n</code></pre> <p>To remove a person with netid <code>aa111</code> from sharing of a directory called <code>shared</code>:</p> <pre><code>setfacl -R -x u:aa111 shared\n</code></pre>"},{"location":"data/staging/","title":"Stage Data for Compute Jobs","text":"<p>Large datasets are often stored off-cluster on departmental servers, Storage@Yale, in cloud storage, etc. Since the permanent home of the data remains on off-cluster storage, you need to transfer a working copy to the cluster temporarily. When your computation finishes, you would then remove the copy and transfer the results to a more permanent location.</p>"},{"location":"data/staging/#temporary-storage","title":"Temporary Storage","text":"<p>We recommend staging data into your scratch storage space on the cluster, as the working copy of the data can then be removed manually or left to be deleted (which will happen automatically after 60-days). </p>"},{"location":"data/staging/#interactive-transfers","title":"Interactive Transfers","text":"<p>For interactive transfers, please see our Transfer Data page for a more complete list of ways to move data efficiently to and from the clusters.  </p> <p>A sample workflow using <code>rsync</code> would be:</p> <pre><code># connect to the transfer node from the login node\n[netID@cluster ~] ssh transfer\n# copy data to temporary cluster storage\n[netID@transfer ~]$ rsync -avP netID@department_server:/path/to/data $HOME/palmer_scratch/\n# process data on cluster\n[netID@transfer ~]$ sbatch data_processing.sh\n# return results to permanent storage for safe-keeping\n[netID@transfer ~]$ rsync -avP $HOME/palmer_scratch/output_data netID@department_server:/path/to/outputs/\n</code></pre> <p>Tip</p> <p>To protect your transfer from network interruptions between your computer and the transfer node, launch your <code>rsync</code> inside a tmux session on the transfer node.</p>"},{"location":"data/staging/#transfer-partition","title":"Transfer Partition","text":"<p>Both Grace and McCleary have dedicated data transfer partitions (named <code>transfer</code>) designed for staging data onto the cluster. All users are able to submit jobs to these partitions. Note each users is limited to running two transfer jobs at one time. If your workflow requires more simultaneuous transfers, contact us for assistance.</p>"},{"location":"data/staging/#transfers-as-batch-jobs","title":"Transfers as Batch Jobs","text":"<p>A sample <code>sbatch</code> script for an <code>rsync</code> transfer is show here:</p> <p><pre><code>#!/bin/bash\n\n#SBATCH --partition=transfer\n#SBATCH --time=6:00:00\n#SBATCH --cpus-per-task=1\n#SBATCH --job-name=my_transfer\n#SBATCH --output=transfer.txt\n\nrsync -av netID@department_server:/path/to/data $HOME/palmer_scratch/\n</code></pre> This will launch a batch job that will transfer data from <code>remote.host.yale.edu</code> to your scratch directory. Note, this will only work if you have set up password-less logins on the remote host.</p>"},{"location":"data/staging/#transfer-job-dependencies","title":"Transfer Job Dependencies","text":"<p>There are <code>sbatch</code> options that allow you to hold a job from running until a previous job finishes. These are called Job Dependencies, and they allow you to include a data-staging step as part of your data processing pipe-line.</p> <p>Consider a workflow where we would like to process data located on a remote server. We can break this into two separate Slurm jobs: a transfer job followed by a processing job.</p>"},{"location":"data/staging/#transfersbatch","title":"transfer.sbatch","text":"<pre><code>#!/bin/bash\n\n#SBATCH --partition=transfer\n#SBATCH --time=6:00:00\n#SBATCH --cpus-per-task=1\n#SBATCH --job-name=my_transfer\n\nrsync -av netID@department_server:/path/to/data $HOME/palmer_scratch/\n</code></pre>"},{"location":"data/staging/#processsbatch","title":"process.sbatch","text":"<pre><code>#!/bin/bash\n\n#SBATCH --partition=day\n#SBATCH --time=6:00:00\n#SBATCH --cpus-per-task=1\n#SBATCH --job-name=my_process\n\nmodule purge\nmodule load miniconda\n\nconda activate my_env\n\npython $HOME/process_script.py $HOME/palmer_scratch/data\n</code></pre> <p>First we would submit the transfer job to Slurm:</p> <pre><code>$ sbatch transfer.sbatch\nSubmitted batch job 12345678\n</code></pre> <p>Then we can pass this jobID as a dependency for the processing job:</p> <p><pre><code>$ sbatch --dependency=afterok:12345678 process.sbatch\nSubmitted batch job 12345679\n</code></pre> Slurm will now hold the processing job until the transfer finishes:</p> <pre><code>$ squeue\nJOBID    PARTITION  NAME       USER   ST      TIME  NODES NODELIST(REASON)\n12345679       day  process    netID  PD      0:00      1 (Dependency)\n12345678  transfer  transfer   netID  R       0:15      1 c01n04\n</code></pre>"},{"location":"data/staging/#storageyale-transfers","title":"Storage@Yale Transfers","text":"<p>Storage@Yale shares are mounted on the transfer partition, enabling you to stage data from these remote servers. The process is somewhat simpler than the above example because we do not need to <code>rsync</code> the data, and can instead use <code>cp</code> directly.</p> <p>Here, we have modified the <code>transfer.sbatch</code> file from above:</p>"},{"location":"data/staging/#transfersbatch_1","title":"transfer.sbatch","text":"<pre><code>#!/bin/bash\n\n#SBATCH --partition=transfer\n#SBATCH --time=6:00:00\n#SBATCH --cpus-per-task=1\n#SBATCH --job-name=my_transfer\n\ncp /SAY/standard/my_say_share/data $HOME/palmer_scratch/\n</code></pre> <p>This will transfer <code>data</code> from the Storage@Yale share to <code>palmer_scratch</code> where it can be processed on any of the compute nodes.</p>"},{"location":"data/transfer/","title":"Transfer Data","text":"<p>For all transfer methods, you need to have set up your account on the cluster(s) you want to tranfer data to/from.</p>"},{"location":"data/transfer/#data-transfer-nodes","title":"Data Transfer Nodes","text":"<p>Each cluster has dedicated nodes specially networked for high speed transfers both on and off-campus using the Yale Science Network. You may use transfer nodes to transfer data from your local machine using one of the below methods. From off-cluster, the nodes are accessible at the following hostnames. You must still be on-campus or on the VPN to access the transfer nodes.</p> Cluster Transfer Node Bouchet <code>transfer-bouchet.ycrc.yale.edu</code> Grace <code>transfer-grace.ycrc.yale.edu</code> McCleary <code>transfer-mccleary.ycrc.yale.edu</code> Milgram <code>transfer-milgram.ycrc.yale.edu</code> Misha <code>transfer-misha.ycrc.yale.edu</code> <p>From the login node of any cluster, you can <code>ssh</code> into the transfer node. This is useful for transferring data to or from locations other than your local machine (see below for details).</p> <pre><code>[netID@cluster ~] ssh transfer\n</code></pre>"},{"location":"data/transfer/#transferring-data-tofrom-your-local-machine","title":"Transferring Data to/from Your Local Machine","text":""},{"location":"data/transfer/#graphical-transfer-tools","title":"Graphical Transfer Tools","text":""},{"location":"data/transfer/#ood-web-transfers","title":"OOD Web Transfers","text":"<p>On each cluster, you can use their respective Open OnDemand portals to transfer files. This works best for small numbers of relatively small files. You can also directly edit scripts through this interface, alleviating the need to transfer scripts to your computer to edit.</p>"},{"location":"data/transfer/#mobaxterm-windows","title":"MobaXterm (Windows)","text":"<p>MobaXterm is an all-in-one graphical client for Windows that includes a transfer pane for each cluster you connect to. Once you have established a connection to the cluster, click on the \"Sftp\" tab in the left sidebar to see your files on the cluster. You can drag-and-drop data into and out of the SFTP pane to upload and download, respectively.</p>"},{"location":"data/transfer/#cyberduck-maclinux","title":"Cyberduck (Mac/Linux)","text":"<p>Windows Computers Disclaimer: Recent updates as of 12/18/2024 have made Cyberduck incompatible with the authentication methods used on our clusters.                 If interested in using a client on windows, please use MobaXterm where setup instructions can be found here You can also transfer files between your local computer and a cluster using an FTP client, such as Cyberduck (OSX/Windows). You will need to configure the client with:</p> <ul> <li>Your netid as the \"Username\"</li> <li>Cluster transfer node (see above) as the \"Server\"</li> <li>Select your private key as the \"SSH Private Key\"</li> <li>Leave \"Password\" blank (you will be prompted on connection for your ssh key passphrase)</li> </ul> <p>An example configuration of Cyberduck is shown below.</p> <p></p>"},{"location":"data/transfer/#cyberduck-with-mfa","title":"Cyberduck with MFA","text":"<p>Our clusters require Multi-Factor Authentication so there are a couple additional configuration steps. Under <code>Cyberduck &gt; Preferences &gt; Transfers &gt; General</code> change the setting to \"Use browser connection\" instead of \"Open multiple connections\".</p> <p>When you connect type one of the following when prompted with a \"Partial authentication success\" window.</p> <ul> <li>\"push\" to receive a push notification to your smart phone (requires the Duo mobile app)</li> <li>\"phone\" to receive a phone call</li> </ul>"},{"location":"data/transfer/#large-file-transfers-globus","title":"Large File Transfers (Globus)","text":"<p>You can use the Globus service to perform larger data transfers between your local machine and the clusters. Globus provides a robust and resumable way to transfer larger files or datasets. Please see our Globus page for Yale-specific documentation and their official docs to get started.</p>"},{"location":"data/transfer/#command-line-transfer-tools","title":"Command-Line Transfer Tools","text":""},{"location":"data/transfer/#scp-and-rsync-macoslinuxlinux-on-windows","title":"scp and rsync (macOS/Linux/Linux on Windows)","text":"<p>Linux and macOS users can use scp or rsync. Use the hostname of the cluster transfer node (see above) to transfer files. These transfers must be initiated from your local machine.</p> <p>scp and sftp are both used from a Terminal window. The basic syntax of <code>scp</code> is</p> <pre><code>scp [from] [to]\n</code></pre> <p>The from and to can each be a filename or a directory/folder on the computer you are typing the command on or a remote host (e.g. the transfer node).</p>"},{"location":"data/transfer/#example-transfer-a-file-from-your-computer-to-a-cluster","title":"Example: Transfer a File from Your Computer to a Cluster","text":"<p>Using the example netid <code>abc123</code>, following is run on your computer's local terminal.</p> <pre><code>scp myfile.txt abc123@transfer-grace.ycrc.yale.edu:/home/fas/admins/abc123/test\n</code></pre> <p>In this example, <code>myfile.txt</code> is copied to the directory <code>/home/fas/admins/abc123/test:</code> on Grace. This example assumes that <code>myfile.txt</code> is in your current directory. You may also specify the full path of <code>myfile.txt</code>.</p> <pre><code>scp /home/xyz/myfile.txt abc123@transfer-grace.ycrc.yale.edu:/home/fas/admins/abc123/test\n</code></pre>"},{"location":"data/transfer/#example-transfer-a-directory-to-a-cluster","title":"Example: Transfer a Directory to a Cluster","text":"<pre><code>scp -r mydirectory abc123@transfer-grace.ycrc.yale.edu:/home/fas/admins/abc123/test\n</code></pre> <p>In this example, the contents of <code>mydirectory</code> are transferred. The <code>-r</code> indicates that the copy is recursive.</p>"},{"location":"data/transfer/#example-transfer-files-from-the-cluster-to-your-computer","title":"Example: Transfer Files from the Cluster to Your Computer","text":"<p>Assuming you would like the files copied to your current directory:</p> <pre><code>scp abc123@transfer-grace.ycrc.yale.edu:/home/fas/admins/abc123/myfile.txt .\n</code></pre> <p>Note that <code>.</code> represents your current working directory. To specify the destination, simply replace the <code>.</code> with the full path:</p> <pre><code>scp abc123@transfer-grace.ycrc.yale.edu:/home/fas/admins/abc123/myfile.txt /path/myfolder\n</code></pre>"},{"location":"data/transfer/#transfer-data-tofrom-other-locations","title":"Transfer Data to/from Other Locations","text":""},{"location":"data/transfer/#globus-endpoints","title":"Globus Endpoints","text":"<p>Globus is a web-enabled GridFTP service that transfers large datasets fast, securely, and reliably between computers configured to be endpoints. Please see our Globus page for Yale-specific documentation and their official docs to get started.</p> <ul> <li>We have configured endpoints for most of the Yale clusters and many other institutions and compute facilities have Globus endpoints. </li> <li>You can also use Globus to transfer data to/from Eliapps Google Drive and S3 buckets.</li> </ul>"},{"location":"data/transfer/#cluster-transfer-nodes","title":"Cluster Transfer Nodes","text":"<p>You can use the cluster transfer nodes to download/upload data to locations off-cluster. For data that is primarily hosted elsewhere and is only needed on the cluster temporarily, see our guide on Staging Data for additional information. For any data that hosted outside of Yale, you will need to initiate the transfer from the cluster's data transfer node as the clusters are not accessible without the VPN. On Milgram, which does not have a transfer node, you can initiate the transfers from a login node. However, please be mindful of that other users will also be using the login nodes for regular cluster operations.</p> <p>Tip</p> <p>If you are running a large transfer without Globus, run it inside a tmux session on the transfer node. This protects your transfer from network interruptions between your computer and the transfer node.</p>"},{"location":"data/transfer/#rsync","title":"rsync","text":"<pre><code># connect to the transfer node from the login node\n[netID@cluster ~] ssh transfer\n# copy data to cluster storage\n[netID@transfer ~]$ rsync -avP netID@department_server:/path/to/data $HOME/scratch60/\n</code></pre>"},{"location":"data/transfer/#rclone","title":"Rclone","text":"<p>To move data to and from cloud storage (Box, Dropbox, Wasabi, AWS S3, or Google Cloud Storage, etc.), we recommend using Rclone. It is installed on all of the clusters and can be installed on your computer. You will need to configure it for each kind of storage you would like to transfer to with:</p> <pre><code>rclone configure\n</code></pre> <p>You'll be prompted for a name for the connection (e.g mys3), and then details about the connection.  Once you've saved that configuration, you can connect to the transfer node (using <code>ssh transfer</code> from the login node) and then use that connection name to copy files with similar syntax to <code>scp</code> and <code>rsync</code>:</p> <pre><code>rclone copy localpath/myfile mys3:bucketname/\nrclone sync localpath/mydir mys3:bucketname/remotedir\n</code></pre> <p>We recommend that you protect your configurations with a password. You'll see that as an option when you run rclone config. Please see our Rclone page for additional information on how to set up and use Rclone on the YCRC clusters. For all the Rclone documentaion please refer to the official site.</p>"},{"location":"data/transfer/#sites-behind-a-vpn","title":"Sites Behind a VPN","text":"<p>If you need to transfer data to or from an external site that is only accessible via VPN, please contact us for assistance as we might be able to provide a workaround to enable a direct transfer between the YCRC clusters and your external site.</p>"},{"location":"data/ycga-data/","title":"YCGA Data","text":"<p>Data associated with YCGA projects and sequencers are located on the YCGA storage system, accessible at <code>/gpfs/ycga/sequencers</code> on McCleary.</p> <p>Important</p> <p>On Oct 1, 2024, we will be tightening access restrictions to the primary sequence data.  More information is available here.</p>"},{"location":"data/ycga-data/#ycga-access-policy","title":"YCGA Access Policy","text":"<p>The McCleary high-performance computing system has specific resources that are dedicated to YCGA users. This includes a slurm partition (\u2018ycga\u2019) and a large parallel storage system (/gpfs/ycga). The following policy guidelines govern the use of these resources on McCleary for data storage and analysis.</p>"},{"location":"data/ycga-data/#yale-university-faculty-user","title":"Yale University Faculty User","text":"<ol> <li>All Yale PIs using YCGA for library preparation and/or sequencing will have an additional 5 TB storage area called \u2018work\u2019 for data storage. This is in addition to the 5 TB storage area called \u2018project\u2019 that all McCleary groups receive.</li> <li>Currently, neither work or project storage is backed up. Users are responsible for protecting their own data.</li> <li>All Fastq files are available on the /gpfs/ycga storage system for one year. After that, the files are available in an archive that allows self-service retrieval, as described below. Issues or questions about archived data can be addressed to ycga@yale.edu.</li> <li>Users processing sequence data on McCleary should be careful to submit their jobs to the \u2018ycga\u2019 partition. Jobs submitted to other partitions may incur additional charges.</li> <li>Members of Yale PI labs using YCGA for library preparation and/or sequencing may apply for accounts on McCleary with PI\u2019s approval.</li> <li>Each Yale PI lab will have a dedicated 'work' directory to store their data, and permission to lab members will be granted with the authorization of the respective PI. Furthermore, such approval will be terminated upon request from the PI or termination of Yale Net ID.</li> <li>Lab members moving to a new university will get access to HPC resources for an additional six months only upon permission from Yale PI. If Yale NetID is no longer accessible, former Yale members who were YCGA users should request a Sponsored Identity NetID from their business office. Sponsored Identity NetIDs will be valid for six months. Such users will also need to request VPN access.</li> <li>A PI moving to a new university to establish their lab will have access to their data for one year from the termination of their Yale position. During this time, the PI or one lab member from the new lab will be provided access to the HPC system. Request for Guest NetID should be made to their business office. Guest NetID will be valid for one year.</li> <li>Any new Yale faculty member will be given access to McCleary once they start using YCGA services.</li> </ol>"},{"location":"data/ycga-data/#external-collaborators","title":"External Collaborators","text":"<ol> <li>Access to McCleary can be granted to collaborating labs, with the authorization of the respective Yale PI. A maximum of one account per collaborating lab will be granted. Furthermore, such approval will be terminated upon request from the PI. This requires obtaining a Sponsored Netid. The expectation is that the collaborator, with PI consent, will download data from the McCleary HPC system to their own internal system for data analysis.</li> </ol>"},{"location":"data/ycga-data/#non-yale-users","title":"Non-Yale Users","text":"<p>Users not affiliated with Yale University will not be provided access to the McCleary high-performance computing system.</p>"},{"location":"data/ycga-data/#ycga-data-retention-policy","title":"YCGA Data Retention Policy","text":"<p>YCGA-produced sequence data is initially written to YCGA's main storage system, which is located in the main HPC datacenter at Yale's West Campus. Data stored there is protected against loss by software RAID.  Raw basecall data (e.g. bcl files) is immediately transformed into DNA sequences (fastq files).</p> <ul> <li>~45 days after sequencing, the raw files are deleted.</li> <li>~60 days after sequencing, the fastq files are written to an archive.  This archive exists in two geographically distinct copies for safety.</li> <li>~365 days after sequencing, all data is deleted from main storage.  Users continue to have access to the data via the archive.  Data is retained on the archive indefinitely.  See below for instructions for retrieving archived data.</li> </ul> <p>All compression of sequence data is lossless.  Gzip is used for data stored on the main storage, and quip is used for data stored on the archive. Disaster recovery is provided by the archive copy.</p>"},{"location":"data/ycga-data/#accessing-sequence-data","title":"Accessing Sequence Data","text":"<p>YCGA will send you an email informing you that your data is ready, and will include a url that looks like: http://fcb.ycga.yale.edu:3010/randomstring/sample</p> <p>Tip</p> <p>To find the actual location of your data, do: <pre><code>$ readlink -f /gpfs/ycga/work/lsprog/tools/external/data/randomstring/sample_dir_YourSampleNumber\n</code></pre> For standard Illumina data (not 10X/singlecell or pacbio data)  you can browse to the YCGA-provided URL and open ruddle_paths.txt.  It contains the  true locations of the files.</p>"},{"location":"data/ycga-data/#retrieving-illumina-sequencing-data","title":"Retrieving Illumina sequencing data","text":"<p>If you want to copy the data to your own computer, you can use that link to download your data using a browser or a tool such as wget.  However, if you plan to process the data on McCleary, there are better alternatives.</p> <p>We recommend using the ycgaFastq tool to link to the sequencing files: <pre><code>$ module load ycga-public\n$ ycgaFastq fcb.ycga.yale.edu:3010/randomstring/sample\n</code></pre></p> <p>If the run has been archived and deleted, ycgaFastq will retrieve the data</p> <p>ycgaFastq has several alternative invocations:</p> <p>If you don't know the URL, you can use the  sample submitter's netid and the flowcell (run) name:</p> <pre><code>$ ycgaFastq rdb9 AHFH66DSXX\n</code></pre> <p>If you have a path to the original location of the sequencing data, ycgaFastq can retrieve the data using that, even if the run has been archived and deleted: <pre><code>$ ycgaFastq /ycga-gpfs/sequencers/illumina/sequencerD/runs/190607_A00124_0104_AHLF3MMSXX/Data/Intensities/BaseCalls/Unaligned-2/Project_Lz438\n</code></pre></p> <p>If you have a ruddle_paths.txt file that contains the paths to all of the data files in a dataset, you can use ycgaFastq as well:</p> <pre><code>$ ycgaFastq ruddle_paths.txt\n</code></pre> <p>ycgaFastq can be used in a variety of other ways to retrieve data.  For more information, see the documentation or contact us.</p> <p>Tip</p> <p>Original sequence data are archived pursuant to the YCGA retention policy. For long-running projects we recommend you keep a personal backup of your sequence files. If you need to retrieve archived sequencing data, please see our below.</p>"},{"location":"data/ycga-data/#retrieving-10x-or-pacbio-sequencing-data","title":"Retrieving 10X or Pacbio sequencing data","text":"<p>You will be provided with a URL to your data that looks like: http://fcb.ycga.yale.edu:3010/randomstring/sample</p> <p>If you want to copy the data to your own computer, you can use that link to download your data using a browser or a tool such as wget.  However, if you plan to process the data on McCleary, there are better alternatives.</p> <p>We recommend using the URLFetch tool to link to the sequencing files: <pre><code>$ module load ycga-public\n$ URLFetch fcb.ycga.yale.edu:3010/randomstring/sample\n</code></pre></p> <p>URLFetch will simply link to the data if it has not been deleted.  If it is archived and deleted, it will download a tarball of the data, which you can then untar.</p>"},{"location":"data/ycga-permissions/","title":"YCGA Data Permissions","text":"<p>Primary sequencing data produced by YCGA is only made accessible to the groups that submitted the data.</p> <p>In order to improve the security of fastq sequence data on the Yale HPC system (McCleary) we will be tightening permissions on that data to restrict access to the group that submitted the samples.  We will do our best to allow access by the correct groups, but in some cases you may lose access when we make the change.</p> <p>This change only impacts data in directories managed by YCGA; it does not impact sequencing data in your own directories.  </p> <p>In addition, you will no longer be able to directly access or download data from the sequence data archive (/SAY/archive/YCGA-729009-YCGA-A2/archive/\u2026).  To access archived data, you must use ycgaFastq or URLFetch, see below.</p> <p>We will make this change on Oct 1, 2024.  After that date, if you discover that you no longer have access to your data, please contact hpc@yale.edu.  Please provide the path to the data you wish to access, as well as a list of groups and/or users who you believe should have access.  YCRC and YCGA will review the request and restore access if approved.  Also, if you frequently require access to another group\u2019s sequencing data, we recommend that you ask the group\u2019s PI to add you to their group, which will give you access to all of their sequencing data.</p> <p>You can check to see who is a member of your group by running  /share/admins/bin/lman show groupname. To add users to a group, the PI should send an email to hpc@yale.edu.  You can use this template:</p> <p>Please add  to my group, and also make them a member of the ycga group. Illumina (not 10x) data: This is data located here: /gpfs/ycga/sequencers/illumina You can continue to access your illumina data using ycgaFastq and the url provided by YCGA or the original path to the data, exactly as before. <p>module load ycga-public ycgaFastq fcb.ycga.yale.edu:3010/randomstring/sample_dir_001 or ycgaFastq /gpfs/ycga/sequencers/illumina/sequencerC/runs/\u2026/Project_Foo</p> <p>If the fastq files are still on disk, ycgaFastq will simply make links.  If the data has been archived, it will be extracted from the archive and copied to your directory. 10x and Pacbio data: You may continue to use wget or similar to download data using the url.  However, we recommend that you access this data using URLFetch:</p> <p>module load ycga-public URLFetch http://fcb.ycga.yale.edu:3010/randomstring/stagingdir</p> <p>If the fastq files are still on disk, URLFetch will simply make links.  If the data has been archived, it will be automatically extracted from the archive and copied to your directory.</p> <p>Please let us know if you have any questions or concerns about this change.</p> <p>Sincerely,</p> <p>Shrikant Mane Director, YCGA</p>"},{"location":"news/2022-02-grace/","title":"2022 02 grace","text":""},{"location":"news/2022-02-grace/#grace-maintenance","title":"Grace Maintenance","text":"<p>February 3-6, 2022</p>"},{"location":"news/2022-02-grace/#software-updates","title":"Software Updates","text":"<ul> <li>Latest security patches applied</li> <li>Slurm updated to version 21.08.5</li> <li>NVIDIA driver updated to version 510.39.01 (except for nodes with K80 GPUs which are stranded at 470.82.01)</li> <li>Singularity updated to version 3.8.5</li> <li>Open OnDemand updated to version 2.0.20</li> </ul>"},{"location":"news/2022-02-grace/#hardware-updates","title":"Hardware Updates","text":"<ul> <li>Changes have been made to networking to improve performance of certain older compute nodes</li> </ul>"},{"location":"news/2022-02-grace/#changes-to-grace-home-directories","title":"Changes to Grace Home Directories","text":"<p>During the maintenance, all home directories on Grace have been moved to our new all-flash storage filesystem, Palmer. The move is in anticipation of the decommissioning of Loomis at the end of the year and will provide a robust login experience by protecting home directory interactions from data intensive compute jobs.</p> <p>Due to this migration, your home directory path has changed from <code>/gpfs/loomis/home.grace/&lt;netid&gt;</code> to <code>/vast/palmer/home.grace/&lt;netid&gt;</code>. Your home directory can always be referenced in bash and submission scripts and from the command line with the <code>$HOME</code> variable. Please update any scripts and workflows accordingly.</p>"},{"location":"news/2022-02-grace/#interactive-jobs","title":"Interactive Jobs","text":"<p>We have added an additional way to request an interactive job. The Slurm command <code>salloc</code> can be used to start an interactive job similar to <code>srun --pty bash</code>. In addition to being a simpler command (no <code>--pty bash</code> is needed), <code>salloc</code> jobs can be used to interactively test <code>mpirun</code> executables.</p>"},{"location":"news/2022-02-grace/#palmer-scratch","title":"Palmer scratch","text":"<p>Palmer is out of beta! We have fixed the issue with Plink on Palmer, so now you can use Palmer scratch for any workloads. See https://docs.ycrc.yale.edu/data/hpc-storage#60-day-scratch for more information on Palmer scratch. </p>"},{"location":"news/2022-02/","title":"2022 02","text":""},{"location":"news/2022-02/#february-2022","title":"February 2022","text":""},{"location":"news/2022-02/#announcements","title":"Announcements","text":""},{"location":"news/2022-02/#grace-maintenance","title":"Grace Maintenance","text":"<p>The biannual scheduled maintenance for the Grace cluster will be occurring February 1-3. During this time, the cluster will be unavailable. See the Grace maintenance email announcement for more details.</p>"},{"location":"news/2022-02/#data-transfers","title":"Data Transfers","text":"<p>For non-Milgram users doing data transfers, transfers should not be performed on the login nodes. We have a few alternative ways to get better networking and reduce the impact on the clusters\u2019 login nodes:</p> <ol> <li>Dedicated transfer node. Each cluster has a dedicated transfer node, <code>transfer-&lt;cluster&gt;.hpc.yale.edu</code>. You can ssh directly to this node and run commands. </li> <li>\u201ctransfer\u201d Slurm partition. This is a small partition managed by the scheduler for doing data transfer. You can submit jobs to it using <code>srun/sbatch -p transfer \u2026</code>  *For recurring or periodic data transfers (such as using cron), please use Slurm\u2019s scrontab to schedule jobs that run on the transfer partition instead.</li> <li>Globus. For robust transfers of larger amount of data, see our Globus documentation.</li> </ol> <p>More info about data transfers can be found in our Data Transfer documentation.</p>"},{"location":"news/2022-02/#software-highlights","title":"Software Highlights","text":"<ul> <li>Rclone is now installed on all nodes and loading the module is no longer necessary.</li> <li>MATLAB/2021b is now on all clusters.</li> <li>Julia/1.7.1-linux-x86_64 is now on all clusters.</li> <li>Mathematica/13.0.0 is now on Grace.</li> <li>QuantumESPRESSO/6.8-intel-2020b and QuantumESPRESSO/7.0-intel-2020b are now on Grace.</li> <li>Mathematica documentation has been updated with regards to configuring parallel jobs.</li> </ul>"},{"location":"news/2022-03/","title":"2022 03","text":""},{"location":"news/2022-03/#march-2022","title":"March 2022","text":""},{"location":"news/2022-03/#announcements","title":"Announcements","text":""},{"location":"news/2022-03/#snapshots","title":"Snapshots","text":"<p>Snapshots are now available on all clusters for home and project spaces. Snapshots enable self-service restoration of modified or deleted files for at least 2 days in the past. See our User Documentation for more details on availability and instructions.</p>"},{"location":"news/2022-03/#ood-file-browser-tip-shortcuts","title":"OOD File Browser Tip: Shortcuts","text":"<p>You can add shortcuts to your favorite paths in the OOD File Browser. See our OOD documentation for instructions on setting up shortcuts.</p>"},{"location":"news/2022-03/#software-highlights","title":"Software Highlights","text":"<ul> <li>R/4.1.0-foss-2020b is now on Grace.</li> <li>GCC/11.2.0 is now on Grace.</li> </ul>"},{"location":"news/2022-04-farnam/","title":"2022 04 farnam","text":""},{"location":"news/2022-04-farnam/#farnam-maintenance","title":"Farnam Maintenance","text":"<p>April 4-7, 2022</p>"},{"location":"news/2022-04-farnam/#software-updates","title":"Software Updates","text":"<ul> <li>Security updates</li> <li>Slurm updated to 21.08.6</li> <li>NVIDIA drivers updated to 510.47.03 (note: driver for NVIDIA K80 GPUs was upgraded to 470.103.01)</li> <li>Singularity replaced by Apptainer version 1.0.1 (note: the \"singularity\" command will still work as expected)</li> <li>Open OnDemand updated to 2.0.20</li> </ul>"},{"location":"news/2022-04-farnam/#hardware-updates","title":"Hardware Updates","text":"<ul> <li>Four new nodes with 4 NVIDIA GTX3090 GPUs each have been added</li> </ul>"},{"location":"news/2022-04-farnam/#changes-to-the-bigmem-partition","title":"Changes to the <code>bigmem</code> Partition","text":"<p>Jobs requesting less than 120G of memory are no longer allowed in the \"bigmem\" partition. Please submit these jobs to the general or scavenge partitions instead.</p>"},{"location":"news/2022-04-farnam/#changes-to-non-interactive-sessions","title":"Changes to non-interactive sessions","text":"<p>Non-interactive sessions (e.g. file transfers, commands sent over ssh) no longer load the standard cluster environment to alleviate performance issues due to unnecessary module loads. Please contact us if this change affects your workflow so we can resolve the issue or provide a workaround.</p>"},{"location":"news/2022-04/","title":"2022 04","text":""},{"location":"news/2022-04/#april-2022","title":"April 2022","text":""},{"location":"news/2022-04/#announcements","title":"Announcements","text":""},{"location":"news/2022-04/#updates-to-r-on-open-ondemand","title":"Updates to R on Open OnDemand","text":"<p>RStudio Server is out of beta! With the deprecation of R 3.x (see below), we will be removing RStudio Desktop with module R from Open OnDemand on June 1st.</p>"},{"location":"news/2022-04/#improvements-to-r-installpackages-paths","title":"Improvements to R install.packages Paths","text":"<p>Starting with the R 4.1.0 software module, we now automatically set an environment variable (<code>R_LIBS_USER</code>) which directs these packages to be stored in your project space. This will helps ensure that packages are not limited by home-space quotas and that packages installed for different versions of R are properly separated from each other. Previously installed packages should still be available and there should be no disruption from the change.</p>"},{"location":"news/2022-04/#instructions-for-running-a-mysql-server-on-the-clusters","title":"Instructions for Running a MySQL Server on the Clusters","text":"<p>Occasionally it could be useful for a user to run their own MySQL database server on one of the clusters.  Until now, that has not been possible, but recently we found a way using singularity.  Instructions may be found in our new MySQL guide.</p>"},{"location":"news/2022-04/#software-highlights","title":"Software Highlights","text":"<ul> <li>R 3.x modules have been deprecated on all clusters and are no longer supported. If you need to continue to use an older version of R, look at our R conda documentation.</li> <li>R/4.1.0-foss-2020b is now available on all clusters.</li> <li>Seurat/4.1.0-foss-2020b-R-4.1.0 (for using the Seurat R package) is now available on all clusters.</li> </ul>"},{"location":"news/2022-05-ruddle/","title":"2022 05 ruddle","text":""},{"location":"news/2022-05-ruddle/#ruddle-maintenance","title":"Ruddle Maintenance","text":"<p>May 2, 2022</p>"},{"location":"news/2022-05-ruddle/#software-updates","title":"Software Updates","text":"<ul> <li>Security updates</li> <li>Slurm updated to 21.08.7</li> <li>Singularity replaced by Apptainer version 1.0.1 (note: the \"singularity\" command will still work as expected)</li> <li>Lmod updated to 8.7</li> </ul>"},{"location":"news/2022-05-ruddle/#changes-to-non-interactive-sessions","title":"Changes to non-interactive sessions","text":"<p>Non-interactive sessions (e.g. file transfers, commands sent over ssh) no longer load the standard cluster environment to alleviate performance issues due to unnecessary module loads. Please contact us if this change affects your workflow so we can resolve the issue or provide a workaround.</p>"},{"location":"news/2022-05/","title":"2022 05","text":""},{"location":"news/2022-05/#may-2022","title":"May 2022","text":""},{"location":"news/2022-05/#announcements","title":"Announcements","text":""},{"location":"news/2022-05/#ruddle-maintenance","title":"Ruddle Maintenance","text":"<p>The biannual scheduled maintenance for the Ruddle cluster will be occurring May 3-5. During this time, the cluster will be unavailable. See the Ruddle maintenance email announcements for more details.</p>"},{"location":"news/2022-05/#remote-visualization-with-hardware-acceleration","title":"Remote Visualization with Hardware Acceleration","text":"<p>VirtualGL is installed on all GPU nodes on Grace, Farnam, and Milgram to provide hardware accelerated 3D rendering. Instructions on how to use VirtualGL to accelerate your 3D applications can be found at https://docs.ycrc.yale.edu/clusters-at-yale/guides/virtualgl/. </p>"},{"location":"news/2022-05/#software-highlights","title":"Software Highlights","text":"<ul> <li>Singularity is now called \"Apptainer\". Singularity is officially named \u201cApptainer\u201d as part of its move to the Linux Foundation. The new command <code>apptainer</code> works as drop-in replacement for <code>singularity</code>. However, the previous <code>singularity</code> command will also continue to work for the foreseeable future so no change is needed. The upgrade to Apptainer is on Grace, Farnam and Ruddle (as of the maintenance completion). Milgram will be upgraded to Apptainer during the June maintenance.</li> <li>Slurm has been upgraded to version 21.08.6 on Grace</li> <li>MATLAB/2022a is available on all clusters</li> </ul>"},{"location":"news/2022-06-milgram/","title":"2022 06 milgram","text":""},{"location":"news/2022-06-milgram/#milgram-maintenance","title":"Milgram Maintenance","text":"<p>June 7-8, 2022</p>"},{"location":"news/2022-06-milgram/#software-updates","title":"Software Updates","text":"<ul> <li>Security updates</li> <li>Slurm updated to 21.08.8-2</li> <li>NVIDIA drivers updated to 515.43.04</li> <li>Singularity replaced by Apptainer version 1.0.2 (note: the \"singularity\" command will still work as expected)</li> <li>Lmod updated to 8.7</li> <li>Open OnDemand updated to 2.0.23</li> </ul>"},{"location":"news/2022-06-milgram/#hardware-updates","title":"Hardware Updates","text":"<ul> <li>The hostnames of the compute nodes on Milgram were changed to bring them in line with YCRC naming conventions.</li> </ul>"},{"location":"news/2022-06-milgram/#changes-to-non-interactive-sessions","title":"Changes to non-interactive sessions","text":"<p>Non-interactive sessions (e.g. file transfers, commands sent over ssh) no longer load the standard cluster environment to alleviate performance issues due to unnecessary module loads. Please contact us if this change affects your workflow so we can resolve the issue or provide a workaround.</p>"},{"location":"news/2022-06/","title":"2022 06","text":""},{"location":"news/2022-06/#june-2022","title":"June 2022","text":""},{"location":"news/2022-06/#announcements","title":"Announcements","text":""},{"location":"news/2022-06/#farnam-decommission-mccleary-announcement","title":"Farnam Decommission &amp; McCleary Announcement","text":"<p>After more than six years in service, we will be retiring the Farnam HPC cluster later this year. Farnam will be replaced with a new HPC cluster, McCleary. The McCleary HPC cluster will be Yale's first direct-to-chip liquid cooled cluster, moving the YCRC and the Yale research computing community into a more environmentally friendly future. For more information about the decommission process and the launch of McCleary, see our website.</p>"},{"location":"news/2022-06/#rstudio-with-module-r-has-been-retired-from-open-ondemand-as-of-june-1st","title":"RStudio (with module R) has been retired from Open OnDemand as of June 1st","text":"<p>Please switch to RStudio Server which provides a better user experience. For users using a conda environment with RStudio, RStudio (with Conda R) will continue to be served on Open OnDemand.</p>"},{"location":"news/2022-06/#milgram-maintenance","title":"Milgram Maintenance","text":"<p>The biannual scheduled maintenance for the Milgram cluster will be occurring June 7-9. During this time, the cluster will be unavailable. See the Milgram maintenance email announcements for more details.</p>"},{"location":"news/2022-06/#software-highlights","title":"Software Highlights","text":"<ul> <li>QTLtools/1.3.1-foss-2020b is now available on Farnam.</li> <li>R/4.2.0-foss-2020b is available on all clusters.</li> <li>Seurat for R/4.2.0 is now available on all clusters through the R-bundle-Bioconductor/3.15-foss-2020b-R-4.2.0 module along with many other packages. Please check to see if any packages you need are available in these modules before running <code>install.packages</code>. </li> </ul>"},{"location":"news/2022-07/","title":"2022 07","text":""},{"location":"news/2022-07/#july-2022","title":"July 2022","text":""},{"location":"news/2022-07/#announcements","title":"Announcements","text":""},{"location":"news/2022-07/#loomis-decommission","title":"Loomis Decommission","text":"<p>After almost a decade in service, the primary storage system on Grace, Loomis (<code>/gpfs/loomis</code>), will be retired later this year. The usage and capacity on Loomis will be replaced by two existing YCRC storage systems, Palmer and Gibbs, which are already available on Grace. Data in Loomis project storage will be migrated to <code>/gpfs/gibbs/project</code> during the upcoming August Grace maintenance. See the Loomis Decommission documenation for more information and updates.</p>"},{"location":"news/2022-07/#updates-to-ood-jupyter-app","title":"Updates to OOD Jupyter App","text":"<p>OOD Jupyter App has been updated to handle conda environments more intelligently. Instead of listing all the conda envs in your account, the app now lists only the conda environments with Jupyter installed. If you do not see your desired environment listed in the dropdown, check that you have installed Jupyter in that environment. In addition, the \u201cjupyterlab\u201d checkbox in the app will only be visible if the environment selected has jupyterlab installed. </p>"},{"location":"news/2022-07/#ycrc-conda-environment","title":"YCRC conda environment","text":"<p><code>ycrc_conda_env.list</code> has been replaced by <code>ycrc_conda_env.sh</code>. To update your conda enviroments in OOD for the Jupyter App and RStudio Desktop (with Conda R), please run <code>ycrc_conda_env.sh update</code>.</p>"},{"location":"news/2022-07/#software-highlights","title":"Software Highlights","text":"<ul> <li>miniconda/4.12.0 is now available on all clusters</li> <li>RStudio/2022.02.3-492 is now available on all clusters. This is currently the only version that is compatible with the graphic engine used by R/4.2.0-foss-2020b.</li> <li>fmriprep/21.0.2 is now available on Milgram.</li> <li>cellranger/7.0.0 is now available on Farnam.</li> </ul>"},{"location":"news/2022-08-grace/","title":"2022 08 grace","text":""},{"location":"news/2022-08-grace/#grace-maintenance","title":"Grace Maintenance","text":"<p>August 2-4, 2022</p>"},{"location":"news/2022-08-grace/#software-updates","title":"Software Updates","text":"<ul> <li>Security updates</li> <li>Slurm updated to 22.05.2</li> <li>NVIDIA drivers updated to 515.48.07 (except for nodes with K80 GPUs, which are stranded at 470.129.06)</li> <li>Singularity replaced by Apptainer version 1.0.3 (note: the \"singularity\" command will still work as expected)</li> <li>Lmod updated to 8.7</li> <li>Open OnDemand updated to 2.0.26</li> </ul>"},{"location":"news/2022-08-grace/#hardware-updates","title":"Hardware Updates","text":"<ul> <li>Core components of the ethernet network were upgraded to improve performance and increase overall capacity.</li> </ul>"},{"location":"news/2022-08-grace/#loomis-decommission-and-project-data-migration","title":"Loomis Decommission and Project Data Migration","text":"<p>After over eight years in service, the primary storage system on Grace, Loomis (<code>/gpfs/loomis</code>), will be retired later this year.</p> <p>Project. We have migrated all of the Loomis project space (<code>/gpfs/loomis/project</code>) to the Gibbs storage system at <code>/gpfs/gibbs/project</code> during the maintenance. You will need to update your scripts and workflows to point to the new location (<code>/gpfs/gibbs/project/&lt;group&gt;/&lt;netid&gt;</code>). The \"project\" symlink in your home directory has been updated to point to your new space (with a few exceptions described below), so scripts using the symlinked path will not need to be updated. If you have jobs in a pending state going into the maintenance that used the absolute Loomis path, we recommend canceling, updating and then re-submitting those jobs so they do not fail.</p> <p>If you had a project space that exceeds the no-cost allocation (4 TiB), you have received a separate communication from us with details about your data migration. In these instances, your group has been granted a new, empty \"project\" space with the default no-cost quota. Any scripts will need to be updated accordingly.</p> <p>Conda.  By default, all conda environments are installed into your project directory. However, most conda environments do not survive being moved from one location to another, so you may need to regenerate your conda environment(s). To assist with this, we provide conda-export documentation.</p> <p>R.  Similarly, in 2022 we started redirecting user R packages to your project space to conserve home directory usage. If your R environment is not working as expected, we recommend deleting the old installation (found in <code>~/project/R/&lt;version&gt;</code>) and rerunning install.packages.</p> <p>Custom Software Installation. If you or your group had any self-installed software in the project directory, it is possible that the migration will have broken the software and it will need to be recompiled. Contact us if you need assistance recompiling.</p> <p>Scratch60. The Loomis scratch space (<code>/gpfs/loomis/scratch60</code>) is now read-only. All data  in that directory will be purged in 60 days on October 3, 2022. Any data in <code>/gpfs/loomis/scratch60</code> you wish to retain needs to be copied into another location by that date (such as your Gibbs project or Palmer scratch).</p>"},{"location":"news/2022-08-grace/#changes-to-non-interactive-sessions","title":"Changes to Non-Interactive Sessions","text":"<p>Non-interactive sessions (e.g. file transfers, commands sent over ssh) no longer load the standard cluster environment to alleviate performance issues due to unnecessary module loads. Please contact us if this change affects your workflow so we can resolve the issue or provide a workaround.</p>"},{"location":"news/2022-08/","title":"2022 08","text":""},{"location":"news/2022-08/#august-2022","title":"August 2022","text":""},{"location":"news/2022-08/#announcements","title":"Announcements","text":""},{"location":"news/2022-08/#grace-maintenance-storage-changes","title":"Grace Maintenance &amp; Storage Changes","text":"<p>The biannual scheduled maintenance for the Grace cluster will be occurring August 2-4. During this time, the cluster will be unavailable. See the Grace maintenance email announcement for more details.</p> <p>During the maintenance, significant changes will be made to the project and scratch60 directories on Grace. See our website for more information and updates.</p>"},{"location":"news/2022-08/#spinup-researcher-image-containers","title":"SpinUp Researcher Image &amp; Containers","text":"<p>Yale offers a simple portal for creating cloud-based compute resources called SpinUp. These cloud instances are hosted on Amazon Web Services, but have access to Yale services like Active Directory, DNS, and Storage at Yale. SpinUp offers a range of services including virtual machines, web servers, remote storage, and databases. </p> <p>Part of this service is a Researcher Image, an Ubuntu-based system which contains a suite of pre-installed commonly utilized software utilities, including: - PyTorch, TensorFlow, Keras, and other GPU-accelerated deep learning frameworks - GCC, Cmake, Go, and other development tools - Singularity/Apptainer and Docker for container development</p> <p>We recommend researchers looking to develop containers for use on YCRC HPC resources to utilize SpinUp to build containers which can then be copied to the clusters. </p> <p>If there are software utilities or commonly used tools that you would like added to the Researcher Image, let us know and we can work with the Cloud Team to get them integrated.</p>"},{"location":"news/2022-08/#software-highlights","title":"Software Highlights","text":"<ul> <li>AFNI/2022.1.14 is now available on Farnam and Milgram.</li> <li>cellranger/7.0.0 is now available on Grace.</li> </ul>"},{"location":"news/2022-09/","title":"2022 09","text":""},{"location":"news/2022-09/#september-2022","title":"September 2022","text":""},{"location":"news/2022-09/#announcements","title":"Announcements","text":""},{"location":"news/2022-09/#software-module-extensions","title":"Software Module Extensions","text":"<p>Our software module utility (Lmod) has been enhanced to enable searching for Python and R (among other software) extensions. This is a very helpful way to know which software modules contain a specific library or package. For example, to see what versions of <code>ggplot2</code> are available, use the <code>module spider</code> command. </p> <pre><code>$ module spider ggplot2\n--------------------------------------------------------\n  ggplot2:\n--------------------------------------------------------\n     Versions:\n        ggplot2/3.3.2 (E)\n        ggplot2/3.3.3 (E)\n        ggplot2/3.3.5 (E)\n</code></pre> <pre><code>$ module spider ggplot2/3.3.5\n-----------------------------------------------------------\n  ggplot2: ggplot2/3.3.5 (E)\n-----------------------------------------------------------\n    This extension is provided by the following modules. To access the extension you must load one of the following modules. Note that any module names in parentheses show the module location in the software hierarchy.\n\n       R/4.2.0-foss-2020b\n</code></pre> <p>This indicates that by loading the <code>R/4.2.0-foss-2020b</code> module you will gain access to <code>ggplot2/3.3.5</code>. </p>"},{"location":"news/2022-09/#software-highlights","title":"Software Highlights","text":"<ul> <li>topaz/0.2.5-fosscuda-2020b for use with RELION (fosscuda-2020b toolchain) is now available as a module on Farnam.</li> </ul>"},{"location":"news/2022-10-farnam/","title":"2022 10 farnam","text":""},{"location":"news/2022-10-farnam/#farnam-maintenance","title":"Farnam Maintenance","text":"<p>October 4-5, 2022</p>"},{"location":"news/2022-10-farnam/#software-updates","title":"Software Updates","text":"<ul> <li>Security updates</li> <li>Slurm updated to 22.05.3</li> <li>NVIDIA drivers updated to 515.65.01</li> <li>Lmod updated to 8.7</li> <li>Apptainer updated to 1.0.3 </li> <li>Open OnDemand updated to 2.0.28</li> </ul>"},{"location":"news/2022-10-farnam/#hardware-updates","title":"Hardware Updates","text":"<ul> <li>No hardware changes during this maintenance.</li> </ul>"},{"location":"news/2022-10/","title":"2022 10","text":""},{"location":"news/2022-10/#october-2022","title":"October 2022","text":""},{"location":"news/2022-10/#announcements","title":"Announcements","text":""},{"location":"news/2022-10/#farnam-maintenance","title":"Farnam Maintenance","text":"<p>The biannual scheduled maintenance for the Farnam cluster will be occurring Oct 4-6. During this time, the cluster will be unavailable. See the Farnam maintenance email announcements for more details.</p>"},{"location":"news/2022-10/#gibbs-maintenance","title":"Gibbs Maintenance","text":"<p>Additionally, the Gibbs storage system will be unavailable on Grace and Ruddle on Oct 4 to deploy an urgent firmware fix. All jobs on those clusters will be held, and no new jobs will be able to start during the upgrade to avoid job failures.</p>"},{"location":"news/2022-10/#new-command-for-interactive-jobs","title":"New Command for Interactive Jobs","text":"<p>The new version of Slurm (the scheduler) has improved the process of launching an interactive compute job. Instead of the clunky <code>srun --pty bash</code> syntax from previous versions, this is now replaced with <code>salloc</code>. In addition, the interactive partition is now the default partition for jobs launched using <code>salloc</code>. Thus a simple (1 core, 1 hour) interactive job can be requested like this:</p> <pre><code>salloc\n</code></pre> <p>which will submit the job and then move your shell to the allocated compute node. </p> <p>For MPI users, this allows multi-node parallel jobs to be properly launched inside an interactive compute job, which did not work as expected previously. For example, here is a two-node job, launched with <code>salloc</code> and then a parallel job-step launched with <code>srun</code>:</p> <pre><code>[user@grace1 ~]$ salloc --nodes 2 --ntasks 2 --cpus-per-task 1\nsalloc: Nodes p09r07n[24,28] are ready for job\n\n[user@p09r07n24 ~]$ srun hostname\np09r07n24.grace.hpc.yale.internal\nP09r07n28.grace.hpc.yale.internal\n</code></pre> <p>For more information on <code>salloc</code>, please refer to Slurm\u2019s documentation.</p>"},{"location":"news/2022-10/#software-highlights","title":"Software Highlights","text":"<ul> <li>cellranger/7.0.1 is now available on Farnam.</li> <li>LAMMPS/23Jun2022-foss-2020b-kokkos is now available on Grace.</li> </ul>"},{"location":"news/2022-11-ruddle/","title":"2022 11 ruddle","text":""},{"location":"news/2022-11-ruddle/#ruddle-maintenance","title":"Ruddle Maintenance","text":"<p>November 1, 2022</p>"},{"location":"news/2022-11-ruddle/#software-updates","title":"Software Updates","text":"<ul> <li>Security updates</li> <li>Slurm updated to 22.05.5</li> <li>Apptainer updated to 1.1.2 </li> <li>Open OnDemand updated to 2.0.28</li> </ul>"},{"location":"news/2022-11-ruddle/#hardware-updates","title":"Hardware Updates","text":"<ul> <li>No hardware changes during this maintenance.</li> </ul>"},{"location":"news/2022-11/","title":"2022 11","text":""},{"location":"news/2022-11/#november-2022","title":"November 2022","text":""},{"location":"news/2022-11/#announcements","title":"Announcements","text":""},{"location":"news/2022-11/#ruddle-maintenance","title":"Ruddle Maintenance","text":"<p>The biannual scheduled maintenance for the Ruddle cluster will be occurring Nov 1-3. During this time, the cluster will be unavailable. See the Ruddle maintenance email announcements for more details.</p>"},{"location":"news/2022-11/#grace-and-milgram-maintenance-schedule-change","title":"Grace and Milgram Maintenance Schedule Change","text":"<p>We will be adjusting the timing of Grace and Milgram's scheduled maintenance periods. Starting this December, Grace's maintenance periods will occur in December and June, with the next scheduled for December 6-8, 2022. Milgram's next maintenance will instead be performed in February and August, with the next scheduled for February 7-9, 2023. Please refer to previously sent communications for more information and see the full maintenance schedule for next year on our status page.</p>"},{"location":"news/2022-11/#requeue-after-timeout","title":"Requeue after Timeout","text":"<p>The YCRC clusters all have maximum time-limits that sometimes are shorter than a job needs to finish. This can be a frustration for researchers trying to get a simulation or a project finished. However, a number of workflows have the ability to periodically save the status of a process to a file and restart from where it left off. This is often referred to as \"checkpointing\" and is built into many standard software tools, like Gaussian and Gromacs. </p> <p>Slurm is able to send a signal to your job just before it runs out of time. Upon receiving this signal, you can have your job save its current status and automatically submit a new version of the job which picks up where it left off.  Here is an example of a simple script that resubmits a job after receiving the <code>TIMEOUT</code> signal:</p> <pre><code>#!/bin/bash\n#SBATCH -p day\n#SBATCH -t 24:00:00\n#SBATCH -c 1\n#SBATCH --signal=B:10@30 # send the signal `10` at 30s before job finishes\n#SBATCH --requeue        # mark this job eligible for requeueing\n\n# define a `trap` that catches the signal and requeues the job\ntrap \"echo -n 'TIMEOUT @ '; date; echo 'Resubmitting...'; scontrol requeue ${SLURM_JOBID}  \" 10\n\n# run the main code, with the `&amp;` to \u201cbackground\u201d the task\n./my_code.exe &amp;\n\n# wait for either the main code to finish to receive the signal\nwait\n</code></pre> <p>This tells Slurm to send <code>SIGNAL10</code> at ~30s before the job finishes. Then we define an action (or <code>trap</code>) based on this signal which requeues the job. Don\u2019t forget to add the <code>&amp;</code> to the end of the main executable and the <code>wait</code> command so that the trap is able to catch the signal. </p>"},{"location":"news/2022-11/#software-highlights","title":"Software Highlights","text":"<ul> <li>MATLAB/2022b is now available on all clusters.</li> </ul>"},{"location":"news/2022-12-grace/","title":"2022 12 grace","text":""},{"location":"news/2022-12-grace/#grace-maintenance","title":"Grace Maintenance","text":"<p>December 6-8, 2022</p>"},{"location":"news/2022-12-grace/#software-updates","title":"Software Updates","text":"<ul> <li>Slurm updated to 22.05.6</li> <li>NVIDIA drivers updated to 520.61.05</li> <li>Apptainer updated to 1.1.3 </li> <li>Open OnDemand updated to 2.0.28</li> </ul>"},{"location":"news/2022-12-grace/#hardware-updates","title":"Hardware Updates","text":"<ul> <li>Roughly 2 racks worth of equipment were moved to upgrade the effective InfiniBand connection speeds of several compute nodes (from 56 to 100 Gbps)</li> <li>The InfiniBand network was modified to increase capacity and allow for additional growth</li> <li>Some parts of the regular network were improved to shorten network paths and increase shared-uplink bandwidth</li> </ul>"},{"location":"news/2022-12-grace/#loomis-decommission","title":"Loomis Decommission","text":"<p>The Loomis GPFS filesystem has been retired and unmounted from Grace, Farnam, and Ruddle.  For additional information please see the Loomis Decommission page.</p>"},{"location":"news/2022-12/","title":"2022 12","text":""},{"location":"news/2022-12/#december-2022","title":"December 2022","text":""},{"location":"news/2022-12/#announcements","title":"Announcements","text":""},{"location":"news/2022-12/#grace-gibbs-maintenance","title":"Grace &amp; Gibbs Maintenance","text":"<p>The biannual scheduled maintenance for the Grace cluster will be occurring December 6-8. During this time, the cluster will be unavailable. Additionally, the Gibbs filesystem will be unavailable on Farnam and Ruddle on Tuesday, December 6th to deploy a critical firmware upgrade. See the maintenance email announcements for more details.</p>"},{"location":"news/2022-12/#loomis-decommission","title":"Loomis Decommission","text":"<p>The Loomis GPFS filesystem will be retired and unmounted from Grace and Farnam during the Grace December maintenance starting on December 6th. All data except for a few remaining private filesets have already been transferred to other systems (e.g., current software, home, scratch to Palmer and project to Gibbs). The remaining private filesets are being transferred to Gibbs in advance of the maintenance and owners should have received communications accordingly. The only potential user impact of the retirement is on anyone using the older, deprecated software trees. Otherwise, the Loomis retirement should have no user impact but please reach out if you have any concerns or believe you are still using data located on Loomis. See the Loomis Decommission documentation for more information. </p>"},{"location":"news/2022-12/#apptainer-upgrade-on-grace-and-ruddle","title":"Apptainer Upgrade on Grace and Ruddle","text":"<p>The newest version of Apptainer (v1.1, available now on Ruddle and, after December maintenance, on Grace) comes the ability to create containers without needing elevated privileges (i.e. <code>sudo</code> access). This greatly simplifies the container workflow as you no longer need a separate system to build a container from a definition file. You can simply create a definition file and run the build command. </p> <p>For example, to create a simple toy container from this def file (<code>lolcow.def</code>):</p> <pre><code>BootStrap: docker\nFrom: ubuntu:20.04\n\n%post\n    apt-get -y update\n    apt-get -y install cowsay lolcat\n\n%environment\n    export LC_ALL=C\n    export PATH=/usr/games:$PATH\n\n%runscript\n    date | cowsay | lolcat\n</code></pre> <p>You can run:</p> <pre><code>salloc -p interactive -c 4 \napptainer build lolcow.sif lolcow.def\n</code></pre> <p>This upgrade is live on Ruddle and will be applied on Grace during the December maintenance. For more information, please see the Apptainer documentation site  and our docs page on containers.  </p>"},{"location":"news/2022-12/#software-highlights","title":"Software Highlights","text":"<ul> <li>RELION/4.0.0-fosscuda-2020b for cryo-EM/cryo-tomography data processing is now available on Farnam. RELION/3.1 will no longer be updated by the RELION developer. Note that data processed with RELION 4 are not backwards compatible with RELION 3.</li> </ul>"},{"location":"news/2023-01/","title":"2023 01","text":""},{"location":"news/2023-01/#january-2023","title":"January 2023","text":""},{"location":"news/2023-01/#announcements","title":"Announcements","text":""},{"location":"news/2023-01/#open-ondemand-vscode","title":"Open OnDemand VSCode","text":"<p>A new OOD app code-server is now available on all clusters, except Milgram (coming in Feb). Code-server allows you to run VSCode in a browser on a compute node. All users who have been running VSCode on a login node via the ssh extension should switch to code-server immediately.  The app allows you to use GPUs, to allocate large memories, and to specify a private partition (if you have the access), things you won\u2019t be able to do if you run VSCode on a login node. The app is still in beta version and your feedback is much appreciated.</p>"},{"location":"news/2023-01/#milgram-transfer-node","title":"Milgram Transfer Node","text":"<p>Milgram now has a node dedicated to data transfers to and from the cluster. To access the node from within Milgram, run <code>ssh transfer</code> from the login node. To upload or download data from Milgram via the transfer node, use the hostname <code>transfer-milgram.hpc.yale.edu</code> (must be on VPN). More information can be found in our Transfer Data documentation.</p> <p>With the addition of the new transfer node, we ask that the login nodes are no longer used for data transfers to limit impact on regular login activities.</p>"},{"location":"news/2023-02-milgram/","title":"2023 02 milgram","text":""},{"location":"news/2023-02-milgram/#milgram-maintenance","title":"Milgram Maintenance","text":"<p>February 7, 2023</p>"},{"location":"news/2023-02-milgram/#software-updates","title":"Software Updates","text":"<ul> <li>Slurm updated to 22.05.7</li> <li>NVIDIA drivers updated to 525.60.13</li> <li>Apptainer updated to 1.1.4</li> <li>Open OnDemand updated to 2.0.29</li> </ul>"},{"location":"news/2023-02-milgram/#hardware-updates","title":"Hardware Updates","text":"<ul> <li>Milgram\u2019s network was restructured to reduce latency, and improve resiliency.</li> </ul>"},{"location":"news/2023-02/","title":"2023 02","text":""},{"location":"news/2023-02/#february-2023","title":"February 2023","text":""},{"location":"news/2023-02/#announcements","title":"Announcements","text":""},{"location":"news/2023-02/#milgram-maintenance","title":"Milgram Maintenance","text":"<p>The biannual scheduled maintenance for the Milgram cluster will be occurring Feb 7-9. During this time, the cluster will be unavailable. See the Milgram maintenance email announcements for more details.</p>"},{"location":"news/2023-02/#mccleary-launch","title":"McCleary Launch","text":"<p>The YCRC is pleased to announce the launch of the new McCleary HPC cluster.  The McCleary HPC cluster will be Yale's first direct-to-chip liquid cooled cluster, moving the YCRC and the Yale research computing community into a more environmentally friendly future. McCleary will be available in a \u201cbeta\u201d phase to Farnam and Ruddle users later on this month. Keep an eye on your email for further announcements about McCleary\u2019s availability.</p>"},{"location":"news/2023-03/","title":"2023 03","text":""},{"location":"news/2023-03/#march-2023","title":"March 2023","text":""},{"location":"news/2023-03/#announcements","title":"Announcements","text":""},{"location":"news/2023-03/#mccleary-now-available","title":"McCleary Now Available","text":"<p>The new McCleary HPC cluster is now available for active Farnam and Ruddle users\u2013all other researchers who conduct life sciences research can request an account using our Account Request form. Farnam and Ruddle will be retired in mid-2023 so we encourage all users on those clusters to transition their work to McCleary at your earliest convenience. If you see any issues on the new cluster or have any questions, please let us know at hpc@yale.edu.</p>"},{"location":"news/2023-03/#open-ondemand-vscode-available-everywhere","title":"Open OnDemand VSCode Available Everywhere","text":"<p>A new OOD app code-server is now available on all YCRC clusters, including Milgram and McCleary. The new code-server allows you to run VSCode in a browser on a compute node. All users who have been running VSCode on a login node via the ssh extension should switch to code-server at their earliest convenience. Unlike VSCode on the login node, the new app also enables you to use GPUs, to allocate large memory nodes, and to specify a private partition (if applicable) The app is still in beta version and your feedback is much appreciated.</p>"},{"location":"news/2023-03/#software-highlights","title":"Software Highlights","text":"<ul> <li>GPU-enabled LAMMPS (LAMMPS/23Jun2022-foss-2020b-kokkos-CUDA-11.3.1) is now available on Grace.</li> <li>AlphaFold/2.3.1-fosscuda-2020b is now available on Farnam and McCleary.</li> </ul>"},{"location":"news/2023-04/","title":"2023 04","text":""},{"location":"news/2023-04/#april-2023","title":"April 2023","text":""},{"location":"news/2023-04/#announcements","title":"Announcements","text":""},{"location":"news/2023-04/#mccleary-in-production-status","title":"McCleary in Production Status","text":"<p>During March, we have been adding nodes to McCleary, including large memory nodes (4 TiB), GPU nodes and migrating most of the commons nodes from Farnam to McCleary (that are not being retired). Moreover, we have finalized the setup of McCleary and the system is now production stable. Please feel comfortable to migrate your data and workloads from Farnam and Ruddle to McCleary at your earliest convenience.</p>"},{"location":"news/2023-04/#new-ycga-nodes-online-on-mccleary","title":"New YCGA Nodes Online on McCleary","text":"<p>McCleary now has over 3000 new cores dedicated to YCGA work! We encourage you to test your workloads and prepare to migrate from Ruddle to McCleary at your earliest convenience. More information can be found here. </p>"},{"location":"news/2023-04/#software-highlights","title":"Software Highlights","text":"<ul> <li>QuantumESPRESSO/7.1-intel-2020b available on Grace</li> <li>RELION/4.0.1 available on McCleary</li> <li>miniconda/23.1.0 available on all clusters</li> <li>scikit-learn/0.23.2-foss-2020b on Grace and McCleary</li> <li>seff-array updated to 0.4 on Grace, McCleary and Milgram</li> </ul>"},{"location":"news/2023-05-23/","title":"2023 05 23","text":""},{"location":"news/2023-05-23/#upcoming-maintenances","title":"Upcoming Maintenances","text":"<ul> <li> <p>The McCleary cluster will be unavailable from 9am-1pm on Tuesday May 30 while maintenance is performed on the YCGA storage.</p> </li> <li> <p>The Milgram, Grace and McCleary clusters will not be available from 2pm on Monday June 19 until 10am on Wednesday June 21, due to electrical work being performed in the HPC data center.  No changes will be made that impact users of the clusters.</p> </li> <li> <p>The regular Grace maintenance that had been scheduled for June 6-8 will be performed on August 15-17.  This change is being made in preparation for the upgrade to RHEL 8 on Grace.</p> </li> </ul>"},{"location":"news/2023-05/","title":"2023 05","text":""},{"location":"news/2023-05/#may-2023","title":"May 2023","text":""},{"location":"news/2023-05/#announcements","title":"Announcements","text":""},{"location":"news/2023-05/#farnam-decommission-june-1-2023","title":"Farnam Decommission: June 1, 2023","text":"<p>After many years of supporting productive science, the Farnam cluster will be decommissioned this summer as we transition to the newly deployed McCleary cluster. Logins will be disabled June 1, 2023, which will mark the official end of Farnam\u2019s service. Read-only access to Farnam\u2019s storage system (/gpfs/ysm) will be available on McCleary until July 13, 2023. All data on YSM (that you want to keep) will need to be transferred off YSM, either to non-HPC storage or to McCleary project space by you prior to YSM\u2019s retirement.</p>"},{"location":"news/2023-05/#ruddle-decommission-july-1-2023","title":"Ruddle Decommission: July 1, 2023","text":"<p>After many years of serving YCGA, the Ruddle cluster will also be decommissioned this summer as we transition to the newly deployed McCleary cluster. Logins will be disabled July 1, 2023, which will mark the official end of Ruddle\u2019s service. We will be migrating project and sequencing directories from Ruddle to McCleary. However, you are responsible for moving home and scratch data to McCleary before July 1, 2023.  </p> <p>Please begin to migrate your data and workloads to McCleary at your earliest convenience and reach out with any questions.</p>"},{"location":"news/2023-05/#mccleary-transition-reminder","title":"McCleary Transition Reminder","text":"<p>With our McCleary cluster now in a production stable state, we ask all Farnam users to ensure all home, project and scratch data the group wishes to keep is migrated to the new cluster ahead of the June 1st decommission. As June 1st is the formal retirement of Farnam, compute service charges on McCleary commons partitions will begin at this time. Ruddle users will have until July 1st to access the Ruddle and migrate their home and scratch data as needed. Ruddle users will NOT need to migrate their project directories; those will be automatically transferred to McCleary. As previously established on Ruddle, all jobs in the YCGA partitions will be exempt from compute service charges on the new cluster. For more information visit our McCleary Transition documentation.</p>"},{"location":"news/2023-05/#software-highlights","title":"Software Highlights","text":"<ul> <li>Libmamba solver for conda 23.1.0+ available on all clusters. Conda installations 23.1.0 and newer are now configured to use the faster environment solving algorithm developed by <code>mamba</code> by default. You can simply use <code>conda install</code> and enjoy the significantly faster solve times.</li> <li>GSEA available in McCleary and Ruddle OOD. Gene Set Enrichment Analysis (GSEA) is now available in McCleary OOD and Ruddle OOD for all users. You can access it by clicking \u201cInteractive Apps'' and then selecting \u201cGSEA\u201d. GSEA is a popular computational method to do functional analysis of multi omics data. Data files for GSEA are not centrally stored on the clusters, so you will need to download them from the GSEA website by yourself.</li> <li>NAG/29-GCCcore-11.2.0 available on Grace</li> <li>AFNI/2023.1.01-foss-2020b-Python-3.8.6 on McCleary</li> </ul>"},{"location":"news/2023-06/","title":"2023 06","text":""},{"location":"news/2023-06/#june-2023","title":"June 2023","text":""},{"location":"news/2023-06/#announcements","title":"Announcements","text":""},{"location":"news/2023-06/#mccleary-officially-launches","title":"McCleary Officially Launches","text":"<p>Today marks the official beginning of the McCleary cluster\u2019s service. In addition to compute nodes migrated from Farnam and Ruddle, McCleary features our first set of direct-to-chip liquid cooled (DLC) nodes, moving YCRC into a more environmentally friendly future. McCleary is significantly larger than the Farnam and Ruddle clusters combined. The new DLC compute nodes are able to run faster and with higher CPU density due to their superior cooling system.</p> <p>McCleary is named for Beatrix McCleary Hamburg, who received her medical degree in 1948 and was the first female African American graduate of Yale School of Medicine. </p>"},{"location":"news/2023-06/#farnam-farewell-june-1-2023","title":"Farnam Farewell: June 1, 2023","text":"<p>On the occasion of decommissioning the Farnam cluster on June 1, YCRC would like to acknowledge the profound impact Farnam has had on computing at Yale. Farnam supported biomedical computing at YSM and across the University providing compute resources to hundreds of research groups. Farnam replaced the previous biomedical cluster Louise, and began production in October 2016. Since then, it has run user jobs comprising more than 139 million compute hours. Farnam is replaced by the new cluster McCleary. </p> <p>Please note: Read-only access to Farnam\u2019s storage system (/gpfs/ysm) will be available on McCleary until July 13, 2023. For more information see McCleary transfer documentation.</p>"},{"location":"news/2023-06/#ruddle-decommission-july-1-2023","title":"Ruddle Decommission: July 1, 2023","text":"<p>The Ruddle cluster will be decommissioned and access will be disabled July 1, 2023. We will be migrating project and sequencing directories from Ruddle to McCleary.</p> <p>Please note: Users are responsible for moving home and scratch data to McCleary prior to July 1, 2023. For more information and instructions, see our McCleary transfer documentation.</p>"},{"location":"news/2023-06/#software-highlights","title":"Software Highlights","text":"<ul> <li>** R/4.3.0-foss-2020b+** available on all clusters. The newest version of R is now available on Grace, McCleary, and Milgram. This updates nearly 1000 packages and can be used in batch jobs and in RStudio sessions via Open OnDemand. </li> <li>AlphaFold/2.3.2-foss-2020b-CUDA-11.3.1 The latest version of AlphaFold (2.3.2, released in April) has been installed on McCleary and is ready for use. This version fixes a number of bugs and should improve GPU memory usage enabling longer proteins to be studied. </li> <li>LAMMPS/23Jun2022-foss-2020b-kokkos available on McCleary</li> <li>RevBayes/1.2.1-GCC-10.2.0 available on McCleary</li> <li>Spark 3.1.1 (CPU-only and GPU-enabled versions) available on McCleary</li> </ul>"},{"location":"news/2023-07/","title":"2023 07","text":""},{"location":"news/2023-07/#july-2023","title":"July 2023","text":""},{"location":"news/2023-07/#announcements","title":"Announcements","text":""},{"location":"news/2023-07/#red-hat-8-test-partitions-on-grace","title":"Red Hat 8 Test partitions on Grace","text":"<p>As Red Hat Enterprise Linux (RHEL) 7 approaches its end of life, we will be upgrading the Grace cluster to RHEL8 during the August 15th-17th maintenance. This will bring Grace in line with McCleary and provide a number of key benefits:</p> <ul> <li>continued security patches and support beyond 2023</li> <li>updated system libraries to better support modern software</li> <li>improved node management system to facilitate the growing number of nodes on Grace</li> <li>shared application tree between McCleary and Grace, which brings software parity between clusters</li> </ul> <p>While we have performed extensive testing, both internally and with the new McCleary cluster, we recognize that there are large numbers of custom workflows on Grace that may need to be modified to work with the new operating system.</p> <p>Please note:  To enable debugging and testing of workflows ahead of the scheduled maintenance, we have set aside <code>rhel8_day</code>, <code>rhel8_gpu</code>, and <code>rhel8_mpi</code> partitions. You should access them from the <code>rhel8_login</code> node.</p>"},{"location":"news/2023-07/#two-factor-authentication-for-mccleary","title":"Two-factor Authentication for McCleary","text":"<p>To assure the security of the cluster and associated services, we have implemented two-factor authentication on the McCleary cluster. To simplify the transition, we have collected a set of best-practices and configurations of many of the commonly used access tools, including CyberDuck, MobaXTerm, and WinSCPon, which you can access on our docs page. If you are using other tools and experiencing issues, please contact us for assistance.  </p>"},{"location":"news/2023-07/#new-gpu-nodes-on-mccleary-and-grace","title":"New GPU Nodes on McCleary and Grace","text":"<p>We have installed new GPU nodes for McCleary and Grace, dramatically increasing the number of GPUs available on both clusters. McCleary has 14 new nodes (56 GPUs) added to the gpu partition and six nodes (24 GPUs) added to <code>pi_cryoem</code>.  Grace has 12 new nodes, available in the <code>rhel8_gpu</code> partition. Each of the new nodes contains 4 NVIDIA A5000 GPUs, with 24GB of on-board VRAM and PCIe4 connection to improve data-transport time.</p>"},{"location":"news/2023-07/#software-highlights","title":"Software Highlights","text":"<ul> <li>MATLAB/2023a available on all clusters</li> <li>Beast/2.7.4-GCC-12.2.0 available on McCleary</li> <li>AFNI/2023.1.07-foss-2020b available on McCleary</li> <li>FSL 6.0.5.1 (CPU-only and GPU-enabled versions) available on McCleary</li> </ul>"},{"location":"news/2023-08-grace/","title":"2023 08 grace","text":""},{"location":"news/2023-08-grace/#grace-maintenance","title":"Grace Maintenance","text":"<p>August 15-17, 2023</p>"},{"location":"news/2023-08-grace/#software-updates","title":"Software Updates","text":"<ul> <li>Red Hat Enterprise Linux (RHEL) updated to 8.8</li> <li>Slurm updated to 22.05.9</li> <li>NVIDIA drivers updated to 535.86.10</li> <li>Apptainer updated to 1.2.2</li> <li>Open OnDemand updated to 2.0.32</li> </ul>"},{"location":"news/2023-08-grace/#upgrade-to-red-hat-8","title":"Upgrade to Red Hat 8","text":"<p>As part of this maintenance, the operating system on Grace has been upgraded to Red Hat 8.  A new unified software tree that is shared with the McCleary cluster has been created.</p> <p>The ssh host keys for Grace's login nodes were changed during the maintenance, which will result in a \"WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!\" error when you attempt to login. To access the cluster again, first remove the old host keys with the following command (if accessing the cluster via command line):</p> <pre><code>ssh-keygen -R grace.hpc.yale.edu\n</code></pre> <p>If you are using a GUI, such as MobaXterm, you will need to manually edit your known host file and remove the list related to Grace. For MobaXterm, this file is located (by default) in <code>Documents/MobaXterm/home/.ssh</code>.</p> <p>Then attempt a new login and accept the new host key.</p>"},{"location":"news/2023-08-grace/#new-open-ondemand-web-portal-url","title":"New Open OnDemand (Web Portal) URL","text":"<p>The new URL for the Grace Open OnDemand web portal is https://ood-grace.ycrc.yale.edu.</p>"},{"location":"news/2023-08-milgram/","title":"2023 08 milgram","text":""},{"location":"news/2023-08-milgram/#milgram-maintenance","title":"Milgram Maintenance","text":"<p>August 22, 2023_</p>"},{"location":"news/2023-08-milgram/#software-updates","title":"Software Updates","text":"<ul> <li>Slurm updated to 22.05.9</li> <li>NVIDIA drivers updated to 535.86.10</li> <li>Apptainer updated to 1.2.42</li> <li>Open OnDemand updated to 2.0.32</li> </ul>"},{"location":"news/2023-08-milgram/#multi-factor-authentication","title":"Multi-Factor Authentication","text":"<p>Multi-factor authentication is now required for ssh for all users on Milgram.  For most usage, this additional step is minimally invasive and makes our clusters much more secure.  However, for users who use graphical transfer tools such as Cyberduck, please see our MFA transfer documentation.</p>"},{"location":"news/2023-08/","title":"2023 08","text":""},{"location":"news/2023-08/#august-2023","title":"August 2023","text":""},{"location":"news/2023-08/#announcements","title":"Announcements","text":""},{"location":"news/2023-08/#ruddle-farewell-july-24-2023","title":"Ruddle Farewell: July 24, 2023","text":"<p>On the occasion of decommissioning the Ruddle cluster on July 24, the Yale Center for Genome Analysis (YCGA) and the Yale Center for Research Computing (YCRC) would like to acknowledge the profound impact Ruddle has had on computing at Yale. Ruddle provided the compute resources for YCGA's high throughput sequencing and supported genomic computing for hundreds of research groups at YSM and across the University. In February 2016, Ruddle replaced the previous biomedical cluster BulldogN. Since then, it has run more than 24 million user jobs comprising more than 73 million compute hours.</p> <p>Funding for Ruddle came from NIH grant 1S10OD018521-01, with Shrikant Mane as PI. Ruddle is replaced by a dedicated partition and storage on the new McCleary cluster, which were funded by NIH grant 1S10OD030363-01A1, also awarded to Dr. Mane.</p>"},{"location":"news/2023-08/#upcoming-grace-maintenance-august-15-17-2023","title":"Upcoming Grace Maintenance: August 15-17, 2023","text":"<p>Scheduled maintenance will be performed on the Grace cluster starting on Tuesday, August 15, 2023, at 8:00 am. Maintenance is expected to be completed by the end of day, Thursday, August 17, 2023.</p>"},{"location":"news/2023-08/#upcoming-milgram-maintenance-august-22-24-2023","title":"Upcoming Milgram Maintenance: August 22-24, 2023","text":"<p>Scheduled maintenance will be performed on the Milgram cluster starting on Tuesday, August 22, 2023, at 8:00 am. Maintenance is expected to be completed by the end of day, Thursday, August 24, 2023.</p>"},{"location":"news/2023-08/#grace-operating-system-upgrade","title":"Grace Operating System Upgrade","text":"<p>As Red Hat Enterprise Linux (RHEL) 7 approaches its end of life, we will be upgrading the Grace cluster from RHEL7 to RHEL8 during the August maintenance window. This will bring Grace in line with McCleary and provide a number of key benefits:</p> <ul> <li>continued security patches and support beyond 2023</li> <li>updated system libraries to better support modern software</li> <li>improved node management system to facilitate the growing number of nodes on Grace</li> <li>shared application tree between McCleary and Grace, which brings software parity between clusters</li> </ul> <p>Three test partitions are available (<code>rhel8_day</code>, <code>rhel8_gpu</code>, and <code>rhel8_mpi</code>) for use in debugging workflows before the upgrade. These partitions should be accessed from the <code>rhel8_login</code> node.</p>"},{"location":"news/2023-08/#software-highlights","title":"Software Highlights","text":"<ul> <li>Julia/1.9.2-linux-x86_64 available on Grace</li> <li>Kraken2/2.1.3-gompi-2020b available on McCleary</li> <li>QuantumESPRESSO/7.0-intel-2020b available on Grace</li> </ul>"},{"location":"news/2023-09/","title":"2023 09","text":""},{"location":"news/2023-09/#september-2023","title":"September 2023","text":""},{"location":"news/2023-09/#announcements","title":"Announcements","text":""},{"location":"news/2023-09/#grace-rhel8-upgrade","title":"Grace RHEL8 Upgrade","text":"<p>As Red Hat Enterprise Linux (RHEL) 7 approaches its end of life, we upgraded the Grace cluster from RHEL7 to RHEL8 during the August maintenance window.  This brings Grace in line with McCleary and provide a number of key benefits:</p> <ul> <li>continued security patches and support beyond 2023</li> <li>updated system libraries to better support modern software</li> <li>improved node management system to facilitate the growing number of nodes on Grace</li> <li>shared application tree between McCleary and Grace, which brings software parity between clusters</li> </ul> <p>There are a small number of compute nodes in the <code>legacy</code> partition with the old RHEL7 operating system installed for workloads that still need to be migrated. We expect to retire this partition during the Grace December 2023 maintenance. Please contact us if you need help upgrading to RHEL8 in the coming months.</p>"},{"location":"news/2023-09/#grace-old-software-deprecation","title":"Grace Old Software Deprecation","text":"<p>The RHEL7 application module tree (<code>/gpfs/loomis/apps/avx</code>) is now deprecated and will be removed from the default module environment during the Grace December maintenance. The software will still be available on Grace, but YCRC will no longer provide support for those old packages after December. If you are using a software package in that tree that is not yet installed into the new shared module tree, please let us know as soon as possible so we can help avoid any disruptions.</p>"},{"location":"news/2023-09/#software-highlights","title":"Software Highlights","text":"<ul> <li> <p>intel/2022b toolchain is now available on Grace and McCleary</p> <ul> <li>MKL 2022.2.1</li> <li>Intel MPI 2022.2.1</li> <li>Intel Compilers 2022.2.1</li> </ul> </li> <li> <p>foss/2022b toolchain is now available on Grace and McCleary</p> <ul> <li>FFTW 3.3.10</li> <li>ScaLAPACK 2.2.0</li> <li>OpenMPI 4.1.4</li> <li>GCC 12.2.0</li> </ul> </li> </ul>"},{"location":"news/2023-10-mccleary/","title":"2023 10 mccleary","text":""},{"location":"news/2023-10-mccleary/#mccleary-maintenance","title":"McCleary Maintenance","text":"<p>October 3-5, 2023_</p>"},{"location":"news/2023-10-mccleary/#software-updates","title":"Software Updates","text":"<ul> <li>Slurm updated to 23.02.5</li> <li>NVIDIA drivers updated to 535.104.12</li> <li>Lmod updated to 8.7.30</li> <li>Apptainer updated to 1.2.3</li> <li>System Python updated to 3.11</li> </ul>"},{"location":"news/2023-10/","title":"2023 10","text":""},{"location":"news/2023-10/#october-2023","title":"October 2023","text":""},{"location":"news/2023-10/#announcements","title":"Announcements","text":""},{"location":"news/2023-10/#mccleary-maintenance","title":"McCleary Maintenance","text":"<p>The biannual scheduled maintenance for the McCleary cluster will be occurring Oct 3-5. During this time, the cluster will be unavailable. See the McCleary maintenance email announcements for more details.</p>"},{"location":"news/2023-10/#interactive-jobs-on-day-on-mccleary","title":"Interactive jobs on <code>day</code> on McCleary","text":"<p>Interactive jobs are now allowed to be run on the <code>day</code> partition on McCleary. Note you are still limited to 4 interactive-style jobs of any kind (salloc or OpenOnDemand) at one time. Additional instances will be rejected until you delete older open instances. For OnDemand jobs, closing the window does not terminate the interactive app job. To terminate the job, click the \"Delete\" button in your \"My Interactive Apps\" page in the web portal.</p>"},{"location":"news/2023-10/#papermill-for-jupyter-command-line-execution","title":"\"Papermill\" for Jupyter Command-Line Execution","text":"<p>Many scientific workflows start as interactive Jupyter notebooks, and our Open OnDemand portal has dramatically simplified deploying these notebooks on cluster resources.  However, the step from running notebooks interactively to running jobs as a batch script can be challenging and is often a barrier to migrating to using <code>sbatch</code> to run workflows non-interactively. </p> <p>To help solve this problem, there are a handful of utilities that can execute a notebook as if you were manually hitting \"shift-Enter\" for each cell. Of note is Papermill which provides a powerful set of tools to bridge between interactive and batch-mode computing.</p> <p>To get started, install papermill into your Conda environments:</p> <pre><code>module load miniconda\nconda install papermill\n</code></pre> <p>Then you can simply evaluate a notebook, preserving figures and output inside the notebook, like this:</p> <pre><code>papermill /path/to/notebook.ipynb\n</code></pre> <p>This can be run inside a batch job that might look like this:</p> <pre><code>#!/bin/bash\n#SBATCH -p day\n#SBATCH -c 1\n#SBATCH -t 6:00:00\n\nmodule purge miniconda\nconda activate my_env\npapermill /path/to/notebook.ipynb\n</code></pre> <p>Variables can also be parameterized and passed in as command-line options so that you can run multiple copies simultaneously with different input variables. For more information see the [Papermill docs pages](https://papermill.readthedocs.io/.</p>"},{"location":"news/2023-11/","title":"2023 11","text":""},{"location":"news/2023-11/#november-2023","title":"November 2023","text":""},{"location":"news/2023-11/#announcements","title":"Announcements","text":""},{"location":"news/2023-11/#globus-available-on-milgram","title":"Globus Available on Milgram","text":"<p>Globus is now available to move data in and out from Milgram. For increased security, Globus only has access to a staging directory (<code>/gpfs/milgram/globus/$NETID</code>) where you can temporarily store data. Please see our documentation page for more information and reach out to hpc@yale.edu if you have any questions.</p>"},{"location":"news/2023-11/#rstudio-server-updates","title":"RStudio Server Updates","text":"<p>RStudio Server on the OpenDemand web portal for all clusters now starts an R session in a clean environment and will not save the session when you finish. If you want to save your session and reuse it next time, please select the checkbox \"Start R from your last saved session\".</p>"},{"location":"news/2023-12-grace/","title":"2023 12 grace","text":""},{"location":"news/2023-12-grace/#grace-maintenance","title":"Grace Maintenance","text":"<p>December 5-7, 2023</p>"},{"location":"news/2023-12-grace/#software-updates","title":"Software Updates","text":"<ul> <li>Slurm updated to 23.02.6</li> <li>NVIDIA drivers updated to 545.23.08</li> <li>Lmod updated to 8.7.32</li> <li>Apptainer updated to 1.2.4</li> </ul>"},{"location":"news/2023-12-grace/#multifactor-authentication-mfa","title":"Multifactor Authentication (MFA)","text":"<p>Multi-Factor authentication via Duo is now required for ssh for all users on Grace after the maintenance.  For most usage, this additional step is minimally invasive and makes our clusters much more secure. However, for users who use graphical transfer tools such as Cyberduck, please see our MFA transfer documentation.</p>"},{"location":"news/2023-12-grace/#transfer-node-host-key-change","title":"Transfer Node Host Key Change","text":"<p>The ssh host key for Grace's transfer node was changed during the maintenance, which will result in a \"WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!\" error when you attempt to login. To access the cluster again, first remove the old host keys with the following command (if accessing the cluster via command line):</p> <pre><code>ssh-keygen -R transfer-grace.ycrc.yale.edu\n</code></pre> <p>If you are using a GUI, such as MobaXterm, you will need to manually edit your known host file and remove the list related to Grace. For MobaXterm, this file is located (by default) in <code>Documents/MobaXterm/home/.ssh</code>.</p> <p>Then attempt a new login and accept the new host key.</p>"},{"location":"news/2023-12/","title":"2023 12","text":""},{"location":"news/2023-12/#december-2023","title":"December 2023","text":""},{"location":"news/2023-12/#announcements","title":"Announcements","text":""},{"location":"news/2023-12/#grace-maintenance-multi-factor-authentication","title":"Grace Maintenance - Multi-Factor Authentication","text":"<p>The biannual scheduled maintenance for the Grace cluster will be occurring Dec 5-7. During this time, the cluster will be unavailable. See the Grace maintenance email announcements for more details.</p> <p>Multi-Factor authentication via Duo will be required for ssh for all users on Grace after the maintenance.  For most usage, this additional step is minimally invasive and makes our clusters much more secure. However, for users who use graphical transfer tools such as Cyberduck, please see our MFA transfer documentation.</p>"},{"location":"news/2023-12/#scavenge_gpu-and-scavenge_mpi","title":"<code>scavenge_gpu</code> and <code>scavenge_mpi</code>","text":"<p>In addition to the general purpose scavenge partition, we also have two resource specific scavenge partitions, <code>scavenge_gpu</code> (Grace, McCleary) and <code>scavenge_mpi</code> (Grace only). The <code>scavenge_gpu</code> partition contains all GPU enabled nodes, commons and privately owned. Similarly, the <code>scavenge_mpi</code> partition contains all nodes similar to the <code>mpi</code> partition. Both partitions have higher priority for their respective nodes than normal scavenge (meaning jobs submitted to <code>scavenge_gpu</code> or <code>scavenge_mpi</code> will preempt normal scavenge jobs). All scavenge partitions are exempt from CPU charges.</p>"},{"location":"news/2023-12/#software-highlights","title":"Software Highlights","text":"<ul> <li>IMOD/4.12.56_RHEL7-64_CUDA12.1 is now available on McCleary and Grace</li> </ul>"},{"location":"news/2024-01/","title":"2024 01","text":""},{"location":"news/2024-01/#january-2024","title":"January 2024","text":""},{"location":"news/2024-01/#announcements","title":"Announcements","text":""},{"location":"news/2024-01/#upcoming-milgram-rhel8-upgrade","title":"Upcoming Milgram RHEL8 Upgrade","text":"<p>As Red Hat Enterprise Linux (RHEL) 7 approaches its end of life, we will be upgrading the Milgram cluster from RHEL7 to RHEL8 during the February maintenance window.  This will bring Milgram in line with our other clusters and provide a number of key benefits: continued security patches and support beyond 2024, updated system libraries to better support modern software, improved node management system</p> <p>We have set aside rhel8_devel and rhel8_day partitions for use in debugging and testing of workflows before the February maintenance. For more information on testing your workflows see our explainer.</p>"},{"location":"news/2024-02-milgram/","title":"2024 02 milgram","text":""},{"location":"news/2024-02-milgram/#milgram-maintenance","title":"Milgram Maintenance","text":"<p>Februrary 6-8, 2024docs/news/2024-02-milgram.md</p>"},{"location":"news/2024-02-milgram/#software-updates","title":"Software Updates","text":"<ul> <li>Slurm updated to 23.02.7</li> <li>NVIDIA drivers updated to 545.23.08</li> <li>Apptainer updated to 1.2.5</li> <li>Lmod updated to 8.7.32</li> </ul>"},{"location":"news/2024-02-milgram/#upgrade-to-red-hat-8","title":"Upgrade to Red Hat 8","text":"<p>As part of this maintenance, the operating system on Milgram has been upgraded to Red Hat 8.</p> <p>Jobs submitted prior to maintenance that were held and now released will run under RHEL8 (instead of RHEL7).  This may cause some jobs to not run properly, so we encourage you to check on your job output.  Our docs page provides information on the RHEL8 upgrade, including fixes for common problems.  Please notify hpc@yale.edu if you require assistance. </p>"},{"location":"news/2024-02-milgram/#changes-to-interactive-partitions-and-jobs","title":"Changes to Interactive Partitions and Jobs","text":"<p>We have made the following changes to interactive jobs during the maintenance.  </p> <p>The 'interactive' and 'psych_interactive` partitions have been renamed to 'devel' and 'psych_devel', respectively, to bring Milgram in alignment with the other clusters.  This change was made on other clusters in recognition that interactive-style jobs (such as OnDemand and 'salloc' jobs) are commonly run outside of the 'interactive' partition.  Please adjust your workflows accordingly after the maintenance.</p> <p>Additionally, all users are limited to 4 interactive app instances (of any type) at one time.  Additional instances will be rejected until you delete older open instances.  For OnDemand jobs, closing the window does not terminate the interactive app job.  To terminate the job, click the \"Delete\" button in your \"My Interactive Apps\" page in the web portal.</p> <p>Please visit the status page at research.computing.yale.edu/system-status for the latest updates.  If you have questions, comments, or concerns, please contact us at hpc@yale.edu.</p>"},{"location":"news/2024-02/","title":"2024 02","text":""},{"location":"news/2024-02/#february-2024","title":"February 2024","text":""},{"location":"news/2024-02/#announcements","title":"Announcements","text":""},{"location":"news/2024-02/#milgram-maintenance","title":"Milgram Maintenance","text":"<p>The biannual scheduled maintenance for the Milgram cluster will be occurring Feb 6-8. During this time, the cluster will be unavailable. See the Milgram maintenance email announcements for more details. </p>"},{"location":"news/2024-02/#changes-to-rstudio-on-the-web-portal","title":"Changes to RStudio on the Web Portal","text":"<p>The \u201cRStudio Server\u201d app on the Open OnDemand web portal has been upgraded to support both the R software modules and R installed via conda. As such the \u201cRStudio Desktop\u201d app has been retired and removed from the web portal. If you still require RStudio Desktop, we provide instructions for running under the \u201cRemote Desktop\u201d app (please note that this is not a recommended practice for most users).</p>"},{"location":"news/2024-02/#software-highlights","title":"Software Highlights","text":"<ul> <li>ChimeraX is now available as an app on the McCleary Open OnDemand web portal</li> <li>FSL/6.0.5.2-centos7_64 is now available on McCleary</li> </ul>"},{"location":"news/2024-03/","title":"2024 03","text":""},{"location":"news/2024-03/#march-2024","title":"March 2024","text":""},{"location":"news/2024-03/#announcements","title":"Announcements","text":""},{"location":"news/2024-03/#cpu-usage-reporting-with-getusage","title":"CPU Usage Reporting with <code>getusage</code>","text":"<p>Researchers frequently wish to get a breakdown of their groups' cluster-usage. While Slurm provides tooling for querying the database, it is not particularly user-friendly. We have developed a tool, <code>getusage</code>, which allows researchers to quickly get insight into their groups\u2019 usage, broken down by date and user, including a monthly summary report. Please try this tool and let us know if there are any enhancement requests or ideas.   </p>"},{"location":"news/2024-03/#changes-to-rstudio-on-the-web-portal","title":"Changes to RStudio on the Web Portal","text":"<p>Visual Studio Code (VSCode) is a popular development tool that is widely used by our researchers. While there are several extensions that allow users to connect to remote servers over SSH, these are imperfect and often drops connection. Additionally, these remote sessions connect to the clusters' login nodes, where resources are limited. </p> <p>To meet the growing demand for this particular development tool, we have deployed an application for Open OnDemand that launches VS Code Server directly on a compute node which can then be accessed via web-browser. This OOD application is called <code>code_server</code> and is available on all clusters. For more information see [our OOD docs page](https://docs.ycrc.yale.edu/clusters-at-yale/access/ood/.</p>"},{"location":"news/2024-03/#retirement-of-grace-rhel7-apps-tree","title":"Retirement of Grace RHEL7 Apps Tree","text":"<p>As part of our routine deprecation of older software we removed Grace's old application tree (from before the RedHat 8 upgrade) from the default Standard Environment on March 1st. After March 1st, the older module tree will no longer appear when <code>module avail</code> is run and will fail to load going forward. If you have concerns about any missing software, please contact us at hpc@yale.edu.</p>"},{"location":"news/2024-03/#software-highlights","title":"Software Highlights","text":"<ul> <li>FSL/6.0.5.2-centos7_64 is now available on Milgram</li> <li>Nextflow/23.10.1 is now available on Grace &amp; McCleary</li> </ul>"},{"location":"news/2024-04/","title":"2024 04","text":""},{"location":"news/2024-04/#april-2024","title":"April 2024","text":""},{"location":"news/2024-04/#announcements","title":"Announcements","text":""},{"location":"news/2024-04/#new-grace-nodes","title":"New Grace Nodes","text":"<p>We are pleased to announce the addition of 84 new direct-liquid-cooled compute nodes to the commons partitions (day and week) on Grace.  These new nodes are of the Intel Icelake generation and have 48 cores each. These nodes also have increased RAM compared to other nodes on Grace, with 10GB per core. The day partition is now close to 11,000 cores and the week partition is now entirely composed of these nodes. A significant number of purchased nodes of similar design have also been added to respective private partitions and are available to all users via the scavenge partition.</p>"},{"location":"news/2024-04/#limited-mccleary-maintenance-april-2nd","title":"Limited McCleary Maintenance - April 2nd","text":"<p>Due to the limited updates needed on McCleary at this time, the upcoming April maintenance will not be a full 3-day downtime, but rather a one-day maintenance with limited disruption.  The McCleary cluster and storage will remain online and available throughout the maintenance period and there will be no disruption to running or pending batch jobs.  However, certain services will be unavailable for short periods throughout the day.  See maintenance announcement email for full details.</p>"},{"location":"news/2024-04/#cluster-node-status-in-open-ondemand","title":"Cluster Node Status in Open OnDemand","text":"<p>A Cluster Node Status app is now available in the Open OnDemand web portal on all clusters. This new app presents information about CPU, GPU and memory utilization for each compute node with the cluster. The app can be found under \u2018Utilities\u2019 -&gt; \u2018Cluster Node Status\u2019. </p>"},{"location":"news/2024-04/#retirement-of-grace-rhel7-apps-tree","title":"Retirement of Grace RHEL7 Apps Tree","text":"<p>As part of our routine deprecation of older software, we removed Grace's old application tree (from before the RedHat 8 upgrade) from the default Standard Environment on March 6th. After March 6th, the older module tree will no longer appear when <code>module avail</code> is run and will fail to load. If you have concerns about missing software, please contact us at hpc@yale.edu.</p>"},{"location":"news/2024-04/#software-highlights","title":"Software Highlights","text":"<ul> <li>R/4.3.2-foss-2022b is now available on Grace and McCleary<ul> <li>Corresponding Bioconductor and CRAN bundles are also now available</li> </ul> </li> <li>PyTorch/2.1.2-foss-2022b-CUDA-12.0.0 with CUDA is available on Grace and McCleary</li> </ul>"},{"location":"news/2024-05/","title":"2024 05","text":""},{"location":"news/2024-05/#may-2024","title":"May 2024","text":""},{"location":"news/2024-05/#announcements","title":"Announcements","text":""},{"location":"news/2024-05/#yale-joins-mghpcc","title":"Yale Joins MGHPCC","text":"<p>We are excited to share that Yale University has recently become a member of the Massachusetts Green High Performance Computing Center (MGHPCC), a not-for-profit data center designed for computationally-intensive research. The construction of a dedicated space for Yale within the facility and the installation of high-speed networking between Yale's campus and MGHPCC are currently underway. The first High-Performance Computing (HPC) hardware installations are expected to take place later this year. As more information becomes available, we will keep our users updated</p>"},{"location":"news/2024-05/#oneit-conference","title":"OneIT Conference","text":"<p>Earlier this year Yale ITS hosted the first in-person One IT conference, [\u201cAdvancing Collaborations: One IT as a Catalyst\u201d](https://your.yale.edu/news/2024/04/conference-edition-capturing-spirit-one-it. IT and IT-adjacent personnel from across campus came together to discuss topics that impact research and university operations. YCRC team members participated in a variety of sessions ranging from Research Storage and Software to the role of AI in higher education. </p> <p>Additionally, YCRC team members presented two posters. The first, [A Graphical Interface for Research and Education](https://image.s10.sfmc-content.com/lib/fe4515707564047b751572/m/1/83cc1a22-d9d3-498d-8a17-8088de496674.pdf, highlighted Open OnDemand and its barrier-reducing impact on courses and research alike. The second, [Globus: a platform for secure, efficient file transfer](https://image.s10.sfmc-content.com/lib/fe4515707564047b751572/m/1/52559eed-25ab-49c9-80be-6ce99fb95b25.pdf, demonstrated our successful deployment of Globus to improve data management and cross-institutional sharing of research materials. </p>"},{"location":"news/2024-05/#software-highlights","title":"Software Highlights","text":"<ul> <li>Spark is now available on Grace, McCleary and Milgram</li> <li>Nextflow/22.10.6 is now available on Grace and McCleary</li> </ul>"},{"location":"news/2024-06-grace/","title":"2024 06 grace","text":""},{"location":"news/2024-06-grace/#grace-maintenance","title":"Grace Maintenance","text":"<p>June 4-6, 2024</p>"},{"location":"news/2024-06-grace/#software-updates","title":"Software Updates","text":"<ul> <li>Slurm updated to 23.11.7</li> <li>NVIDIA drivers updated to 555.42.02</li> <li>Apptainer updated to 1.3.1</li> </ul>"},{"location":"news/2024-06-grace/#hardware-updates","title":"Hardware Updates","text":"<ul> <li>The remaining Broadwell generation nodes have been decommissioned.</li> <li>The <code>oldest</code> node constraint now returns Cascade Lake generation nodes.</li> <li>The <code>devel</code> partition is now composed of 5 Cascade Lake generation 6240 nodes and 1 Skylake generation (same as the mpi partition) node.</li> <li>The FDR InfiniBand fabric has been fully decommissioned, and networking has been updated across the Grace cluster.</li> <li>The Slayman storage system is no longer available from Grace (but remains accessible from McCleary).</li> </ul>"},{"location":"news/2024-06/","title":"2024 06","text":""},{"location":"news/2024-06/#june-2024","title":"June 2024","text":""},{"location":"news/2024-06/#announcements","title":"Announcements","text":""},{"location":"news/2024-06/#grace-maintenance","title":"Grace Maintenance","text":"<p>The biannual scheduled maintenance for the Grace cluster will be occurring Jun 4-6. During this time, the cluster will be unavailable. See the Grace maintenance email announcements for more details. </p>"},{"location":"news/2024-06/#compute-usage-monitoring-in-web-portal","title":"Compute Usage Monitoring in Web Portal","text":"<p>We have developed a suite of tools to enable research groups to monitor their combined utilization of cluster resources. We perform nightly queries of Slurm's database to aggregate cluster usage (in cpu_hours) broken down by user, account, and partition. These data are available both as a command-line utility (getusage) and a recently deployed web-application built into Open OnDemand. This can be accessed directly via:</p> <p>Grace: ood-grace.ycrc.yale.edu/pun/sys/ycrc_getusage McCleary: ood-mccleary.ycrc.yale.edu/pun/sys/ycrc_getusage Milgram: ood-milgram.ycrc.yale.edu/pun/sys/ycrc_getusage</p> <p>Additionally, the Getusage web-app can be accessed via the \u201cUtilities\u201d pull-down menu after logging into Open OnDemand.</p>"},{"location":"news/2024-06/#nairr-resources-for-ai","title":"NAIRR Resources for AI","text":"<p>Looking for compute resource for your AI or AI-enabled research? In the NAIRR Pilot, the US National Science Foundation (NSF), the US Department of Energy (DOE), and numerous other partners are providing access to a set of computing, model, platform and educational resources for projects related to advancing AI research. Applications for resources from the NAIRR pilot are lightweight and we are happy to assist with any questions you may have. </p> <p>https://nairrpilot.org/opportunities/allocations</p>"},{"location":"news/2024-07/","title":"2024 07","text":""},{"location":"news/2024-07/#july-2024","title":"July 2024","text":""},{"location":"news/2024-07/#announcements","title":"Announcements","text":""},{"location":"news/2024-07/#compute-charges-rate-freeze","title":"Compute Charges Rate Freeze","text":"<p>The compute charging model for the YCRC clusters is currently under review.  As a result, we are freezing the per-CPU-hour charge at its current value of $0.0025, effective immediately. For more information on the compute charging model, please see the Billing for HPC services page on the YCRC website.</p>"},{"location":"news/2024-07/#matlab-proxy-server","title":"MATLAB Proxy Server","text":"<p>\"MATLAB (Web)\" is now available as an Open OnDemand app. A MATLAB session is connected directly to your web browser tab, rather than launched via a Remote Desktop session as with the traditional MATLAB app. This allows more of the requested resources to be dedicated to MATLAB itself.  Page through the full App list in Open OnDemand to launch. (Note that this is a work in progress that might not yet have all the functionality of a regular MATLAB session.)</p>"},{"location":"news/2024-07/#fairshare-weights-adjustment","title":"FairShare Weights Adjustment","text":"<p>Periodically we adjust the relative impact of resource allocations on a group\u2019s FairShare (the way that the scheduler determines which job gets scheduled next). We have adjusted the \u201cservice unit\u201d weights for memory and GPUs to better match their cost to acquire and maintain:</p> <pre><code>CPU: 1 SU\nMemory: 0.067 (15G/SU)\nA100 GPU: 100 SU\nnon-A100 GPU: 15 SU\n</code></pre> <p>For more information about FairShare and how we use it to ensure equity in scheduling, please visit our docs page.</p>"},{"location":"news/2024-07/#software-highlights","title":"Software Highlights","text":"<ul> <li>SBGrid is available on McCleary. Contact us for more information on access.</li> </ul>"},{"location":"news/2024-08-milgram/","title":"2024 08 milgram","text":""},{"location":"news/2024-08-milgram/#milgram-maintenance","title":"Milgram Maintenance","text":"<p>August 20-22, 2024</p>"},{"location":"news/2024-08-milgram/#software-updates","title":"Software Updates","text":"<ul> <li>Slurm updated to 23.11.9</li> <li>NVIDIA drivers updated to 555.42.06</li> <li>Apptainer updated to 1.3.3</li> </ul>"},{"location":"news/2024-08-milgram/#hardware-updates","title":"Hardware Updates","text":"<ul> <li>We proactively replaced two core file servers, as part of our best practices for GPFS (our high-performance parallel filesystem).  This does not change our storage capacity, but allows us to maintain existing services and access future upgrades.</li> </ul>"},{"location":"news/2024-08/","title":"2024 08","text":""},{"location":"news/2024-08/#august-2024","title":"August 2024","text":""},{"location":"news/2024-08/#announcements","title":"Announcements","text":""},{"location":"news/2024-08/#milgram-maintenance","title":"Milgram Maintenance","text":"<p>The biannual scheduled maintenance for the Milgram cluster will be occurring August 20-22. During this time, the cluster will be unavailable. See the Milgram maintenance email announcements for more details.</p>"},{"location":"news/2024-08/#ycrc-hpc-user-portal","title":"YCRC HPC User Portal","text":"<p>We have recently deployed a web-based User Portal for Grace, McCleary, and Milgram clusters to help researchers view information about their activity on our clusters. Accessible under the \u201cUtilities\u201d tab on Open OnDemand (or at the links below), the portal currently features five pages with personalized data about your cluster usage and guidance on navigating and operating the clusters. Users can easily track their jobs and visualize their utilization through charts, tables, and graphs, helping to optimize their cluster use. To try out the User portal please visit either:</p> <ul> <li>Grace</li> <li>McCleary</li> <li>Milgram</li> </ul> <p>If you have any suggestions for useful pages for the User Portal, please email hpc@yale.edu</p>"},{"location":"news/2024-08/#yale-task-force-on-artificial-intelligence","title":"Yale Task Force on Artificial Intelligence","text":"<p>As recommended by the Yale Task Force on Artificial Intelligence, the YCRC will be adding a large number of new high-end GPUs to the clusters to meet growing demand for AI compute resources. Stay tuned for details and updates on availability in the coming months!</p>"},{"location":"news/2024-08/#research-support-at-pearc24","title":"Research Support at PEARC24","text":"<p>During the week of July 22nd, the YCRC Research Support team attended the annual PEARC conference, a conference specifically focused on the research computing community. The team had the opportunity to meet with many of our peer institutions to discuss our common challenges and opportunities and attend sessions to learn about new solutions. We look forward to bringing some new ideas to the YCRC this year!</p>"},{"location":"news/2024-08/#software-highlights","title":"Software Highlights","text":"<ul> <li>ORCA/6.0.0-gompi-2022b is now available on Grace and McCleary</li> </ul>"},{"location":"news/2024-09/","title":"2024 09","text":""},{"location":"news/2024-09/#september-2024","title":"September 2024","text":""},{"location":"news/2024-09/#announcements","title":"Announcements","text":""},{"location":"news/2024-09/#the-ycrc-is-hiring","title":"The YCRC is Hiring!","text":"<p>The YCRC is looking to add permanent members to our Research Support team. If helping others use the clusters and learning about other work done at the YCRC interests you, consider joining the YCRC! If you have any questions about the position, contact Kaylea Nelson (kaylea.nelson@yale.edu).</p> <p>https://research.computing.yale.edu/about/careers</p>"},{"location":"news/2024-09/#clarity-access","title":"Clarity Access","text":"<p>Yale is launching the Clarity platform. In its initial phase, Clarity offers an AI chatbot powered by OpenAI\u2019s ChatGPT-4o. Importantly, Clarity provides a \u201cwalled-off\u201d environment; its use is limited to Yale faculty, students, and staff, and information entered into its chatbot is not saved or used to train external AI models. Clarity is appropriate for use with all data types, including [high-risk data](https://your.yale.edu/policies-procedures/policies/1604-data-classification-policy, provided that all security standards are observed. Its chatbot is capable of content creation, coding assistance, data and image analysis, text-to-speech, and more. Over time, the platform may expand to incorporate additional AI tools, including other large language models. Clarity is designed to evolve as generative AI develops and the community offers feedback. </p> <p>Before using the Clarity AI chatbot, please review training resources and guidance on appropriate use.</p>"},{"location":"news/2024-09/#job-performance-monitoring","title":"Job Performance Monitoring","text":"<p>We have recently deployed a new tool for measuring and monitoring job performance called <code>jobstats</code>. Available on all clusters, <code>jobstats</code> provides a report of the utilization of CPU, Memory, and GPU resources for in-progress and recently completed jobs. To generate the report simply run (replacing the ID number of the job in question):</p> <pre><code>[ab123@grace ~]$ jobstats 123456789\n\n======================================================================\n                         Slurm Job Statistics\n======================================================================\n         Job ID: 123456789\n  NetID/Account: ab123/agroup\n       Job Name: my_job\n          State: RUNNING\n          Nodes: 1\n      CPU Cores: 4\n     CPU Memory: 256GB (64GB per CPU-core)\n  QOS/Partition: normal/week\n        Cluster: grace\n     Start Time: Thu Sep 5, 2024 at 10:58 AM\n       Run Time: 1-06:43:41 (in progress)\n     Time Limit: 4-04:00:00\n\n                         Overall Utilization\n======================================================================\n  CPU utilization  [|||||||||||||                            26%]\n  CPU memory usage [||||                                      8%]\n\n                         Detailed Utilization\n======================================================================\n  CPU utilization per node (CPU time used/run time)\n      r816u29n04: 1-07:48:36/5-02:54:45 (efficiency=25.9%)\n\n  CPU memory usage per node - used/allocated\n      r816u29n04: 19.9GB/256.0GB (5.0GB/64.0GB per core of 4)\n</code></pre>"},{"location":"news/2024-09/#software-highlights","title":"Software Highlights","text":"<ul> <li>R/4.4.1-foss-2022b is now available on all clusters</li> <li>R-bundle-Bioconductor/3.19-foss-2022b-R-4.4.1 (INCLUDES SEURAT) is now available on all clusters</li> </ul>"},{"location":"news/2024-10/","title":"2024 10","text":""},{"location":"news/2024-10/#october-2024","title":"October 2024","text":""},{"location":"news/2024-10/#announcements","title":"Announcements","text":""},{"location":"news/2024-10/#announcing-bouchet","title":"Announcing Bouchet","text":"<p>The Bouchet HPC cluster, YCRC's first installation at MGHPCC, will be in beta in Fall 2024. The first installation of nodes, approximately 4,000 direct-liquid-cooled cores, will be dedicated to tightly coupled parallel workflows, such as those run in the \u201cmpi\u201d partition on the Grace cluster. Later this year, we will acquire and install a large number of general-purpose compute nodes and GPU-enabled compute nodes. At that point, Bouchet will be available to all Yale researchers for computational work involving low-risk data.  Visit the Bouchet page for more information and updates.</p>"},{"location":"news/2024-10/#jobstats-on-the-web","title":"Jobstats on the Web","text":"<p>In our quest to provide detailed information about job performance and efficiency, we have recently enhanced the web-based jobstats portal to show summary statistics and plots of CPU, Memory, GPU, and GPU memory usage over time. </p> <p>These plots are helpful diagnostics for understanding why jobs fail or how to more efficiently request resources. These plots and statistics are available for in-progress jobs, a great way to keep track of performance while jobs are still running. This tool is part of the User Portal which can be accessed via Open OnDemand for each cluster:</p> <ul> <li>Grace</li> <li>McCleary</li> <li>Milgram</li> </ul>"},{"location":"news/2024-10/#software-highlights","title":"Software Highlights","text":"<ul> <li>GROMACS/2023.3-foss-2022b-CUDA-12.1.1-PLUMED-2.9.2 is now available on Grace and McCleary</li> </ul>"},{"location":"news/2024-11/","title":"2024 11","text":""},{"location":"news/2024-11/#november-2024","title":"November 2024","text":""},{"location":"news/2024-11/#announcements","title":"Announcements","text":""},{"location":"news/2024-11/#ycrc-welcomes-ruth-marinshaw-as-the-new-executive-director","title":"YCRC Welcomes Ruth Marinshaw as the New Executive Director","text":"<p>YCRC Team is delighted to welcome Ruth Marinshaw as our new Executive Director. Ruth joined Yale this week to serve as the university's primary technologist to support the computing needs of Yale's research community. She will work with colleagues across campus to implement and operate computational technologies that support Yale faculty, students, and staff in conducting cutting-edge research.</p> <p>Ruth brings to Yale over twenty years of experience leading technology and research services in higher education. Ruth had held multiple positions at The University of North Carolina Chapel Hill, overseeing research computing services, staff, and systems and creating new service partnerships across the university. Over the last twelve years, Ruth served as chief technology officer for research computing at Stanford University. Under Ruth's leadership, Stanford's research computing and cyberinfrastructure services, systems, facilities, and support have grown significantly. She developed a team of research computing professionals and forged critical partnerships across the institution. Ruth was pivotal in establishing an NVIDIA SuperPOD - a data science and AI-focused research instrument envisioned by Stanford\u2019s faculty.</p> <p>Throughout her career, Ruth has contributed expertise to national conversations on research computing and currently serves as co-chair of the National Science Foundation's Advisory Committee for Cyberinfrastructure.</p> <p>With exciting initiatives on the horizon, we look forward to Ruth's leadership and guidance in supporting Yale's research community's current and emerging needs. Welcome aboard, Ruth!</p> <p>Read the announcement from John Barden, Vice President for Information Technology and Campus Services, and Michael Crair, Vice Provost for Research.</p>"},{"location":"news/2024-11/#bouchet-beta-testing-for-tightly-coupled-parallel-workflows","title":"Bouchet Beta Testing for Tightly Coupled Parallel Workflows","text":"<p>The YCRC\u2019s first installation at Massachusetts Green High Performance Computing Center will be the HPC cluster Bouchet*.</p> <p>The first installation of nodes, approximately 4,000 direct-liquid-cooled cores, will be dedicated to tightly coupled parallel workflows, such as those run in the \u201cmpi\u201d partition on the Grace cluster.</p> <p>We would like to invite you to participate in Bouchet beta testing. We are seeking tightly coupled, parallel workloads only to participate in this phase of development. If you have a suitable parallel workload and would like to participate in testing, please complete the Bouchet Beta Request Form and we will contact you with additional information about accessing and using Bouchet.</p> <p>Following the beta testing (early 2025), we will be acquiring and installing thousands of general purpose compute cores as well as GPU-enabled compute nodes. At that time Bouchet will become available to all Yale researchers for computational work involving low-risk data. Stay tuned for more information on availability of the cluster. </p> <p>*The cluster named after Edward Bouchet (1852-1918) who earned a PhD in physics at Yale University in 1876, making him the first self-identified African American to earn a doctorate from an American university. </p>"},{"location":"resources/","title":"Training &amp; Other Resources","text":"<p>The YCRC offers training sessions in a wide range of topics related to research computing taught by YCRC staff, HPC experts at national HPC centers or our vendor partners.</p>"},{"location":"resources/glossary/","title":"Glossary","text":"<p>To help clarify the way we refer to certain terms in our user documentation, here is a brief list of some of the words that regularly come up in our documents. Please reach out to us at hpc@yale.edu if there are any other words that need to be added.</p> <p>Account - used to authenticate and grant permission to access resources</p> <p>Account (Slurm) - an accounting mechanism to keep track of a group's computing usage</p> <p>Activate - making something operational</p> <p>Array - a data structure across a series of memory locations consisting of elements organized in an index</p> <p>Array (job) - a series of jobs that all request the same resources and run the same batch script</p> <p>Array Task ID - a unique sequential number with an appended number that refers to an individual task within the set of submitted jobs</p> <p>Channel - Community-led collections of packages created by a group or lab installed with conda to allow for a homogenous environment across systems</p> <p>CLI - Command Line Interface processes commands to a computer program in the form of lines of text in a window</p> <p>Cluster - a set of computers, called nodes) networked together so nodes can perform the tasks facilitated by a scheduling software</p> <p>Command - a specific order from a computer to execute a service with either an application or the operating system</p> <p>Compute Node - the nodes that work runs on to perform computational work</p> <p>Container - A stack of software, libraries and operating system that is independent of the host computer and can be accessed on other computers</p> <p>Container Image - Self-contained read-only files used to run applications</p> <p>CPU - Central Processing Units are the components of a system that perform basic operations and exchange data with the system\u2019s memory (also known as a processor)</p> <p>Data - items of information collected together for reference or analysis</p> <p>Database - a collection of structured data held within a computer</p> <p>Deactivate - making something de-operational</p> <p>Environment - a collection of hardware, software, data storage and networks that work together in facilitating the processing and exchange of information</p> <p>Extension - Suffix at the end of a filename to indicate the file type</p> <p>Fileset - a section of a storage device that is given a designated purpose</p> <p>Filesystem - a process that manages how and where data is stored</p> <p>Flag - (see Options)</p> <p>GPU - Graphics Processing Units are specialized circuits designed to rapidly manipulate memory and create images in a frame buffer for a displayed output</p> <p>GridFTP - an extension of the Fire Transfer Protocol for grid computing that allows users to transfer and save data on a different account such as Google Drive or other off network memory</p> <p>Group - a collection of users who can all be given the same permissions on a system</p> <p>GUI - Graphical User Interface allows users to interact with devices and applications through a visual window that can commonly display icons and predetermined fields</p> <p>Hardware - the physical parts of a computer</p> <p>Host - (ie. Host Computer) A device connected to a computer network that offers resources, services and applications to users on the network</p> <p>Image - (See Container Image)</p> <p>Index - a method of sorting data by creating keywords or a listing of data</p> <p>Interface - a boundary across which two or more computer system components can exchange information</p> <p>Job - a unit of work given to an operating system by a scheduler</p> <p>Job Array - a way to submit multiple similar jobs by associating each subjob with an index value based on an array task id</p> <p>Key - a variable value applied using an algorithm to a block of unencrypted text to produce encrypted text</p> <p>Load - transfer a program or data into memory or into the CPU</p> <p>Login Node - a node that users log in on to access the cluster</p> <p>Memory - (see RAM)</p> <p>Metadata - A set of data that describes and gives basic information about other data</p> <p>Module - a number of distinct but interrelated units that build up or into a program</p> <p>MPI - Message Passing Interface is a standardized and portable message-passing standard designed to function on parallel computing architectures</p> <p>Multiprocessing - the ability to operate more than one task simultaneously on the same program across two or more processors in a computer</p> <p>Node - a server in the cluster</p> <p>Option - a single letter or full word that modifies the behavior of a command in a predetermined way (also known as a flag or switch)</p> <p>Package - a collection of hardware and software needed to create a working system</p> <p>Parallel - (ex. Computing/Programming) Architecture in which several processes are carried out simultaneously across smaller, independent parts</p> <p>Partition - a section of a storage device that is given a designated purpose</p> <p>Partition (Slurm) - a collection of compute nodes available via the scheduler</p> <p>Path - A string of characters used to identify locations throughout a directory structure</p> <p>Pane - (Associate with window) A subdivision within a window where an independent terminal can run simultaneously alongside another terminal</p> <p>Processor - (see CPU)</p> <p>Queue - a sequence of objects arranged according to priority waiting to be processed</p> <p>RAM - Random Access Memory, also known as \"Memory\" can be read and changed in any order and is typically used to to store working data</p> <p>Reproducibility - the ability to execute the same results across multiple systems by different individuals using the same data</p> <p>Scheduler - the software used to assign resources to a job for tasks</p> <p>Scheduling - the act of assigning resources to a task through a software product</p> <p>Session - a temporary information exchange between two or more devices</p> <p>SSH - secure shell is a cryptographic network protocol for operating network services securely over an unsecured network</p> <p>Software - a collection of data and instructions that tell a computer how to operate</p> <p>Switch - (see Options)</p> <p>System - a set of integrated hardware and software that input, output, process, and store data and information</p> <p>Task ID - a unique sequential number used to refer to a task</p> <p>Terminal - Referring to a terminal program, a text-based interface for typing commands</p> <p>Toolchain - a set of tools performing individual actions used in delivering an operation</p> <p>Unload - remove a program or data from memory or out of the CPU</p> <p>User - a person interacting and utilizing a computing service</p> <p>Variable - assigned and referenced data values that can be called within a program and changed depending on how the program runs</p> <p>Window - (Associate with pane) the whole screen being displayed, containing subdivisions, or panes, that can run independent terminals alongside each other</p>"},{"location":"resources/intro_to_hpc_tutorial/","title":"Introduction to HPC Tutorials","text":"<p>To begin, access the cluster through Open OnDemand and open the shell window. This can be done by by going to the top navigation bar, clicking on the Clusters tab and selecting the Shell Access button.</p> <p></p> <p>Once the new shell window is loaded, you will be able use this interface like your local command interface. Now that you're setup in a shell window, you can begin the first task like so:</p>"},{"location":"resources/intro_to_hpc_tutorial/#part-1-interactive-jobs","title":"Part 1: Interactive Jobs","text":"<p>Inside of the shell window, start an interactive job with the default resource requests. Once you are allocated space off the login node, load the Miniconda module and create a Conda environment for this exercise. This can be done like so:</p> <pre><code># Ask for an interactive session\nsalloc\n\n# Load the Miniconda module\nmodule load miniconda\n\n# Create a test environment with Conda that contains the default Python version\nconda create -yn tutorial_env python jupyter\n\n# Activate the new environment\nconda activate tutorial_env\n\n# Deactivate the new environment\nconda deactivate\n\n# Exit your interactive job to free the resources\nexit\n</code></pre>"},{"location":"resources/intro_to_hpc_tutorial/#part-2-batch-jobs","title":"Part 2: Batch Jobs","text":"<p>Going off of the environment we created in part 1, navigate to the Files tab in OOD and select your project directory. Click the '+ New File' button and name the file <code>message_decode_tutorial.py</code>. Once the new file is created, open this file in the OOD text editor by going to the file, clicking the three-dot more button, and selecting edit in the dropdown menu like so:</p> <p></p> <p>Once the text editor is open, paste this python example inside of the file:</p> <pre><code>def message_decode_tutorial(message, c):\n    holder = \"\"\n    for letter in range(0, len(message)):\n        if (letter + 1) % c == 0:\n            holder = holder + message[letter]\n    return holder\n\nmessage = 'gT baZu lWp Kjv uXyeS nViU fdlH gJr KaIc tBpl Sy\\\nJox MtUl Qbm kGTp UdHe hdLJf Nu IcPRu XhBtDjf TsmPf\\\no DoKfw xP qyTcJ tUpYrv Pk ArBCf Wrtp JfRcX JqPdKLC'\n\ncypher = message_decode_tutorial(message, 10)\n\nwith open('/home/NETID/decoded_example.txt','w+') as output:\n    print(cypher, file=output)\n</code></pre> <p>This python function takes a given message and parses through it against the parameters of a cypher, which in our case writes every 10th letter. Make sure to replace the placeholder 'NETID' in the second to last line with your personal NetID. This will allow your output file to go into your homespace.</p> <p>From here, navigate back to your project directory and select the '+ New File' button, this time naming it <code>batch_tutorial.sh</code>. Using Slurm options to define resource requests for this job, paste the following code inside of this file like you did the previous file:</p> <pre><code>#!/bin/bash\n\n#SBATCH --job-name=message_decode_tutorial\n#SBATCH --time=1:00\n#SBATCH --mem-per-cpu=2MB\n#SBATCH --mail-type=ALL\n\nmodule load miniconda\n\nsource activate tutorial_env\n\npython message_decode_tutorial.py\n</code></pre> <p>Because the partition isn't specified for this job, it will run on the cluster's default partition. From there, you can go back to the shell window, navigate to your project directory and run the sbatch command to begin your batch job like so:</p> <pre><code># Navigate to the project directory\ncd project\n\n# Use Slurm to start a batch job\nsbatch batch_tutorial.sh\n</code></pre> <p>Once you receive an email saying the job is complete, navigate to your home-space through the shell window on Open OnDemand. Within this directory you will find a file called <code>decoded_example.txt</code>. To quickly see the file contents, use the <code>cat</code> command to print the file's contents on the standard output, revealing the decoded message like so:</p> <pre><code># Navigate to your homespace (replacing NETID with your netID)\ncd /home/NETID\n\n# Print out the decoded message\ncat decoded_example.txt\n</code></pre>"},{"location":"resources/intro_to_hpc_tutorial/#part-3-interactive-apps-on-ood","title":"Part 3: Interactive Apps on OOD","text":"<p>Now that you have completed both an interactive and batch job, try using Jupyter Notebooks on Open OnDemand for your work. This can be done in the shell window like so:</p> <pre><code># Purge any loaded modules\nmodule purge\n\n# Build your environment dropdown tab on OOD\nycrc_conda_env.sh update\n</code></pre> <p>Now that this is completed, return to the Open OnDemand homepage and select the Interactive Apps dropdown tab in the top navigation bar. From there you can select Jupyter and load the job submission request form. To select your resources, make sure to consult our Slurm documentation as well as the specific cluster's partition information to ensure you're selecting the appropriate resources for your job's needs.</p> <p>Once the session is submitted and running, connect to the notebook and navigate to your working directory. From there you can either select the Upload button to upload an existing Jupyter notebook file or select the New button to create a new notebook.</p> <p>To help with this, make sure to look over the YCRC Jupyter Notebook information as well as Jupyter's User Interface page.</p>"},{"location":"resources/national-hpcs/","title":"National HPCs","text":"<p>Beyond Yale\u2019s on campus clusters, there are a number of ways for researchers to obtain compute resources (both cycles and storage) at national facilities.</p> <p>Yale researchers may use the Data Management Planning Tool (DMPtool) to create, review, and share data management plans that are in accordance with institutional and funder requirements.</p>"},{"location":"resources/national-hpcs/#access-formerly-xsede","title":"ACCESS (formerly XSEDE)","text":"<p>Quarterly | Application &amp; Info</p> <p>\"Explore Allocations\" are readily available on ACCESS resources for benchmarking and planning runs. For even lower commitment allocations (e.g. to just explore the resource), YCRC staff members have \"Campus Champions\" allocations on all ACCESS resources that can be shared upon request. Contact us for access.</p> <p>ACCESS resources include the following. Up to date information is available at access-ci.org:</p> <ul> <li>Stampede2: traditional compute and Phis</li> <li>Jetstream: Science Gateways</li> <li>Bridges2: traditional compute and GPUs</li> <li>Comet: traditional compute and GPUs</li> <li>XStream: GPU cluster</li> </ul>"},{"location":"resources/national-hpcs/#department-of-energy","title":"Department of Energy","text":"<p>NERSC, Argonne Leadership Computing Facility (ALCF), Oak Ridge Leadership Computing Facility (OLCF)</p>"},{"location":"resources/national-hpcs/#incite","title":"INCITE","text":"<p>Due in June | Application &amp; Info</p>"},{"location":"resources/national-hpcs/#alcc","title":"ALCC","text":"<p>Due in June | Application &amp; Info</p>"},{"location":"resources/national-hpcs/#anl-directors-discretionary","title":"ANL Director\u2019s Discretionary","text":"<p>Rolling submission | Application &amp; Info</p> <p>3-6 month duration. Expectation is that you are using it to gather data for ALCC or INCITE proposal</p>"},{"location":"resources/national-hpcs/#olcf-directors-discretionary","title":"OLCF Director\u2019s Discretionary","text":"<p>Rolling submission | Application &amp; Info</p>"},{"location":"resources/national-hpcs/#ncsa-blue-waters","title":"NCSA: Blue Waters","text":""},{"location":"resources/national-hpcs/#prac","title":"PRAC","text":"<p>Due in November | Application &amp; Info</p>"},{"location":"resources/national-hpcs/#blue-waters-innovation-allocations","title":"Blue Water\u2019s Innovation Allocations","text":"<p>Rolling submission | Application &amp; Info</p>"},{"location":"resources/national-hpcs/#open-science-grid-osg","title":"Open Science Grid (OSG)","text":"<p>Rolling Submission | Application &amp; Info</p> <p>The OSG facilitates access to distributed high throughput computing for research in the US. The resources accessible through the OSG are contributed by the community, organized by the OSG, and governed by the OSG consortium.</p>"},{"location":"resources/online-tutorials/","title":"Online Tutorials","text":""},{"location":"resources/online-tutorials/#linuxunix-and-command-line","title":"Linux/Unix and Command Line","text":""},{"location":"resources/online-tutorials/#introduction-to-linux","title":"Introduction to Linux","text":"<ul> <li>YCRC Workshop: Practical Introduction to Linux, (Video) *Recommended</li> <li>Most Commonly Used Commands - RedHat.com</li> <li>Command Line for Beginners - Ubuntu.com</li> </ul> <p>Note: You can learn more about most commands you come across by typing \"man [command]\" into the terminal.</p>"},{"location":"resources/online-tutorials/#awk-text-extractionparsing","title":"<code>awk</code> (text extraction/parsing)","text":"<p><code>awk</code> is a tool for parsing text and extracting certain section. It is particularly useful for extracting, and even reordering, columns out of tables in text files.</p> <ul> <li>Introduction to <code>awk</code> and examples of common usage</li> <li>In-depth guide to <code>awk</code> and more advanced usage</li> </ul>"},{"location":"resources/online-tutorials/#grep","title":"<code>grep</code>","text":"<p>Grep is tool for searching command line output or files for a certain string (phrase) or regular expression.</p> <ul> <li>Introduction to <code>grep</code> and examples of common usage</li> <li>In-depth guide to <code>grep</code> and more advanced usage</li> </ul>"},{"location":"resources/online-tutorials/#sed","title":"<code>sed</code>","text":"<p><code>sed</code> (Stream EDitor) is a tool for making substitutions in a text file. For example, it can be useful for cleaning (e.g. replace NAN with 0) or reformatting data files. The syntax <code>sed</code> uses for substitutions is common in Linux (for example, the same syntax is used in the VIM text editor).</p> <ul> <li>Introduction to <code>sed</code> and examples of common usage</li> <li>In-depth guide to <code>sed</code> and more advanced usage</li> </ul>"},{"location":"resources/online-tutorials/#ssh-connecting-to-the-clusters-or-other-remote-linux-servers","title":"SSH (connecting to the clusters or other remote linux servers)","text":"<ul> <li>Connecting to the Yale clusters</li> <li>Transfer files to/from the cluster</li> <li>Advanced SSH configuration</li> <li>In-depth guide to <code>ssh</code></li> </ul>"},{"location":"resources/online-tutorials/#bashrc-and-bash-profiles","title":"Bashrc and Bash Profiles","text":"<ul> <li>What is the <code>.bashrc</code> and <code>.bash_profile</code>?</li> <li>[Set aliases for commonly used commands]</li> <li>[Environment variables]</li> </ul>"},{"location":"resources/online-tutorials/#tar-or-targz-archive","title":"tar or tar.gz archive","text":"<p><code>.tar</code> or <code>t.ar.gz</code> are common archive (compressed file) formats. Software and data will frequently be distributed in one of these archive formats. The most common command for opening and extracting the contents of a <code>tar</code> archive is <code>tar xvf archive.tar</code> and, for a <code>tar.gz</code> archive, <code>tar xvzf archive.tar.gz</code> . See the following link(s) for more details on creating <code>tar</code> files and more advanced extraction options.</p> <ul> <li>Creating and extracting from a tar file</li> </ul>"},{"location":"resources/online-tutorials/#install-windows-and-linux-on-the-same-computer","title":"Install Windows and Linux on the same computer","text":""},{"location":"resources/online-tutorials/#windows-for-linux","title":"Windows for Linux","text":"<p>It is possible to run Linux terminals and applications from within a Windows installation using the \"Windows Subsystem for Linux\".</p> <ul> <li>Windows Subsystem for Linux</li> </ul>"},{"location":"resources/online-tutorials/#dual-boot","title":"Dual Boot","text":"<p>\"Dual Boot\" means you have two separate installations for Windows and Linux, respectively, that switch between by restarting your computer.</p> <ul> <li>Dual Boot Linux Mint and Windows</li> <li>Dual Boot Ubuntu and Windows</li> </ul>"},{"location":"resources/online-tutorials/#python","title":"Python","text":""},{"location":"resources/online-tutorials/#intro-to-python","title":"Intro to Python","text":"<ul> <li>Fantastic resource for anyone interested in Python</li> <li>LinkedIn Learning: Learning Python (Yale only)</li> </ul>"},{"location":"resources/online-tutorials/#parallel-programming-with-python","title":"Parallel Programming with Python","text":"<ul> <li>Quick Tutorial: Python Multiprocessing</li> <li>Parallel Programming with Python</li> <li>YCRC Workshop: Parallel Python</li> </ul>"},{"location":"resources/online-tutorials/#mpi4py","title":"mpi4py","text":"<ul> <li>YCRC Workshop: mpi4py</li> <li>mpi4py example scripts</li> <li>Documentation for mpi4py</li> </ul>"},{"location":"resources/online-tutorials/#r","title":"R","text":""},{"location":"resources/online-tutorials/#intro-to-r","title":"Intro to R","text":"<ul> <li>Brief intro to R</li> <li>Thorough intro to R</li> <li>Another thorough intro to R</li> </ul>"},{"location":"resources/online-tutorials/#foreach","title":"foreach","text":"<ul> <li>Using the foreach package - Steve Weston</li> </ul>"},{"location":"resources/online-tutorials/#foreach-dompi","title":"foreach + dompi","text":"<ul> <li>Introduction to doMPI</li> </ul>"},{"location":"resources/online-tutorials/#matlab","title":"Matlab","text":"<ul> <li>Mathworks Online Classses</li> </ul>"},{"location":"resources/online-tutorials/#singularity-apptainer","title":"Singularity / Apptainer","text":""},{"location":"resources/online-tutorials/#documentation","title":"Documentation","text":"<p>Singularity has officially been renamed Apptainer, but we expect no changes to its functionality.</p> <ul> <li>Apptainer Docs Page</li> <li>Singularity Google Groups</li> </ul>"},{"location":"resources/online-tutorials/#tutorials","title":"Tutorials","text":"<ul> <li>YCRC Workshop: Containers</li> <li>NIH tutorial on Singularity</li> <li>NVIDIA tutorial for using GPUs with Singularity</li> </ul>"},{"location":"resources/sw_carpentry/","title":"Software Carpentry","text":"<p>To help researchers learn the skills they need, they can utilize Software Carpentry's in-house training as well as their community-led lesson development to help them get started. These in-house lessons are offered in both English and Spanish and go over Unix and Git basics as well as working with Python and R. To learn more about the community-based lessons available to users, see the Carpentries Lab page for more information.</p>"},{"location":"resources/yale_library/","title":"Yale Library","text":"<p>The Yale Library has many resources available to cluster users. For more information about the Yale Library, see the Ask Yale Library page here.</p>"},{"location":"resources/yale_library/#oreilly-safari-ebooks","title":"O'Reilly Safari eBooks","text":"<p>The Yale Library offers access to the O'Reilly Safari eBooks collection through your Yale credentials. This can be accessed by this Safari eBooks access page making sure to sign in with your Yale email. Once logged on, users can access a variety of digital books and courses.</p>"}]}